% !TEX root = main.tex
\documentclass[openany]{book}
    \usepackage[acronym]{glossaries}
    \usepackage[english]{babel}
    \usepackage{lipsum}
    \usepackage{hyperref}
    \usepackage{booktabs}
    \usepackage[utf8]{inputenc}
    \usepackage[a4paper,margin=2.5cm]{geometry}%2.5cm
    \usepackage{algorithm}
    \usepackage{algpseudocode}
    \usepackage{multirow}
    \usepackage{setspace}
    \usepackage[parfill]{parskip}
    \usepackage{graphicx}
    \usepackage[margin=1cm,font=normalsize,labelfont=bf]{caption}
    \usepackage{placeins}
    \usepackage{csquotes} 
    \usepackage{listings,xcolor}
    \usepackage[most]{tcolorbox}     
    \usepackage{pgfplots}
    \usepackage[
    backend=biber,
    style=numeric,
]{biblatex}


\addbibresource{refs.bib}

\newtcolorbox{myquote}{
    enhanced,
    frame hidden,
    colback=gray!10,
    sharp corners,
    left=10pt,
    right=0pt,
    borderline west={2pt}{0pt}{gray!50}
}


\newcommand{\CaptionFontSize}{\small}
% \newcommand{\comment}[1]{}
\newcommand{\mdblockquote}[1]{  \begin{myquote}  #1  \end{myquote}  }

\makeglossaries
\newacronym{ui}{UI}{User Interface}
\newacronym{ux}{UX}{User Experience}
\newacronym{ai}{AI}{Artificial Intelligence}
\newacronym{ml}{ML}{Machine Learning}
\newacronym{hci}{HCI}{Human-Computer Interaction}
\newacronym{gui}{GUI}{Graphical User Interface}
\newacronym{llm}{LLM}{Large Language Model}
\newacronym{nlp}{NLP}{Natural Language Processing}
\newacronym{api}{API}{Application Programming Interface}
\newacronym{vr}{VR}{Virtual Reality}
\newacronym{xr}{XR}{Extended Reality}
\newacronym{ar}{AR}{Augmented Reality}
\newacronym{vlm}{VLM}{Vision-Language Model}
\newacronym{react}{ReAct}{Reasoning and Acting}
\newacronym{rpa}{RPA}{Robotic Process Automation}
\newacronym{nlpui}{NLP UI}{Natural Language Processing User Interface}
\newacronym{dsl}{DSL}{Domain-Specific Language}
\newacronym{tui}{TUI}{Tangible User Interface}
\newacronym{cli}{CLI}{Command-Line Interface}
\newacronym{ide}{IDE}{Integrated Development Environment}
\newacronym{pbd}{PBD}{Programming by Demonstration}
\newacronym{vpl}{VPL}{Visual Programming Language}
\newacronym{cot}{CoT}{Chain of Thought}
\newacronym{seeact}{SeeAct}{Seeing and Acting}
\newacronym{autogpt}{AutoGPT}{Autonomous GPT}
\newacronym{gpt}{GPT}{Generative Pre-trained Transformer}

\definecolor{background}{HTML}{EEEEEE}

\lstdefinelanguage{json}{
    basicstyle=\normalfont\ttfamily,
    numbers=left,
    numberstyle=\scriptsize,
    breaklines=true,
    frame=lines,
    backgroundcolor=\color{gray!10},
    showstringspaces=false,
    string=[db]{"},
    stringstyle=\color{green!50!black},
    morestring=[s][\color{black}]{\ \ "}{":},
    keywordstyle=\color{blue},
    keywords={true,false,null},
    literate=
     *{0}{{{\color{red}0}}}{1}
      {1}{{{\color{red}1}}}{1}
      {2}{{{\color{red}2}}}{1}
      {3}{{{\color{red}3}}}{1}
      {4}{{{\color{red}4}}}{1}
      {5}{{{\color{red}5}}}{1}
      {6}{{{\color{red}6}}}{1}
      {7}{{{\color{red}7}}}{1}
      {8}{{{\color{red}8}}}{1}
      {9}{{{\color{red}9}}}{1}
      {.}{{{\color{red}.}}}{1}
      {:}{{{\color{gray}{:}}}}{1}
      {,}{{{\color{gray}{,}}}}{1}
      {\{}{{{\color{gray}{\{}}}}{1}
      {\}}{{{\color{gray}{\}}}}}{1}
      {[}{{{\color{gray}{[}}}}{1}
      {]}{{{\color{gray}{]}}}}{1},
}


\begin{document}

\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

\center
 
%----------------------------------------------------------------------------------------
%	HEADING SECTIONS
%----------------------------------------------------------------------------------------

\textsc{\LARGE Hasselt University}\\[1.0cm]
\textsc{\Large Master's thesis presented for the attainment of the degree of Master of Science in Computer Science}\\[0.5cm]

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------
\HRule \\[0.2cm]
{ \huge  \bfseries A Multimodal AI-Driven GUI Framework for Dynamic User Adaptation \par} 
\HRule \\[1.5cm]
 
%----------------------------------------------------------------------------------------
%	AUTHOR SECTION
%----------------------------------------------------------------------------------------

\begin{minipage}[t]{0.5\textwidth}
    \begin{flushleft}
        \large \textit{Author}:\vspace*{0.5cm} \\
        Yarne Dirkx
    \end{flushleft}
\end{minipage}%
%
\begin{minipage}[t]{0.5\textwidth}
    \begin{flushright}
        \large \textit{Promotor}:\vspace*{0.5cm} \\
        Prof. Dr. Kris Luyten\\
        \vspace*{0.5cm}
        %\large \textit{Co-promotor}:\vspace*{0.5cm} \\
        %\\
        %\vspace*{0.5cm}
        %\large \textit{Begeleider(s)}:\vspace*{0.5cm} \\
        %Dr. Eva Geurts\\
    \end{flushright}
\end{minipage}

\vspace*{1cm}

%----------------------------------------------------------------------------------------
%	DATE SECTION
%----------------------------------------------------------------------------------------

{\large Academic year 2024-2025}\\[2cm]

%----------------------------------------------------------------------------------------
%	LOGO SECTION
%----------------------------------------------------------------------------------------

\includegraphics[width=0.4\textwidth]{Images/uhasselt.jpg}\\
 
%----------------------------------------------------------------------------------------

\vfill % Fill the rest of the page with whitespace

\end{titlepage}


\chapter*{Abstract}
This thesis presents a multimodal AI-driven GUI framework for dynamic user interface adaptation, enabling real-time, personalised accessibility enhancements across platforms such as Flutter, SwiftUI, and beyond. Designed with a focus on the health domain, the framework supports motor-impaired, visually impaired, and hands-free users by integrating diverse input modalities including touch, keyboard, voice, and gestures.
At its core is Smart Intent Fusion (SIF), a multi-agent architecture powered by large language models (LLMs) via the Google Gemini API. SIF fuses user inputs, events, profiles, and interaction history to infer intent and propose targeted UI adaptations such as button enlargement, contrast enhancement, and navigation mode switching. The reasoning engine combines rule-based logic for predictable, low-latency responses with LLM-driven reasoning for complex or ambiguous cases.
The framework provides developer-friendly integration through standardised JSON contracts and a FastAPI backend, making it portable across domains and platforms. Its feasibility is demonstrated through varied configurations and simulated metrics including adaptation accuracy, task completion time, and latency showing measurable improvements in accessibility. By addressing the gap in intelligent, multimodal UI adaptation, this work helps advance human–computer interaction and lays the foundation for future AI models capable of autonomously rewriting UI code, offering a scalable and extensible solution for personalised user experiences.

\chapter*{Acknowledgments}
I would like to express my deepest appreciation to all those who have supported and guided me throughout the course of my master’s thesis.

First and foremost, I am sincerely grateful to Prof. Dr. Kris Luyten, my thesis supervisor, for his invaluable guidance, constructive feedback, practical tips, and continuous support.

I would also like to thank my fellow students for their encouragement, collaboration, and support during this journey. Special thanks go to my two dogs, Tobias and Casper, whose company during countless late-night coding sessions, and gentle reminders to take breaks helped keep me grounded.

My heartfelt gratitude extends to my family, especially my sister Phaedra, my brother-in-law Dennis, and my parents for their unwavering support, patience, and help with proofreading.

Finally, I am thankful to my friends for their motivation, belief in my abilities, and for making this challenging journey a rewarding one.

This work would not have been possible without the contribution and support of all these people, to whom I am deeply grateful.



\tableofcontents
\listoffigures
\listoftables

\glsaddall
\printglossary[type=\acronymtype, title=List of Abbreviations]

%Chapter 1
\chapter{Introduction}
\section{Background and Motivation}
Whether it's a smartphone app, a website, or a smartwatch, users primarily interact with technology through graphical user interfaces (GUIs). These interfaces rely on visual elements such as icons, buttons, and menus to enable intuitive and efficient interaction, replacing the need for complex text-based commands.
Before GUIs became standard, users had to operate computers through command-line interfaces, requiring memorization of commands and technical know-how. This barrier limited computing to experts. GUIs removed that limitation by introducing visual metaphors, opening up digital technology to the general public.
Since their introduction in the 1980s, GUIs have revolutionized how humans engage with computers, transforming once-specialized machines into everyday tools. As the critical interface layer between human and machine, GUIs translate complex computational tasks into accessible, visually manageable actions.

Over the decades, GUIs have evolved far beyond their early desktop roots. From the classic WIMP model windows, icons, menus, and pointers interfaces have expanded to include touch, voice, gesture, and even spatial interactions in augmented and virtual reality. Today’s user interfaces are increasingly multimodal, allowing for richer and more natural interactions.
Yet, despite this evolution in interaction modalities, the underlying structure of most GUIs remains largely static. Interfaces are typically designed with a one-size-fits-all approach, offering the same layout, behavior, and visual elements to all users, regardless of their needs or context. This static nature becomes a barrier in today’s increasingly diverse and dynamic world, especially for users with accessibility challenges. People with visual impairments, motor difficulties, or those who rely on hands-free interaction methods often struggle with standard interfaces that do not adapt to their specific needs. 
For example, small buttons can be difficult to tap for users with tremors, while low contrast or lack of screen reader support limits usability for visually impaired users.
Moreover, users interact with devices in various environments some noisy, some bright, some on the move, further complicating interaction for those needing alternative input or output modalities. Devices range widely in size and capability, from smartwatches and smartphones to desktop monitors and VR headsets, but accessibility features often remain limited or inconsistent across platforms. This gap highlights a key challenge in interface design: how can we create graphical user interfaces that dynamically adapt to individual users’ accessibility needs and context, ensuring inclusive and efficient interaction for all?

Adaptive UIs are user interfaces that can adjust themselves dynamically to accommodate the user’s accessibility needs and contextual factors. These interfaces are designed to intelligently modify their layout, behavior or visual appearance based on individual user preferences, abilities, and environmental conditions. By responding to multimodal inputs such as touch, voice, gestures, and gaze, adaptive UIs aim to provide a more personalized, inclusive, and efficient user experience, especially for users with diverse accessibility requirements. When we call an interface “intelligent” we mean it can perceive, interpret, and respond to complex user behaviors and contexts autonomously. This level of responsiveness goes beyond simple preset rules, necessitating systems that learn from user interactions and adapt close to or in real-time. Artificial Intelligence (AI), especially advances in machine learning and large language models, offers powerful tools to enable such intelligent adaptation. By processing multimodal inputs, UI context and user data, AI-driven interfaces can dynamically tailor themselves to meet diverse user needs, making accessibility more effective and seamless.

Large Language Models (LLMs) like GPT have revolutionized natural language understanding, processing and generation. Their ability to comprehend context, infer intent, and produce human-like responses makes them highly suitable for interpreting user intent, preferences, and even subtle cues from multimodal inputs such as speech, gestures, or eye movements.
In the context of adaptive UIs, LLMs can act as intelligent controllers that translate diverse and complex user interactions into actionable interface adaptations. For example, an LLM can understand a voice command like “make buttons bigger” or interpret hesitation in gestures to trigger UI changes that enhance usability for motor-impaired users. Furthermore, LLMs can dynamically generate or modify UI elements by turning the interface itself into an API enabling on-the-fly customization that is both personalized and context-aware.

Despite growing research and technological capabilities, adaptive user interfaces remain rare in practical use, often due to complexity in implementation and lack of robust frameworks. This gap motivated this thesis, inspired in part by emerging ideas on treating UIs as APIs controlled by intelligent models, an approach highlighted in recent discussions in the AI and HCI communities. Large Language Models (LLMs), in particular, offer a promising interface for driving these intelligent UI adaptations due to their ability to reason over context and user intent. However, current LLMs also come with challenges: latency when used via APIs, potential for hallucinations, a lack of specialization for GUI reasoning tasks or coding like rewriting UIs at runtime. These limitations highlight the need for a structured framework that can integrate intelligent models into adaptive UI systems while managing their weaknesses effectively. Rather than seeing LLMs as perfect agents, this thesis treats them as powerful, context-aware assistants that when properly guided and constrained, can significantly improve how interfaces adapt to users' real-time accessibility needs.
% X -> tweet
% Apple -> vision pro accessibility
% VR hand usage/controller -> difficult in Health related applications
    % Accessiblity needs -> patients
\section{Problem Statement}
Despite increasing attention on accessibility and personalization in user interfaces, most applications remain static and poorly adapted to users' changing contexts or abilities. While current interfaces can adapt in high-level ways such as adjusting to screen size, orientation or theme, these adaptations are mostly cosmetic. They do not address the deeper challenge of understanding and responding to the user's intent, cognitive state, or physical limitations in real time.

Designing truly adaptive UIs remains uncommon, in part due to the technical difficulty of modeling dynamic behavior and the lack of robust, general-purpose frameworks. Furthermore, real-world interaction is often messy and unpredictable: small variations in input such as a mistap, delayed reaction, or environmental noise can result in significantly different needs or outcomes. This non-linearity mirrors the principles of chaos theory, where minor initial differences lead to divergent results, making rule-based UI design fragile.

Addressing this requires systems capable of interpreting nuanced, multimodal signals and adapting accordingly, a task well suited to modern AI techniques such as large language models (LLMs). Yet existing UI frameworks rarely integrate such models in a way that supports real-time, intelligent adaptation, leaving a significant gap between the potential of adaptive interfaces and their current practical application.

\newpage

% Main accessibility problems
\section{Research Objectives}

The primary objective of this research is to design, implement, and validate a modular, multimodal AI-driven framework for dynamic user interface (UI) adaptation, with a strong focus on accessibility and personalisation. The framework aims to move beyond static or one-size-fits-all designs by enabling real-time, context-aware adaptations that respond to diverse user abilities, preferences, and situational needs. It does so by combining multiple input modalities with hybrid reasoning, rule-based logic for predictable low-latency responses, and large language model (LLM) reasoning for complex or ambiguous cases within a unified architecture.  

The overarching goal is to create a system that is both technically extensible and practically deployable across different platforms, while demonstrating tangible accessibility benefits for key user groups, such as motor-impaired, visually impaired, and hands-free users. This will be validated through a proof-of-concept application and simulated evaluation metrics that assess accuracy, effectiveness, and usability impact.

To achieve this goal, the following specific objectives are defined:
\begin{enumerate}
    \item \textbf{Develop a modular, cross-platform architecture:} Design a three-layer framework (Frontend, Input Adapter, and Backend) that can capture, standardise, and process interaction events across Flutter, SwiftUI, and future platforms (e.g., Unity for VR/AR), using a shared JSON-based event and adaptation format.
    \item \textbf{Implement multimodal input fusion:} Support and combine inputs from touch, keyboard, voice, gestures, and mock or future modalities (e.g., gaze tracking), enabling richer context capture for adaptation decisions.
    \item \textbf{Integrate Smart Intent Fusion (SIF):} Develop and integrate a hybrid reasoning engine that fuses profile data, interaction history, and live events to generate targeted adaptations, blending deterministic rules with multi-agent LLM reasoning.
    \item \textbf{Deliver accessibility-focused adaptations:} Provide real-time UI changes such as enlarging targets, enhancing contrast, switching interaction modes, and adjusting layout or navigation flow, driven by the needs of specific user groups.
    \item \textbf{Provide a developer-friendly integration path:} Expose the framework as an SDK with clear contracts, minimal setup, and reusable patterns, allowing developers to retrofit existing apps or build new ones without major rewrites.
    \item \textbf{Evaluate system performance and impact:} Measure adaptation accuracy, latency, and perform a feasibility study to demonstrate the framework’s effectiveness on real-life scenarios as well as identify future improvements.
\end{enumerate}

\newpage 

\section{Thesis Structure}
This section outlines the organization of the thesis to guide the reader through its content and structure.
The thesis is structured as follows:
\begin{itemize}
     \item \textbf{Chapter 1: Introduction} \\
    Introduces the research problem, motivation, and scope of the work. Defines the challenges of static, non-adaptive UIs for accessibility and personalisation, outlines the research objectives, and presents the contributions of the thesis. Concludes with an overview of the thesis structure.
    \item \textbf{Chapter 2: Background and Related Work} \\
    This chapter reviews the state-of-the-art in human-computer interaction (HCI), focusing on adaptive user interfaces, multimodal input processing, and (AI-driven) personalization. It discusses existing frameworks for accessibility, user profile built UIs and limitations of static UIs, and the role of large language models (LLMs) in UI adaptation, positioning the proposed framework’s novelty.
    \item \textbf{Chapter 3: System Design and Architecture} \\
    This chapter describes the overall architecture of the framework, including its three-layer design (Frontend Layer, Input Adapter Layer, and Smart Intent Fusion Backend). Explains the role of each layer, the JSON-based event and adaptation contracts, and how the system captures and processes multimodal inputs. It also introduces the Smart Intent Fusion (SIF) architecture, detailing its hybrid reasoning approach that combines rule-based logic with LLM-driven reasoning.
    \item \textbf{Chapter 4: Smart Intent Fusion (SIF)} \\
    This chapter delves into the specifics of the Smart Intent Fusion (SIF) component, outlining its architecture, the reasoning processes it employs, and how it integrates with the other layers of the framework. It discusses the challenges of fusing diverse input modalities and the strategies employed to ensure accurate and context-aware adaptations as well as dealing with hallucinations from LLMs.
    \item \textbf{Chapter 5: An Adaptive Multimodal GUI Framework using LLMs} \\
    This chapter presents the implementation of a proof-of-concept for the framework, covering the Flutter-based “Adaptive Smart Home Controller” UI with touch, and simulated voice/gesture inputs, the input adapter layer’s JSON contract, and the SIF backend. A backend Flutter-based interface is also introduced for developer insights and debugging.
    \item \textbf{Chapter 6: Evaluation} \\
    This chapter discusses the evaluation setup, scenarios, results, and reflects on the implications and limitations of the findings.
    \item \textbf{Chapter 7: Discussion and Future Work} \\
    This chapter discusses the broader implications for accessibility and HCI, key findings, and comparison with related work. Addresses limitations and proposes future work, including on-device reasoning, integrated visual UI understanding, reinforcement learning from user feedback, and expanding modality support.
    \item \textbf{Chapter 8: Conclusion} \\
    This chapter summarises the thesis contributions and lessons learned from the development process. Reflects on the evaluation results, revisits the research objectives, and outlines how this work can inform future adaptive UI systems.
\end{itemize}

%Chapter 2
\chapter{Related Work}
\section{Multimodal AI in User Interfaces}

User interfaces have evolved far beyond simple point-and-click paradigms. Modern systems increasingly incorporate \textbf{multimodal inputs}, integrating different sensing and communication modalities such as voice or speech commands, pen and touch interaction, eye tracking, and full-body gesture tracking. These technologies aim to create more natural, flexible, and inclusive experiences \cite{10.1145/319382.319398}. Multimodal interfaces aim to mirror human communication, which rarely relies on a single channel, thereby expanding the possibilities for interaction and enabling more robust and adaptive user experiences. 

A fundamental advantage of multimodal interfaces is their potential to improve accessibility and inclusivity. For example, voice commands can be crucial for users with motor impairments, while gaze-based control may assist users who cannot use their hands for input. Meanwhile, gestural input like in VR settings can support contactless control, useful in medical environments, during physical rehabilitation exercises or even astronaut training. Furthermore, the combination of modalities can reduce error rates and cognitive load by providing redundant and complementary channels of communication, improving user satisfaction and performance \cite{10.1145/319382.319398}.

% example staring as trigger if hands dont work...

Historically, the field of multimodal interaction gained attention through early research prototypes combining speech and pen input, which demonstrated that parallel and redundant channels lead to more fluid interaction \cite{10.1145/319382.319398, Oviatt2004}. In recent years, rapid advances in machine learning, especially deep learning, have further enabled real-time recognition of speech, gestures, and gaze on consumer-grade hardware \cite{lugaresi2019mediapipeframeworkbuildingperception, Choudhury2015}.
% pen input paper microsoft?
% eye tracking in apple IOS

\subsection{Pointing Devices and Touch Interfaces}
Before the rise of multimodal and intelligent input modalities, most user interfaces primarily relied on pointing devices such as the mouse, trackball, and touchpad. Fitts' law \cite{Fitts1954} and subsequent pointing models formed the foundation for optimizing target sizes and layouts to reduce pointing time and error rates.  
Touchscreens expanded on these paradigms, introducing direct manipulation, gesture-based scrolling and multi-touch interactions. Touch interfaces, now ubiquitous in smartphones and tablets, benefit from intuitive mappings between finger movements and on-screen actions but also face challenges related to occlusion, precision, and physical fatigue \cite{Wigdor2011BraveNUI}.

Recent research has introduced adaptive techniques, such as "sticky" or magnetic cursor effects, which are already in use today as normal or accessibility features on for example Apple devices. Which help guide the pointer toward interactive elements, reducing effort and error, particularly for users with motor impairments \cite{Cockburn2008Sticky}. These principles guide contemporary accessibility enhancements and encourage advancements in multimodal environments.
% apple cursor on ipad, apple tv stickyness
In this thesis, inspiration is drawn from those adaptive cursor behaviors to inform dynamic UI element resizing and magnetic effects in multimodal contexts, bridging classical pointing interactions with "smart" AI-driven adaptations.

\subsection{Voice-Driven Interfaces}

Voice interaction has become mainstream through virtual assistants like Amazon Alexa, Google Assistant, and Apple’s Siri. These systems utilize automatic speech recognition (ASR) and natural language processing (NLP) to interpret user commands and execute them on the device \cite{Hoy02012018}. Voice input offers hands-free accessibility benefits, particularly useful for users with mobility impairments or in multitasking scenarios. However, it also presents challenges such as interpreting unclear or ambiguous speech, addressing privacy concerns, and maintaining reliability in noisy environments \cite{HCI-076}. Combining voice with complementary modalities such as gestures or gaze tracking has been shown to improve disambiguation and user confidence \cite{s21237825}. Hybrid systems support error correction, implicit confirmation, and increased interaction bandwidth, thereby enhancing overall usability.

\subsection{Gesture and Gaze Integration}

Gesture-based interactions enable users to control interfaces through hand and body movements, providing expressive and intuitive command options. Advances in computer vision techniques and frameworks such as MediaPipe Hands and OpenPose have made real-time gesture recognition feasible on consumer-grade hardware including desktops and mobile devices \cite{lugaresi2019mediapipeframeworkbuildingperception}.  
Eye tracking is another important modality, capturing user attention and intention. It has been used to implement gaze-contingent interfaces where UI elements respond dynamically to where users are looking, reducing physical effort and increasing interaction speed \cite{Duchowski2017}.  
The integration of gesture and gaze input creates a powerful multimodal system, enabling users to point, select, and confirm actions more naturally. Studies indicate that combining these modalities improves error tolerance and enhances overall user experience, especially in contexts where precise manual input is difficult or limited, such as virtual reality environments \cite{7893331, Gazeinteractioninthepost-WIMPworld}.

One of the most influential early systems that explored the practical use of gaze in everyday computing contexts is the GUIDe project by Kumar and Winograd (2007), which demonstrated how gaze could be used as a lightweight augmentation of traditional input methods rather than a full replacement. Their EyePoint technique introduced a refined two-step “look-press-look-release” interaction loop that allowed for highly accurate gaze-based pointing by combining magnification with gaze refinement. Unlike previous systems that relied solely on dwell-based activation, GUIDe’s use of keyboard-assisted targeting reduced false activations (the “Midas Touch” problem) and allowed for precise user control.

They extended this principle to application switching (EyeExposé) and adaptive scrolling (EyeScroll), showing that gaze input could become a viable interaction modality for able-bodied users if the design balanced gaze with explicit control channels like hotkeys or contextual awareness. Importantly, EyeScroll demonstrated adaptive scrolling speeds based on live gaze-driven reading behavior, which is a strong precedent for personalization and multimodal input adaptation. These contributions illustrate how gaze can serve as both a passive signal and an active intent channel, especially when fused with complementary modalities like touch or speech.
% apple vision pro paper

\section{Adaptive GUIs Across Modalities and Platforms}
As user interfaces evolve to support an increasing variety of input modalities and devices, adaptive graphical user interfaces (GUIs) have gained significant attention. Adaptive GUIs are designed to modify their layout, behavior, and appearance dynamically in response to contextual information, user preferences, or real-time interaction patterns. This adaptivity aims to enhance usability, accessibility, and personalization across a wide range of platforms, from traditional desktop systems to mobile devices and immersive virtual reality (VR) environments.

Adaptation can take many forms. For example, a GUI might increase button sizes for users with motor impairments, change color schemes to improve contrast for users with visual impairments, or rearrange content based on the user's task context or dominant hand. On desktop and mobile devices for example, adaptivity has often focused on supporting accessibility and optimizing layouts for different screen sizes or resolutions. Responsive design frameworks, which automatically adjust content and element placement across devices, represent one form of early structural adaptivity. More advanced adaptive systems might learn user habits over time and present solutions based on the users context by for example integrating large language models (LLMs) and advanced AI reasoning \cite{Gajos2008SUPPLE}.

%In contrast, VR and AR systems demand a higher level of adaptivity because of their inherently dynamic and immersive nature. Unlike flat 2D screens, VR environments require spatially distributed UI elements that accommodate 3D space and head or body movement. Adaptive GUIs in these settings can adjust positions, scales, and even interaction modalities in real time to reduce fatigue, improve comfort, and account for tracking inaccuracies \cite{Jerald2015VRBook}. For example, virtual buttons can become larger and more accessible when they are frequently missed or when the system detects shaking or erratic hand movements.

% freeform 
\subsection{VR Health Applications}
Virtual reality offers unique opportunities for healthcare applications by enabling immersive, engaging, and personalized experiences for therapy, rehabilitation, and training \cite{Rizzo2017VRHealthcare}. Adaptive GUIs in VR can tailor the interface to the user’s physical and cognitive abilities, dynamically adjusting element sizes, positions, or interaction techniques to accommodate motor impairments, fatigue, or sensory limitations \cite{Freeman2020AdaptiveVR}.  
For example, VR systems have been developed to assist stroke patients in motor rehabilitation by adjusting the difficulty and interactivity of tasks in real time, providing immediate feedback and encouragement \cite{Laver2017VRStroke}. Adaptive UI elements also support elderly users or individuals with limited VR experience by simplifying controls and enhancing legibility \cite{Slater2020Usability}.  
Despite these benefits, the use of adaptive GUIs in VR healthcare remains a nascent field with ongoing research to optimize adaptation strategies and validate clinical efficacy.
     
\subsection{Challenges in VR UI Design}
Designing effective VR interfaces poses several challenges distinct from traditional 2D GUIs. First of all, the 3D environment introduces spatial complexity requiring new paradigms for navigation and interaction \cite{Bowman20023DUI}.  
Secondly, user comfort is critical, as poorly designed VR UIs can induce motion sickness or problems with fatigue and cognitive load \cite{LaViola2000VRSimulatorSickness}. Adaptive GUIs must consider these factors by adjusting visual density, interaction distances, and input modalities.  
Third, hardware variability and tracking inaccuracies complicate consistent input interpretation, especially for hand tracking and gaze estimation \cite{Jerald2015VRBook}. Which heavily depends on the hardware precision and its algorithms. 
Finally, user diversity in terms of physical abilities, experience with VR, and cognitive load demands highly flexible and personalized UI adaptation mechanisms \cite{Vasiljevic2017AdaptiveVRAccessibility}.  
Addressing these challenges requires integrating real-time sensor data, user profiling, and AI-driven adaptation logic, motivating the development of multimodal AI-driven frameworks like the one proposed in this thesis.

\section{Classical Adaptive UI Techniques}
Before the widespread use of artificial intelligence and machine learning in dynamic interfaces, adaptive graphical user interfaces (GUIs) primarily relied on deterministic, rule-based strategies. These classical approaches, while limited in personalization and flexibility, were instrumental in shaping the early landscape of user interface adaptation. They focused on fixed logic, device-specific configurations, and contextual parameters defined explicitly by designers or developers. Although static by today’s standards, these techniques addressed fundamental issues of usability, accessibility, and device heterogeneity.

One of the most foundational classical techniques is responsive layout design, which allows a user interface to adjust its layout and elements based on screen size, orientation, and device type. This approach is especially prominent in web and mobile design, where frameworks like CSS media queries or constraint-based layouts dynamically reposition and resize UI components to preserve usability across different platforms. While responsive design improves accessibility and device compatibility, it does not adapt to individual user behavior or preferences the adaptations are based solely on environmental conditions like screen dimensions or device type.

Another key classical technique is context-aware UI adaptation, where the interface adjusts based on predefined environmental or situational variables such as location, time of day, network connectivity, or battery level. For instance, a mobile app might switch to a dark theme automatically at night or reduce update frequency when on a metered connection. These adaptations are typically triggered by fixed if-then rules rather than probabilistic models, and although they improve relevance and efficiency, they lack the ability to learn from user interaction patterns over time.

Additionally, user role-based adaptations were sometimes employed in enterprise applications or educational platforms. For example, an interface might present different levels of information depending on whether the user is an administrator, a student, or a guest. These adaptations were predetermined by role or permissions, not by user behavior or inferred intent.

\subsection{Responsive Layouts}
Responsive design emerged as a critical approach to address the rapid increase of different screen sizes and devices, particularly with the rise of smartphones and tablets. Instead of creating separate fixed interfaces for each device, responsive layouts allow a single design to automatically adjust its elements to fit various screen dimensions and orientations based on core techniques like flexible grids, media queries, and adaptive content strategies.  
Those enable dynamic resizing and rearrangement of UI elements \cite{Marcotte2010Responsive}. In mobile development, declarative UI frameworks such as Flutter and SwiftUI adopt similar principles, using constraints and reactive layouts that adapt hierarchically to screen changes.  
While originally developed to support multiple screen sizes, responsive layouts also improve accessibility by allowing users to zoom or scale interface components without breaking layout integrity. Despite their flexibility, traditional responsive designs are typically static in behavior they do not react to real-time user behavior or contextual signals beyond the device dimensions.

\subsection{Context-Aware Design}
Context-aware interfaces extend the notion of adaptation by responding to more complex environmental and user-specific factors. Originating in ubiquitous and pervasive computing research, context-aware design involves sensing parameters such as location, time of day, user activity, nearby devices, or even physiological signals to modify interface behavior \cite{Schilit1994ContextAware, Dey2001Context}. For example, a context-aware mobile application might switch to a dark theme automatically at night or suggest different UI shortcuts when the user is driving versus sitting at a desk. In smart home systems, interfaces may change based on proximity sensors or room occupancy data.  
Some adaptive systems also integrate user profiles and long-term behavior patterns. For example, the SUPPLE system automatically generates personalized interfaces optimized for a user's specific motor abilities and preferences by solving optimization problems over interface layouts with the effect of facilitating faster access and lower error \cite{Gajos2008SUPPLE}. Such rule-based or optimization-based approaches set the stage for later, more sophisticated AI-driven adaptations by highlighting the importance of context and user modeling.

While classical techniques like responsive layouts and context-aware design lack the semantic understanding and reasoning capabilities of modern AI systems, they provide robustness and predictability. They form a crucial baseline against which the benefits of AI-enhanced adaptations can be evaluated. Furthermore, they remain relevant in many production systems due to their lower computational cost and easier predictability for developers and designers.

\section{Programmable UIs}
Traditionally, user interfaces have been treated as static artifacts, closely tied to fixed layouts and hardcoded interaction patterns. However, recent advances in computational understanding of user interfaces propose a shift towards making UIs "programmable" that is, representing them as dynamic, semantically rich structures that can be analyzed, modified, and generated automatically. This paradigm enables interfaces to become more flexible and adaptable, allowing developers and even AI systems to reason about and transform UI elements in response to changing contexts, user needs, or platform constraints. Such a programmable layer is essential for realizing fully adaptive, AI-driven interfaces that can personalize experiences and improve accessibility in real time.

\subsection{Reflow}
 
A notable example of pixel-based UI adaptation is Reflow, introduced in Chapter 7 of Wu's dissertation on computational understanding of user interfaces \cite{Wu2024}. Reflow proposes a system that automatically refines and optimizes touch interactions in mobile applications using only pixel-level information, without requiring access to the underlying application code or view hierarchy like a widget tree.
The system operates by first detecting UI elements from a screenshot using machine learning-based pixel analysis. It then refines the layout based on a user-specific spatial difficulty map, which identifies the difficult-to-access areas of the screen, derived from calibration tasks that capture individual motor abilities and preferences. Finally, Reflow re-renders a modified version of the UI with optimized element positions and sizes, ensuring that critical interactive components are easier to reach and select.

\newpage

Reflow exemplifies how computational understanding of UIs at the pixel level can enable automated personalization and accessibility enhancements even in closed-source or inflexible applications. It illustrates a promising direction for future adaptive systems that aim to provide user-specific improvements without requiring cooperation from original app developers or extensive code modifications. This aligns directly with the goals of AI-driven UI frameworks focused on dynamic, user-centered adaptation. Furtermore, the user study showed an average increase of 9\% in interaction speed as well as improved interaction speeds by up to 17\% \cite{Wu2024}.

\subsection{UICoder}
A further advancement in making user interfaces programmable is presented through UICoder, as described in Chapter 8 of Wu’s dissertation on computational understanding of user interfaces \cite{Wu2024}. While earlier systems such as Reflow focused on pixel-level adaptations of existing applications, UICoder tackles the challenge of generating high-quality UI code directly from textual descriptions using large language models (LLMs).
UICoder addresses two major limitations of previous code-generation approaches for UIs: the scarcity of high-quality, self-contained UI code in existing datasets and the difficulty LLMs face in incorporating visual or spatial feedback into their training. Rather than relying on manually curated or external datasets, UICoder introduces an automated method to iteratively generate, filter, and refine synthetic UI code datasets.

The system begins by prompting an existing LLM to generate large collections of UI code samples (specifically SwiftUI, Apple's coding framework) from textual descriptions. These outputs are aggressively filtered and scored using compilers (to ensure syntactic correctness) and vision-language models (to assess visual relevance), producing a refined dataset for further finetuning. By iteratively repeating this cycle generation, filtering, and finetuning UICoder progressively learns to produce syntactically correct and visually coherent UI code.
Derived from the StarCoder family (which lacked extensive Swift training data), UICoder ultimately generated around one million SwiftUI programs over five iterations. Despite these constraints, it significantly outperformed all open-source baselines and approached the performance of larger proprietary models in both automated and human evaluations.
By leveraging automated feedback instead of costly human annotations, UICoder demonstrates a scalable approach to training LLMs for UI code generation. This method shows how generative models combined with programmatic refinement loops can enable on-demand creation of high-quality, personalized UIs directly supporting visions of adaptive and dynamically generated interfaces.

UICoder exemplifies the shift from static UI codebases toward dynamic, generatively defined interfaces. In the context of AI-driven adaptive GUIs, such a capability enables systems to not only adjust existing layouts but also generate entirely new interface code on demand, tailored to specific user preferences or device contexts. This directly supports the vision of self-adaptive and user-personalized UI frameworks proposed in this thesis.
% paper MAties

\subsection{User Interface Adaptation using
Reinforcement Learning}
Recent advances in programmable and adaptive user interfaces have explored the integration of machine learning techniques particularly Reinforcement Learning (RL) to support real-time personalization and dynamic behavior. A notable example is the doctoral work by Gaspar-Figueiredo (2023), which proposes a UI adaptation framework that leverages RL in conjunction with physiological data to enhance user experience (UX)~\cite{gaspar2023learning}.

In this framework, RL is used to determine optimal UI adaptations (e.g., layout or content adjustments) by continuously interacting with the user and optimizing long-term reward signals. What sets this work apart is the use of physiological signals such as eye tracking, EEG, or other biosignals as objective measures of user response. This addresses a key limitation of traditional evaluation techniques that rely on subjective self-reporting, which can introduce bias or fail to capture moment-to-moment changes in experience.

The system learns from users' physiological reactions and interaction behaviors to determine which adaptations are effective in improving usability and engagement. Over time, the interface becomes more personalized and context-aware, reacting not only to explicit input but also to subtle cues from the user’s cognitive or emotional state.
This approach exemplifies a new direction in programmable UIs: systems that are not only defined by abstract models (e.g., UIML or USiXML) but are also capable of learning and evolving through interaction. By combining adaptive logic, ML-driven decision-making, and, biosignal-based feedback, such frameworks could serve as the basis for next-generation interfaces especially in applications involving accessibility, cognitive load, or health and wellness.

%https://arxiv.org/pdf/2312.07216

\section{User Profile built UIs}
The increasing diversity of devices and user contexts has driven research into creating adaptable user interfaces that are both device-independent and user-centered. A significant challenge arises in balancing generalization so an interface works across multiple devices and personalization so it aligns with the unique preferences and capabilities of individual users.

To address this, Luyten et al.~\cite{luyten2005profile} proposed a framework that combines high-level XML-based user interface description languages (UIDLs), particularly UIML, with MPEG-21 Part 7 (Digital Item Adaptation) user profiles. UIML (User Interface Markup Language) allows designers to abstract the UI from concrete implementations, enabling interfaces to be rendered differently depending on device constraints. For example, a single abstract element like “choice from a range” can map to different widgets (slider, list, or text input) depending on the target platform. In this approach, MPEG-21 user profiles capture individual user preferences and requirements (e.g., accessibility needs, preferred interaction styles), which can then dynamically guide the adaptation of the UI described in UIML. This enables the generation of multi-device, personalized interfaces that are both broadly deployable and tailored to specific user needs.

An implemented prototype demonstrated how combining UIML and MPEG-21-based user profiles allows for seamless adaptation of UI layouts and interactions while minimizing design effort. By leveraging abstract UI definitions and structured user profiles, this method enhances both accessibility and usability across diverse platforms, making interfaces more "granny-proof" and inclusive. This user-centric adaptation model supports the vision of highly personalized digital experiences without sacrificing cross-device compatibility, contributing an important step toward universal, accessible, and adaptable user interfaces.

Early profile-based UI adaptation systems were often limited by the rigidity of XML-based markup languages and the static nature of their user models. Profiles were typically loaded once and assumed relatively stable user needs, which limited the ability to adapt interfaces dynamically over time or in real-time scenarios.

However, these systems laid foundational groundwork for modern AI-augmented adaptation layers. For instance, where MPEG-21 profiles might have been manually filled or gathered via forms, today’s systems can infer similar parameters from behavioral data using machine learning. Similarly, abstraction principles from UIML still resonate in declarative UI frameworks like Flutter, SwiftUI, or React Native, which separate layout logic from presentation and enable device-responsive rendering.

These classical approaches remain valuable not only for historical understanding but also as a reminder of the importance of modularity, abstraction, and structured user modeling in UI design principles that continue to influence the development of multimodal, intelligent frontends today.

\subsection{XML-Based Runtime UI Systems}

One early precursor to programmable and adaptive UI systems was the use of XML-based runtime user interface description languages, especially for resource-constrained mobile and embedded systems. A notable example is the work by Luyten and Coninx (2001), who proposed a method that leverages XML for describing UI components and Java for rendering them on mobile devices such as PDAs. Their system allowed interfaces to be dynamically serialized, transmitted, and rendered on client devices based on contextual variables such as the user's role, device capabilities, and preferences. For example, the interface for controlling a projector would differ depending on whether the user is a professor or a technician, enabling personalized task-specific controls.

This work introduced several important ideas that prefigure modern programmable and adaptive UIs. First, it treated UIs as dynamic data rather than static views, enabling runtime generation and adaptation. Second, it used constraint-based filtering mechanisms to tailor UI components based on context. Third, it demonstrated that a separation between the UI's description (in XML) and execution (in Java) could support modular, reusable interface logic.

Although this approach predates current AI-powered frameworks, it exemplifies early efforts toward what is now called adaptive or context-aware UI. Its emphasis on platform-independence, modularity, and runtime flexibility directly anticipates features found in modern frameworks like Flutter, React, and Unity UI especially when extended with AI-driven mediation and multimodal inputs.

\section{Multimodal Fusion and Input Event Modeling}
As user interfaces become increasingly multimodal, systems must handle diverse streams of input signals in a coherent way. Multimodal fusion refers to the process of integrating these different input modalities such as voice, touch, gaze, and gestures into unified semantic commands.  This enables interfaces to interpret combined user inputs more naturally and contextually, mirroring how humans often combine speech and gestures in daily communication.

Closely related, input event modeling abstracts low-level raw data (like finger coordinates, gaze points, or audio waveforms) into higher-level representations of user intent (such as “select”, “scroll”, or “confirm”). This abstraction layer serves as an essential bridge between noisy sensor data and actionable interface responses. By modeling input events at different levels of granularity from raw physical movements to semantic-level intents systems can reason about user behavior, handle ambiguities, and provide more robust feedback.

For example, in a health application, simultaneous voice and gaze inputs might be fused to allow a patient to say “yes” while looking at a confirm button, providing redundancy that improves reliability for users with motor or speech impairments. Similarly, in VR or AR, interpreting combined hand gestures and head movements enables more precise object manipulation and scene navigation, which would be challenging with single-modality input alone.

\subsection{Fusion Architectures}
Several architectures have been proposed to integrate multimodal inputs effectively. Early fusion approaches combine raw input data at a low level, for example merging voice and gesture signals before any individual interpretation occurs. While this can enable richer context awareness, it often requires precise synchronization and robust signal alignment, which can be technically challenging.
On the other hand, late fusion architectures process each modality independently to obtain intermediate interpretations (such as recognized words or detected hand poses), and then merge these at a semantic level. This approach tends to be more modular and easier to maintain, as each input channel can be improved or swapped without affecting the others.

Hybrid fusion combines elements of both early and late strategies, allowing certain low-level signals to be shared while keeping higher-level interpretation pipelines separate \cite{Nielsen2021, Oviatt1999}.
Choosing an appropriate fusion architecture is critical for ensuring fluid and error-tolerant interactions. For example, in an adaptive health app, fusing gaze and speech can enable users with motor impairments to confirm commands more easily. Similarly, in AR/VR settings, combining hand tracking with eye gaze supports natural object selection and manipulation.

\subsection{Event Abstraction Models}
Once input signals are fused, systems must convert them into abstracted events that represent user intent rather than raw movements or spoken words. Event abstraction models define a hierarchy of events, from primitive input (e.g., "finger tap at position x,y") to higher-level semantic commands (e.g., "confirm selection", "scroll left", or "open menu").
This abstraction not only simplifies the logic needed to respond to different input combinations but also improves the system’s adaptability across platforms and devices. By mapping diverse physical actions to a common set of abstract commands, a single system can support a broad range of user abilities and contexts.

Recent work in multimodal interaction design has emphasized the importance of flexible and extensible event models that can incorporate new modalities without extensive re-engineering \cite{Bolt1980, Turk2014}. Moreover, integrating AI-driven models, such as large language models or gesture classifiers, can further enrich the abstraction process by inferring user intentions from subtle or ambiguous input cues.
Together, fusion architectures and event abstraction models form a foundation for building robust, adaptive, and inclusive multimodal user interfaces. As systems continue to evolve, these techniques will play a crucial role in creating personalized and accessible experiences across devices and environments.


A notable example of semantic event abstraction in real-world interfaces is the work of Dixon et al. \cite{Dixon}, who implemented a general-purpose, target-aware pointing enhancement based on the Bubble Cursor. Their system uses pixel-level reverse engineering (via Prefab) to identify interface components and overlay interaction semantics onto them, allowing the system to interpret raw pointer movement as intent to interact with semantically meaningful targets even when underlying applications do not expose accessibility metadata. This layered approach of separating visual identification from interaction intent closely mirrors the goals of input event modeling in multimodal systems, especially in contexts where event boundaries are ambiguous or where UI elements are rendered outside standard toolkits.

\section{LLMs as UI Controllers}
Recent advances in large language models (LLMs) have opened new opportunities for bridging natural language and user interfaces. Traditionally, UI control has relied on explicitly designed event handlers and rigid APIs, requiring precise user input or structured interaction patterns. LLMs, by contrast, enable more flexible, high-level interactions that resemble natural human communication, which is especially promising for non-expert users or contexts where accessibility is crucial.

By leveraging LLMs' powerful capabilities in understanding and generating natural language, it becomes possible to control user interfaces using text or voice instructions without needing predefined UI-specific commands. For example, a user could ask an application to "show me my upcoming appointments and move the next one to next week," and an LLM-based controller can parse this instruction, map it to the appropriate UI actions, and execute them seamlessly.

This paradigm allows user interfaces to function more like intelligent agents rather than static interaction surfaces, reducing cognitive load and lowering the technical barrier for interaction. Furthermore, LLM-driven UI controllers can adapt to user-specific phrasing and preferences over time, learning from previous interactions to provide more personalized and efficient support.

\subsection{Turning UIs into APIs}
One promising approach to LLM-driven interfaces is conceptualizing user interfaces as implicit APIs. Instead of interacting with the UI through direct manipulation (e.g., clicking buttons, dragging elements), the UI's functionalities are abstracted into callable actions through an adapter layer that can be invoked through language.
This "UI-as-API" perspective treats every interactive element and functionality as an endpoint or command that can be described and triggered textually. As demonstrated in recent research on program synthesis and UI automation like the UICoder referenced earlier \cite{Wu2024, chen2023programmaticui}, LLMs can be trained to translate high-level instructions into structured API calls or internal code representations. This enables a two-layered interaction model: the LLM interprets free-form user instructions and maps them onto the UI's abstracted actions, which are then executed to update the visible interface.

A significant advantage of this model is that it can help unify interaction modalities. Whether a command is given via voice, text, or even gesture-based language input, it is ultimately funneled into the same set of abstracted API calls. This makes interfaces more robust to modality switching and enhances accessibility.
Moreover, turning UIs into APIs facilitates automation and integration with external services, enabling systems to not only react to individual user commands but also orchestrate multi-step tasks and workflows automatically. While challenges remain such as handling ambiguous instructions or ensuring robust error handling and LLM hallucinations, this direction shows strong potential for creating more adaptive, intelligent, and user-centered interfaces.

\subsection{Agents}
The rise of LLM-based agents represents a powerful evolution of user interface control beyond simple command mapping. Unlike static UI controllers, agents leverage LLMs to autonomously interpret, plan, and execute sequences of actions on behalf of users, effectively turning the interface into an intelligent collaborator rather than a passive tool.
These agents can reason about user goals, maintain conversational context, and dynamically choose the best sequence of interface actions to fulfill complex or vague instructions. For example, an agent might respond to "Help me prepare for my upcoming trip" by checking the user's calendar, suggesting packing lists, booking transportation, and even setting reminders all without requiring the user to explicitly navigate each step.

Recent systems such as OpenAI’s GPT-based function calling, AutoGPT, or tools like Microsoft’s Copilot and Google’s Duet illustrate how agents can operate over complex applications, combining high-level reasoning with programmatic UI actions. In research, approaches like ReAct (Reasoning and Acting) frameworks show how agents can chain thought processes ("chain-of-thought") and API-level actions in iterative loops, enabling them to verify outcomes and adapt their behavior \cite{yao2022react}. Moreover, LLM-based agents can incorporate user feedback continuously to refine their behavior, creating highly personalized assistants that align closely with individual preferences and working styles.

To further enhance their adaptability, modern agents increasingly integrate environment modeling and multimodal perception, enabling them to not only parse language but also interpret visual interfaces, sensor data, and user gestures. Frameworks like SeeAct and ViperGPT have demonstrated how combining vision-language models with action planning allows agents to operate UIs from visual input alone clicking buttons, reading menus, or navigating unfamiliar applications much like a human would. This opens doors to agent-driven interfaces for accessibility scenarios (e.g., voice or gaze-controlled UIs), remote control of complex software, or even autonomous use of standard desktop/web applications. 

\section{Health and Accessibility Applications}
 % current health apps
Health and accessibility applications represent some of the most impactful and socially significant domains for adaptive and intelligent user interfaces. As populations age and chronic health conditions become more prevalent, digital health tools are increasingly critical for supporting independence, self-management, and personalized care. Similarly, accessibility-focused interfaces help reduce barriers for users with diverse abilities, ensuring equitable access to technology.
Modern health applications leverage multimodal inputs and adaptive interfaces to create more engaging and supportive experiences. For example, apps for physical rehabilitation frequently combine motion tracking (via cameras or wearable sensors) with adaptive visual feedback to guide patients through exercises safely and effectively. Examples include tools like Kaia Health for musculoskeletal therapy and Reflexion for cognitive and physical rehabilitation, which adjust exercise difficulty and feedback based on real-time performance data.

In the mental health domain, conversational agents and virtual coaches are becoming increasingly popular. Applications like Woebot or Wysa utilize natural language processing to provide immediate, conversational mental health support. By continuously adapting their conversational style and recommendations to the user's emotional state and progress, these systems illustrate the power of dynamic, user-centered design. Research has shown that such adaptive, agent-based interactions can improve adherence and patient outcomes \cite{fitzpatrick2017delivering}.
Accessibility applications also demonstrate the importance of personalized, context-aware interfaces. For users with visual impairments, screen readers and AI-powered image descriptions enable rich content access. Gaze-based or switch-based interaction systems empower users with severe motor disabilities to control complex interfaces using minimal input. Projects like Apple's VoiceOver and Microsoft's Seeing AI show how integrating multimodal AI into accessibility solutions can drastically improve daily usability and independence.

Furthermore, combining health and accessibility perspectives opens opportunities for fully personalized assistive systems. For instance, an intelligent multimodal interface could dynamically adjust text sizes and contrast for a user with low vision while also simplifying navigation for cognitive accessibility and providing voice guidance tailored to the user's speech patterns or preferences. Looking toward future developments, advances in virtual reality (VR) and immersive technologies can further enhance these assistive systems. In VR-based rehabilitation, for example, adaptive multimodal interfaces can create engaging, safe, and highly individualized therapy environments, dynamically adjusting exercise difficulty, feedback modalities, and visual cues to match each patient’s needs and progress. Such systems show promise for applications ranging from motor skill recovery after stroke to anxiety reduction and pain management.

%Chapter 3
\chapter{System Design and Architecture}

\section{Introduction to System Design}
The adaptive multimodal GUI framework developed in this thesis is designed to deliver personalised, accessibility-focused interface adaptations in real time. It builds on the idea that different users have different needs, and that these needs can change depending on context, device, and input modality. By combining multiple input channels such as touch, voice, and gestures with AI-driven reasoning, the framework can adapt interfaces in a way that is both responsive and context-aware.

The architecture follows a three-layer design. The frontend layer renders the user interface, captures user interactions, and applies adaptations as instructed by the backend. The input adapter layer standardises events across modalities into a common JSON schema, ensuring consistent processing regardless of their origin. The backend layer processes these events using Smart Intent Fusion (SIF), which combines rule-based logic with multi-agent LLM reasoning to generate targeted adaptations.
This layered approach was chosen to ensure modularity, scalability, and extensibility for future modalities. By separating concerns across layers, the framework can easily integrate new input methods or adapt to different platforms without significant rework. Each layer can evolve independently, allowing for targeted improvements and innovations.

This chapter presents the framework at a conceptual level, focusing on its architecture, data flow, goals and key design principles. Detailed implementation aspects, such as widget properties, SIF, API routes, or database queries, are deferred to Chapters 4 and 5.

\section{Overview of the System Architecture}
The framework follows a three-layer architecture that was chosen and designed to be modular, scalable, and adaptable to diverse accessibility needs. Each layer has a clearly defined role and communicates with the others through a common JSON-based event and adaptation format. This separation of responsibilities makes it possible to extend or replace individual components without disrupting the rest of the system, and ensures that adaptations can be applied consistently across platforms and modalities.

At the top, the frontend layer is responsible for rendering the user interface, capturing interactions, and applying adaptations received from the backend. The input adapter layer sits between the frontend and backend, converting raw interaction data from multiple modalities into the shared JSON schema so that all events are processed in the same way. The backend layer implements the Smart Intent Fusion (SIF) process, combining event data, user profiles, and recent interaction history to produce personalised adaptation actions. The backend can rely on both deterministic rules and multi-agent LLM reasoning, described in detail in Chapter 4. 
The system is built around a feedback loop. Each user interaction is captured, processed, and logged along with the resulting adaptation. This log contributes to the user’s profile and informs future adaptation decisions, allowing the interface to become progressively more personalised over time.

To illustrate this flow in practice, consider a motor-impaired user attempting to press a “Lock” button. If the tap misses its target, the event is sent through the adapter, recognised as a miss-tap, and forwarded to the backend. SIF responds by generating an adaptation that increases the size and spacing of the button. The frontend receives this instruction and animates the change in real time. For a hands-free user, the process might begin with a spoken command to “turn on the lamp.” The voice input is captured, standardised, and sent to the backend, which returns instructions to switch the UI into voice mode and activate the lamp control.

\emph{[Diagram Placeholder]}

The high-level diagram of this flow (Figure X) shows the event path from the frontend to the backend and back again, using WebSocket connections for low-latency adaptations and HTTP for profile management and batch operations. Extensibility points are built into the architecture so that new modalities, reasoning methods, or frontend platforms can be introduced without altering the core event–adaptation pipeline.

\textbf{Key Design Principles:}
\begin{itemize}
\item \textbf{Modularity}: Layers can be swapped or upgraded independently.
\item \textbf{Scalability}: Async processing and MongoDB indexing handle high interaction volumes.
\item \textbf{Generalizability}: Platform-agnostic design enables deployment in domains from smart homes to healthcare.
\item \textbf{Accessibility Focus}: All adaptations are guided by WCAG 2.1 and target motor-impaired, visually impaired, and hands-free users.
\end{itemize}

The most challenging aspect of this design is ensuring seamless communication between layers while maintaining low latency for real-time adaptations, as well as carefully integrating LLMs as a central component for processing and understanding user intents. This will bring challenges on its own such as hallucinations or misinterpretations of user input. Making sure the LLM accurately captures user intent and context is crucial for effective adaptations. Prompt engineering and careful design like fallback options will be essential to mitigate these issues.

\subsection{Accessibility Focus and Target User Groups}
While the framework is general enough to support a wide range of adaptive UI scenarios, its design in this thesis is intentionally centred on three key user groups: motor-impaired, visually impaired, and hands-free users. These groups were chosen because they represent distinct accessibility challenges that can be addressed effectively through multimodal interaction and real-time adaptation.

\paragraph{Motor-Impaired Users}
Motor impairments can make precise pointing, dragging, or rapid tapping difficult. For these users, repeated failed attempts at interacting with a UI element (e.g., a small button) can lead to frustration and reduced task completion rates. The framework addresses this by detecting such patterns and applying adaptations like enlarging target sizes, increasing hitbox areas, or enabling alternative input modes such as voice or gesture. These adaptations can be applied temporarily (for the current session) or persistently (as part of the user’s profile), depending on the severity and frequency of the interaction difficulties.

\paragraph{Visually Impaired Users}
Users with partial vision loss or low contrast sensitivity often struggle with visual elements that rely on small fonts, thin borders, or subtle colour differences. Here, multimodal fusion is key: the system can combine direct interaction data with profile information to infer when visual feedback is insufficient, then automatically apply high-contrast themes, larger font sizes, or additional visual cues such as highlighted borders. Because the framework operates in real time, these adaptations can be triggered dynamically when needed, rather than requiring a static accessibility mode to be enabled manually.

\paragraph{Hands-Free Users}
Some scenarios, whether due to physical impairment, temporary constraints (e.g., carrying items), or task context (e.g., sterile environments) require interfaces that can be operated without direct touch. For these users, the framework supports voice, gesture, and potentially gaze-based interactions. Multimodal fusion allows these input channels to work in combination, reducing ambiguity and improving accuracy. For example, a voice command like “turn on the lamp” can be paired with a pointing gesture to confirm the target device, speeding up the interaction and avoiding false positives.
    
\paragraph{Why These Groups?}
These three categories were selected because they cover a spectrum of accessibility challenges that benefit significantly from adaptive, multimodal design: precision (motor), perception (visual), and modality flexibility (hands-free). Each presents distinct technical requirements for sensing, reasoning, and adapting the interface, making them strong drivers for evaluating the framework’s capabilities. By focusing on these groups, the system demonstrates how a single architectural approach can address varied accessibility needs while remaining extensible to other user categories in future work.

% class diagrams
\section{Frontend Layer: UI Design and Interaction}
The frontend layer is the user-facing component of the framework, responsible for rendering an adaptive and personalised interface that adjusts dynamically to user needs. Designed for cross-platform deployment, it captures multimodal interactions such as touch, keyboard, voice, or gestures and applies adaptations returned from the backend in real time.

In this thesis, the frontend is demonstrated through the Adaptive Smart Home Controller, a deliberately chosen example that offers both familiarity and a variety of interaction types. The smart home context provides a set of clear, relatable tasks such as switching on a light or adjusting a thermostat that can be adapted for users with motor, visual, or input-related impairments. This use case also highlights the framework’s potential for deployment in other domains where accessibility is a priority.

\subsection{Interface Elements}
The frontend incorporates a set of core UI elements chosen to balance simplicity with the ability to demonstrate a wide range of adaptations. Together, they form a complete interface that could plausibly be used in real-world scenarios, while remaining portable across platforms. Conceptually, these elements can be grouped into three categories:
\begin{enumerate}
    \item \textbf{Action controls:} for example buttons or sliders, which allow users to manipulate device states or settings.
    \item \textbf{Information displays:} such as text labels, which convey device status or contextual feedback.
    \item \textbf{Organisational elements:} such as cards or list views, which group related controls for clarity and ease of navigation.
\end{enumerate}
These categories were selected because they cover the most common interaction and accessibility challenges. Action controls benefit from size and spacing adjustments for motor-impaired users, information displays can be adapted with text resizing or high-contrast themes for visually impaired users, and organisational elements help reduce cognitive load by presenting related controls together.

\subsection{Adaptation Levels}
For this thesis and its implementation, adaptations in the frontend can be understood at three conceptual levels:
\begin{enumerate}
    \item \textbf{UI-Level Adaptations:} which modify the appearance of visible elements to improve clarity or ease of interaction.
    \item \textbf{Geometry-Level Adaptations:} which adjust layout and spacing to reduce input errors or simplify navigation.
    \item \textbf{Input-Level Adaptations:} which alter the way users interact with the interface, for example by switching to a voice-driven mode.
\end{enumerate}
While the precise mechanics of how these adaptations are implemented, triggered and applied are described in Chapter 5, the design principle remains the same: each change should be both functional and clearly communicated to the user, reinforcing trust and improving interaction efficiency. These high-level principles guide the implementation of the SIF framework, ensuring that adaptations are user-centered and context-aware.
% Diagram?

\section{Input Adapter Layer: Multimodal Input Processing}
The Input Adapter Layer acts as the middleware between the user-facing frontend and the reasoning backend, it takes care of bidirectional communication between both layers. Its main role is to take raw interaction data from any modality such as touch, keyboard, voice, or gestures and transform it into a standardised format that the backend can process consistently and vice versa for the frontend. By separating input capture from input interpretation, this layer allows the rest of the system to operate independently of how the input was generated, making it easier to add new modalities in the future.

Central to this layer is the \textbf{JSON Event Contract}, architecturally this is exposed through the \texttt{AdaptiveUI\\Adapter} class, which defines the public interface for sending interaction data to the backend. The primary entry point is the \texttt{sendEvent(Event eventData)} method. This method accepts an \texttt{Event} object the framework’s internal representation of a user interaction, and is responsible for enriching it with contextual information before forwarding it to the backend.

The \texttt{Event} class acts as the architectural contract for interaction data. While its exact schema is defined in the implementation (Chapter 5), at the design level it contains:
\begin{itemize}
    \item \textbf{Event type:}  the category of interaction, such as a tap, miss-tap, slider miss, or voice command.
    \item \textbf{User ID:} a unique identifier for the user interacting with the system.
    \item \textbf{Timestamp:} the time when the event occurred.
    \item \textbf{Source modality:} the origin of the event (touch, keyboard, voice, gesture, etc.).
    \item \textbf{Target element:} the UI element or control associated with the event.
    \item \textbf{Coordinates:} spatial information where applicable, for example the location of a tap.
    \item \textbf{Confidence score:} a value indicating the certainty of the detected intent.
    \item \textbf{Metadata:} optional context such as UI element type (e.g., button, slider) and more.
\end{itemize}
By enforcing this structure, the adapter guarantees that all events passed to the backend follow the same rules, whether they come from Flutter, SwiftUI, or a future Unity-based frontend.

The adapter also handles profile verification and creation before sending events. This step ensures that the backend always has a corresponding user profile for contextual reasoning. If the profile does not exist, the adapter triggers its creation using default accessibility settings or a new user profile. These profiles are also bound by a JSON contract to ensure consistency, this design is described in chapter 4. Furthermore, for low-latency adaptation feedback, events are transmitted via a persistent WebSocket connection, while non-real-time operations such as profile updates, HTTP endpoints are used instead.

Lastly, the adapter manages adaptations returned by the SIF backend layer by listening to the WebSocket channel. When the backend sends adaptation instructions which is represented in its own JSON contract (in more detail described later), the adapter maps it to an internal representation based off of the JSON contract that can be easily processed by the frontend. This Adapter pattern design is deliberately chosen and platform-agnostic. Although the current implementation is in Dart for Flutter, the architectural pattern can be replicated in any language or platform by implementing the same \texttt{sendEvent()} contract and adhering to the same event object structure. This separation means that adding a new modality, such as eye tracking or brain-computer interfaces, only requires implementing the modality’s input capture and mapping it to the existing \texttt{Event} format, no changes are needed to the backend or reasoning logic.
%Diagram?

% \item \textbf{Description:} The diagram illustrates the flow of multimodal inputs through the Input Adapter Layer.
%     \begin{itemize}
%         \item \textbf{Inputs:} Icons for touch (finger), keyboard (key), voice (mic), gesture (hand), and future modalities (eye).
%         \item \textbf{Processing:} A block for \texttt{AdaptiveUIAdapter}, showing input standardization into JSON contract format.
%         \item \textbf{Outputs:} Arrows to SIF Backend Layer via WebSocket/HTTP, labeled ``Standardized JSON Events''.
%         \item \textbf{Profile Check:} Connection to MongoDB via \texttt{GET/POST /profile}, labeled ``Profile Validation''.
%     \end{itemize}
%     \item \textbf{Purpose:} Visualizes the layer’s role as a modular bridge between the frontend and backend, emphasizing its extensibility and accessibility focus.
% \end{itemize}
    
\section{SIF Backend Layer: Smart Intent Fusion (SIF)}

The SIF backend layer is the reasoning core of the framework, responsible for turning raw user interactions and contextual information into targeted UI adaptations. Operating behind the input adapter, it receives events in the standard JSON contract format and turns them into an internal structured representation (\texttt{Event} object), combines them with the user’s profile and recent interaction history. This forms the basis for the LLM prompts, next it determines the most appropriate changes to apply to the interface using LLM reasoning.

From an architectural perspective, this layer has two main responsibilities:
\begin{itemize}
    \item \textbf{Processing and interpretation:} validating incoming events, interpreting their intent, and prioritising them according to context.
    \item \textbf{Adaptation generation:} producing a set of structured adaptation actions in a JSON contract that the frontend can apply directly.
\end{itemize}

The backend is designed to support multiple reasoning strategies. In its current form, it combines deterministic rules with a multi-agent LLM process called Smart Intent Fusion. Rules handle straightforward accessibility needs; for example, increasing button size after a miss-tap, while the LLM process enables more context-aware adaptations that consider multiple factors simultaneously.

Although the underlying logic is covered in detail in Chapter 4, the architectural position of this layer is central in this framework: it serves as the decision-making hub, fed by standardised events from the input adapter, and returning validated adaptations to the frontend in near real time. This separation allows the reasoning configuration to be changed easily; for example, by swapping models, refining prompts, or integrating new agents without changing the structure of the rest of the framework.

% \textbf{Agent-Based Architecture:}
% To maximize flexibility and maintainability, the backend employs a multi-agent extension architecture (MA-SIF), where specialized LLM agents are responsible for different adaptation domains: \begin{itemize} \item \textbf{UI Agent:} Focuses on visual and interactive adaptations, such as increasing font size, contrast, or button scale for accessibility. \item \textbf{Geometry Agent:} Handles spatial adaptations, including element resizing, spacing adjustments, and layout simplification. \item \textbf{Input Agent:} Manages input modality switching (e.g., from touch to voice), gesture interpretation, and error recovery for hands-free or motor-impaired users. \item \textbf{Validator Agent:} Always required. Ensures that suggested adaptations are valid, non-conflicting, and compliant with accessibility guidelines. It can change or add fields, but not remove nor add adaptations.
%  \end{itemize} Each agent is configured via domain-specific prompts in \texttt{sif\_config.json}, enabling rapid iteration and extensibility for new adaptation strategies or modalities. This design allows the backend to evolve without major architectural changes, simply by updating agent prompts or adding new agents as needed to accommodate the developer's needs. The validation agent plays a crucial role in ensuring that adaptations proposed by the other agents do not conflict or duplicate efforts, maintaining a coherent user experience and is thus required.

% \textbf{Basic example of a 2-agent configuration (validator excluded):}
% \begin{lstlisting}[language=json,firstnumber=1]
% {
%   "ui_agent": {
%     "focus": "visual adaptations",
%     "model": "gemini-1.5-flash",
%     "allowed_actions": ["increase_size", "increase_contrast", "reposition_element"],
%     "prompt": "Given the event data, suggest UI adaptations for accessibility. Consider user profile and interaction history."
%   },
%   "geometry_agent": {
%     "focus": "spatial adaptations",
%     "model": "gemini-1.5-flash",
%     "allowed_actions": ["resize_element", "adjust_spacing", "simplify_layout"],
%     "prompt": "Analyze the UI layout and suggest spatial adaptations to improve usability for motor-impaired users."
%   },
%   "validator_agent": {
%     "model": "gemini-1.5-pro",
%     "allowed_actions": ["switch_to_voice", "interpret_gesture", "recover_from_error", "increase_size", "increase_contrast", "reposition_element","resize_element", "adjust_spacing", "simplify_layout"],
%     "prompt": "Validate proposed adaptations for conflicts, duplicates and inconsistencies based on user context, events and interaction history."
%   }
% }
% \end{lstlisting}

% \textbf{Contextual Reasoning and Personalization:}
% The LLM agents do not operate in isolation; they fuse real-time events with user profiles and interaction history stored in MongoDB. This enables the backend to personalize adaptation decisions based on individual preferences, accessibility flags, and behavioral trends. For example, if a user with motor impairments repeatedly misses a button, the UI agent may suggest enlarging the button, while the input agent may recommend switching to voice mode. The validator agent then ensures these adaptations are compatible and beneficial.

% \textbf{Handling Ambiguity and Edge Cases:}
% LLMs excel at interpreting ambiguous or incomplete input, making them ideal for accessibility scenarios where user intent may not be clearly expressed. The backend can handle vague voice commands, noisy gesture data, or conflicting input events by leveraging the LLM’s reasoning capabilities and fallback rules. In cases where the LLM is unable to generate a confident adaptation, deterministic rule-based logic ensures baseline accessibility is maintained.

% \textbf{Extensibility and Future-Proofing:}
% The agent-based LLM design allows the backend to easily incorporate new modalities (e.g., eye tracking, bio-signals) and adaptation strategies without major architectural changes. By updating agent prompts and event schemas, the system can evolve to support emerging accessibility technologies and user needs.

% \textbf{Summary:}
% Overall, the LLM-driven backend transforms static UI adaptation into a dynamic, context-aware process. By fusing multimodal input, user context, and agent-based reasoning, it delivers personalized, robust, and extensible accessibility solutions for a wide range of users and platforms.

% \subsection{Heatmap Analysis}

% To further refine adaptation decisions, the backend can analyze interaction heatmaps derived from user event logs. By aggregating tap coordinates and gesture paths, the system identifies problematic UI regions (e.g., frequently missed buttons) and adapts layouts or element sizes accordingly. This data-driven approach supports continuous improvement and personalization, especially for users with evolving accessibility needs. By using tap frequency, the backend can suggest adaptations like repositioning elements or enlarging hit areas, enhancing usability.

% \subsection{Integration with Profiles and History}

% User profiles, stored in MongoDB, encapsulate accessibility needs, input preferences, UI settings, and recent interaction history. Upon receiving an event, the backend retrieves the relevant profile and fuses it with real-time input to contextualize adaptation decisions. Interaction history enables the backend to learn from past behavior, supporting progressive personalization (e.g., permanently switching to voice mode after repeated motor-impaired interactions).

% \subsection{Communication and Endpoints}

% The backend exposes WebSocket (\texttt{/ws/adapt}) and HTTP (\texttt{/profile}, \texttt{/context}) endpoints for real-time and batch processing. Events are validated, enriched, and processed asynchronously, with adaptation actions returned to the frontend for immediate UI updates. Profile management endpoints ensure user context is always available, supporting seamless onboarding and continuous learning.

% \subsection{Key Design Considerations}

% \begin{itemize} \item \textbf{Accessibility Focus:} All adaptation logic prioritizes WCAG-compliant features and user-specific needs. \item \textbf{Extensibility:} Modular agent design and JSON contracts allow easy integration of new modalities and adaptation strategies. \item \textbf{Reliability:} Rule-based logic ensures robust baseline adaptations; LLMs provide creative, context-aware enhancements. \item \textbf{Continuous Learning:} Feedback loops and history-driven updates enable the system to evolve with user behavior. \item \textbf{Scalability:} Asynchronous processing and MongoDB storage support high event volumes and large user bases. \end{itemize}


\section{User Profiles and Context Modeling}
User profiles form the backbone of the framework’s personalisation capability. They store accessibility preferences, interaction patterns, and contextual data that allow the system to adapt the interface to an individual’s needs over time. The profile is not a static record, it evolves as the user interacts with the system, incorporating both explicit configuration and implicit observations from their behaviour.

Architecturally, the profile contains three types of information:
\begin{itemize}
    \item \textbf{Static attributes:} such as preferred font size, contrast settings, or dominant input modality, which may be set during initial onboarding.
    \item \textbf{Learned preferences:} derived from patterns in the user’s interactions; for example, frequent miss-taps on small controls may trigger a persistent increase in their size.
    \item \textbf{Contextual data:} including recent interaction history and device environment details, which help the backend reason about the most appropriate adaptations in a given moment.
\end{itemize}
When a new event is processed, the backend combines it with the relevant profile and context data before passing it to the reasoning logic. This ensures that adaptations are not just reactive to the most recent input, but also informed by longer-term patterns and situational factors.

The architecture treats profiles as a shared resource between all layers:
\begin{itemize}
    \item The \textbf{input adapter} ensures that the correct user profile is referenced with every event, as well as creating or updating the profile when necessary.
    \item The \textbf{backend} updates profiles automatically as adaptations are applied or feedback is received.
    \item The \textbf{frontend} can query profile data to adjust default UI settings before any adaptations are applied.
\end{itemize}
By integrating profile and context information into every stage of the adaptation pipeline, the framework moves beyond static changes and supports continuous, data-driven personalisation. The specific data schema, storage mechanisms, and update strategies are described in detail in Chapters 4 and 5.

% \subsection{Profile Structure}
% User profiles are stored in MongoDB’s \texttt{profiles} collection, indexed by \texttt{user\_id} for efficient retrieval. Each profile is a JSON document with the following structure:
% \begin{itemize}
%     \item \textbf{user\_id:} A unique identifier (e.g., UUID or username) to associate events and adaptations with a specific user.
%     \item \textbf{accessibility\_needs:} Boolean flags indicating specific requirements, such as \texttt{motor\_impaired: true}, \texttt{visual\_impaired: false}, \texttt{hands\_free\_preferred: true}. These guide adaptation prioritization (e.g., larger buttons for motor-impaired users).
%     \item \textbf{input\_preferences:} Specifies the preferred input modality (e.g., \texttt{preferred\_modality: "voice"}), influencing mode-switching decisions (e.g., enabling voice navigation for hands-free users).
%     \item \textbf{ui\_preferences:} Stores UI settings like \texttt{font\_size} (e.g., 16), \texttt{contrast\_mode} (e.g., ``normal''), and \texttt{button\_size} (e.g., 1.0), serving as defaults for rendering and adaptation.
%     \item \textbf{interaction\_history:} A capped array (10 events) of past interactions of the user with the UI. This stores all the \texttt{Event} data for those 10 events.
%     This supports continuous learning by tracking user behavior.
% \end{itemize}
% \textbf{Example of a user profile:}
% \begin{lstlisting}[language=json,firstnumber=1]
% {
%   "user_id": "user_123",
%   "accessibility_needs": {
%     "motor_impaired": true,
%     "visual_impaired": false,
%     "hands_free_preferred": true
%   },
%   "input_preferences": {
%     "preferred_modality": "voice"
%   },
%   "ui_preferences": {
%     "font_size": 16,
%     "contrast_mode": "normal",
%     "button_size": 1.0
%   },
%   "interaction_history": [
%     {"event_type": "miss_tap", "target_element": "lamp",
%     "timestamp": "2025-08-04T14:41:00Z"},
%     {"event_type": "voice", "target_element": "lock",
%     "metadata": {"UI_element": "button", "command": "unlock"}, "timestamp": "2025-08-04T14:42:00Z"}
%   ]
% }
% \end{lstlisting}

% \subsection{Context Modeling and Usage}
% Context modeling integrates user profiles with real-time events to provide a comprehensive view for \textbf{Smart Intent Fusion}:
% \begin{itemize}
%     \item \textbf{Profile Retrieval:} On receiving an event (via \texttt{/ws/adapt} or \texttt{/context}), the backend queries MongoDB for the user’s profile using \texttt{user\_id}. If absent, a default profile is created via \texttt{POST /profile}.
    
%     \item \textbf{History Management:} Interaction history is updated asynchronously using MongoDB’s \texttt{\$push} and \texttt{\$slice} operators to append new events while capping at 20 entries for efficiency. This ensures relevant context without excessive storage.

%     \item \textbf{Intent Contextualization:} The SIF Backend Layer uses profile data (e.g., \texttt{motor\_impaired: true}) and history (e.g., frequent miss-taps) to inform LLM reasoning.
%     \begin{itemize}
%         \item For example, repeated miss-taps on the Thermostat slider for a motor-impaired user may trigger \texttt{increase\_slider\_size} or \texttt{switch\_mode: voice}.
%     \end{itemize}

%     \item \textbf{Continuous Learning:} Events are logged in the history, enabling the system to refine future suggestions.
%     \begin{itemize}
%         \item For example, permanently switching to voice mode after increasing button size multiple times.
%     \end{itemize}
% \end{itemize}

\section{Dynamic Adaptation Mechanisms}
The dynamic adaptation mechanisms are the part of the framework responsible for translating reasoning outputs from the backend into real-time, personalised changes for the user interface. Operating within the SIF backend layer, they take standardised events from the input adapter, combine them with profile and context data, and decide on the most suitable adaptation. These decisions are returned as structured actions to the input adapter, which maps the JSON contract to an internal representation that is then applied immediately by the frontend using callbacks.

At the architectural level, the adaptation process follows a continuous feedback loop. Every interaction is captured, processed, and logged, with the resulting adaptations influencing how the interface behaves in the future. This allows the system to progressively refine its responses, ensuring that changes are not only reactive but also shaped by longer-term usage patterns.

\subsection{Adaptation Pipeline}
The adaptation pipeline follows a structured lifecycle from input capture to UI update, and can be viewed as four main stages:
\begin{enumerate}
    \item \textbf{Event reception and context gathering:} Inputs from any modality are captured, standardised, and sent to the backend. The backend retrieves the associated user profile and recent history to build a complete context for reasoning.
    \item \textbf{Intent interpretation:} The backend analyses the combined event and context, using both deterministic rules and LLM-based reasoning to infer the most likely user intent.
    \item \textbf{Adaptation Generation:} A set of structured actions is produced, describing changes to be applied to the interface. These may involve modifying the visual presentation, adjusting interaction geometry, or altering input modes. SIF employs:
        \begin{itemize}
            \item \textbf{Rule-Based Logic:} Deterministic rules provide fast, reliable adaptations for common scenarios (mostly used for a reliable backup).
            \item \textbf{LLM Reasoning:} The AI model infers complex intents and generates creative adaptations for the user.
            \item \textbf{Heatmap Analysis:} Simulated via history counts, prioritizes problems with frequently interacted elements (e.g., repositioning elements after multiple taps).
        \end{itemize}
    \item \textbf{Application and logging:} The frontend applies the adaptations immediately, while the backend logs both the event and its outcome for future learning.
\end{enumerate}

\subsection{Supported Adaptation Actions}
The framework supports a defined set of adaptation types that cover common accessibility needs. These include scaling or spacing adjustments for improved touch accuracy, font and contrast changes for better readability, and modality switches for hands-free interaction. The action set is deliberately kept broad enough to handle varied contexts, yet constrained enough to ensure reliable rendering across platforms.
% The framework supports a set of predefined actions designed for accessibility in the Adaptive Smart Home Controller, designed to ensure compatibility with the Frontend Layer’s rendering capabilities and partial alignment with WCAG 2.1 guidelines (e.g., 1.4.3 Contrast Minimum, 2.5.5 Target Size). These actions are generated by Smart Intent Fusion (SIF) and its multi-agent extension (MA-SIF), leveraging rule-based logic, Gemini LLM reasoning, user profiles and interaction history to address the needs of motor-impaired, visually impaired, and hands-free users.
% \begin{itemize}
%     \item \textbf{increase\_button\_size}: Scales buttons (e.g., Lamp’s On/Off toggle) by a factor (e.g., 1.5x) to increase hit areas, aiding motor-impaired users with dexterity challenges. Applied via smooth animations (e.g., Flutter’s \texttt{AnimatedScale}) for seamless user experience.
%     \item \textbf{increase\_font\_size}: Increases text size (e.g., status “Locked” from 16pt to 18pt) to improve readability for visually impaired users, ensuring compliance with WCAG’s text resizing guidelines.
%     \item \textbf{increase\_slider\_size}: Enlarges slider thumbs (e.g., Thermostat’s temperature control) to enhance precision for motor-impaired users, using custom rendering for visual feedback.
%     \item \textbf{increase\_contrast}: Switches to high-contrast mode (e.g., black backgrounds for buttons and text) to improve visibility for visually impaired users, aligning with WCAG contrast requirements.
%     \item \textbf{highlight\_border}: Applies a bolder border to buttons, aiding visually impaired and motor-impaired users by enhancing element visibility (WCAG 2.4.7 Focus Visible).
%     \item \textbf{adjust\_spacing}: Increases spacing between UI elements (e.g., device cards in the ListView) to reduce accidental taps for motor-impaired users, improving target accessibility (WCAG 2.5.5 Target Size).
%     \item \textbf{show\_tooltip}: Displays contextual tooltips (e.g., “Try saying ‘Unlock’”) near UI elements when modality struggles are detected (e.g., repeated miss-taps), guiding hands-free or motor-impaired users to alternative inputs like voice (WCAG 2.5.3 Label in Name).
%     \item \textbf{switch\_mode}: Shifts to keyboard, voice or gesture navigation mode for hands-free users (e.g., after repeated miss-taps), enabling more seamless interaction without physical input.
%     \item \textbf{trigger\_button}: Automatically activates buttons (e.g., toggles Lamp to “On”) when intent is clear from multimodal inputs (e.g., voice “Turn on” + gesture), reducing interaction barriers.
%     \item \textbf{simplify\_layout}: Reduces UI complexity (e.g., fewer UI elements) for hands-free or cognitively impaired users, streamlining interaction for accessibility.
% \end{itemize}
% These adaptation actions form the cornerstone of the framework’s accessibility-driven approach, addressing diverse user needs in the smart home controller. Each action is carefully designed to align with WCAG 2.1 guidelines, ensuring inclusivity for motor-impaired users (e.g., larger buttons/sliders, increased spacing), visually impaired users (e.g., high-contrast modes, highlighted borders), and hands-free users (e.g., tooltips, mode switching).

\subsection{Continuous Learning and Feedback Loop}
A central design principle is that adaptations are not static changes. The system maintains a history of recent interactions for each user, enabling it to identify recurring patterns such as frequent miss-taps or repeated modality switches. When such patterns are detected, the backend can suggest persistent adjustments; for example, permanently enlarging a frequently used UI element, reducing the need for smaller repeated actions.

\subsection{Design Considerations}
\begin{itemize}
    \item \textbf{Accessibility Focus:} All supported actions are chosen to address motor, visual, and input-related impairments, ensuring that adaptations enhance rather than complicate interaction.
    \item \textbf{Real-Time Performance:} Low-latency communication ensures that adaptations are applied quickly enough to feel seamless.
    \item \textbf{Reliability:} Rule-based logic ensures that adaptations continue to function even if LLM-based reasoning is temporarily unavailable.
    \item \textbf{Extensibility:} The action set and event format are flexible enough to incorporate new adaptation types and modalities in future deployments.
\end{itemize}
By combining immediate, context-aware changes with a persistent feedback loop, the dynamic adaptation mechanisms give the framework its ability to evolve alongside the user’s needs, making it more effective over time.

% A core strength of the framework lies in its continuous learning and feedback loop, which enables dynamic refinement of adaptation strategies over time. Every user interaction is systematically logged in MongoDB (within the \texttt{logs} collection and mirrored in \texttt{adaptation\_log.jsonl}), ensuring that contextual details—such as a miss-tap on the Thermostat—are persistently captured for analysis. These events are not only stored for traceability but are also appended to the user’s profile history, maintained as a capped array to preserve the most recent interactions while optimizing storage and retrieval efficiency.

% This historical data serves as a foundation for the learning mechanism. The large language model (LLM) regularly analyzes patterns within the interaction history, identifying trends such as frequent miss-taps or repeated modality switches. By recognizing these behavioral signals, the system can proactively suggest and implement permanent UI adaptations, such as simplifying the layout or switching to a more suitable input mode for the user.

% Furthermore, the feedback loop supports ongoing evaluation of the framework’s effectiveness. By correlating logged events with adaptation outcomes, the system can compute metrics like adaptation accuracy (e.g., the proportion of correct button triggers) and user performance improvements (e.g., reduction in miss-taps over time). This data-driven approach not only validates the impact of adaptations but also guides future refinements, ensuring that the framework evolves in response to real user needs and behaviors.

\section{Chapter Summary}
This chapter has presented the system design and architecture of the multimodal AI-driven framework for dynamic UI adaptation, focusing on its ability to deliver personalised, accessibility-oriented solutions for motor-impaired, visually impaired, and hands-free users. The framework is built around a modular three-layer architecture: the Frontend Layer, Input Adapter Layer, and SIF Backend Layer that enables the seamless integration of multiple input modalities, including touch, voice, and gestures, and the delivery of real-time UI adaptations.

The architectural design emphasises modularity, scalability, generalisability, and accessibility, creating a solid foundation for extension into other domains such as healthcare and gaming. Communication between layers is handled through WebSocket for low-latency updates and HTTP for reliable profile and debugging operations, while MongoDB and a standardised JSON contract ensure scalability and flexibility in storing and processing interaction data.
Together, these elements provide a framework capable of addressing real-world accessibility needs today.

%Chapter 4
\chapter{Smart Intent Fusion (SIF)}
\section{Introduction to Smart Intent Fusion}
Smart Intent Fusion (SIF) is the central intelligence layer of the proposed multimodal AI-driven UI adaptation framework, responsible for transforming raw, heterogeneous user input signals into concrete, context-aware interface adaptations. 
Where the Frontend Layer renders the UI and the Input Adapter Layer standardises events, SIF performs the reasoning step. Architecturally, SIF occupies the central position in the adaptation loop, receiving standardised events from the Input Adapter Layer, combining them with user profile and context data, and returning validated adaptations back to the frontend for immediate application. This placement ensures that every adaptation decision is both context-aware and modality-agnostic, allowing the framework to maintain consistent behaviour regardless of how the interaction was initiated.
it fuses current interaction data with user profiles, accessibility requirements, and recent interaction history to infer the user’s underlying intent and translate this into actionable UI changes.

The motivation for SIF stems from a simple but critical challenge in accessibility-focused HCI:
users rarely interact with a system through a single, perfectly clean input channel. Instead, interactions are often multimodal, noisy, and incomplete. A motor-impaired user may miss-tap a button but also issue a supporting voice command. A visually impaired user may attempt to activate a control by gesture but with low confidence, relying on high-contrast cues to complete the action. Traditional rule-based adaptive systems tend to process these signals independently, missing the opportunity to combine them into a unified, more reliable understanding of the user’s goal.

SIF addresses this gap through a hybrid reasoning approach:
\begin{itemize}
    \item Rule-based logic handles deterministic adaptations for example, “if miss\_tap on target → increase size by 1.5×” ensuring baseline accessibility support and fast response times even without AI availability.
    \item LLM-driven reasoning can process richer multimodal context and can propose creative or proactive adaptations that go beyond fixed rules such as switching to voice mode after repeated input struggles, or combining voice + gesture input to trigger a button instantly.
    \item Multi-Agent SIF (MA-SIF) extends this further by distributing reasoning across specialised LLM agents (UI, Geometry, Input or more) and validating results through a dedicated Validator Agent to reduce hallucinations and conflicting actions as well ensuring the necessary validation.
\end{itemize}

The SIF layer sits behind well-defined APIs:
\begin{itemize}
    \item WebSocket endpoint \texttt{/ws/adapt} for low-latency, real-time adaptation suggestions.
    \item HTTP endpoints like \texttt{/profile} for profile management, or \texttt{/full\_history} for developer tooling and debugging.
\end{itemize}
These endpoints use a strict JSON contract, making it easy for any frontend platform to connect and work with the backend. This ensures that the system remains flexible and can handle different input modalities.

A defining feature of SIF is its integration with persistent user profiles stored in MongoDB. These profiles encapsulate three layers of information: static attributes such as preferred font size or contrast mode, learned preferences derived from recurring interaction patterns, and contextual data including recent history and environmental details. By merging these dimensions with each incoming event, SIF can make decisions that are not only responsive to the user’s current action but also aligned with their long-term accessibility needs.
By fusing this profile data with incoming events, SIF maintains a continuous personalisation loop progressively adapting the UI to match the user’s abilities and context over time.

For example:
\begin{itemize}
    \item A profile with \texttt{motor\_impaired: true} will bias adaptations towards larger controls and simplified layouts.
    \item Repeated history of slider misses may lead to permanent slider thumb enlargement.
    \item A hands-free preference can automatically promote voice/gesture-driven navigation in relevant contexts.
\end{itemize}

In the context of this thesis, SIF is not only an internal backend feature, it is the core research contribution.
The remainder of this chapter expands on the theoretical underpinnings of SIF, its integration with user profiles and multimodal fusion, the prompt engineering strategies for guiding LLM behaviour, the architecture of its multi-agent extension, and the performance metrics used to evaluate its effectiveness.
% Event Flow Diagram: This diagram should be abstract, without method names
    % Input from the adapter (via JSON contract)
    % Context/profile retrieval from MongoDB
    % Rule-based + LLM + MA-SIF processing
    % Adaptation output sent to the frontend

\section{Theoretical Foundations of Smart Intent Fusion}
The idea behind Smart Intent Fusion is simple:
\\ users don’t interact with a UI in one clean and perfect way. In real life, inputs are messy, mixed, and often incomplete. A person might tap the wrong spot on a button, then say a voice command to make sure it worked. Someone using gestures might point at something but not be perfectly accurate, so they rely on extra visual cues to finish the task. A lot of current adaptive systems still process these signals separately, and that means they miss the chance to combine them into a clearer picture of what the user actually wanted.

Smart Intent Fusion tries to fix that by taking all the inputs together: touch, keyboard, voice, gestures and mixing them with what the system already knows about the user (their profile, their history of interactions, and their accessibility needs). This way, it can guess the real intent and adapt the UI in the most helpful way.

\subsection{Multimodal Fusion}
In HCI, “multimodal fusion” just means combining different types of input to get a more reliable or richer understanding of the user’s action. 
\\This can be done in a few ways:
\begin{itemize}
    \item \textbf{Early fusion}: merge the raw signals before interpreting them (e.g., combining the coordinates of a tap with the audio of a voice command immediately). \\
    → Useful when two inputs happen at the exact same moment.
    \item \textbf{Late fusion}: process each input type separately first, then merge the interpreted results (e.g., “voice command = unlock” + “miss-tap near unlock button” → final decision = trigger Unlock button).\\
    → This is where most SIF reasoning happens.
    \item \textbf{Hybrid fusion}: a mix of both, sharing some data early but also combining results later. \\
    → Used when one modality’s data can make another modality’s interpretation more accurate.
\end{itemize}
SIF uses the hybrid approach. The Input Adapter Layer already standardises each input into a clean JSON format, but the fusion step happens in the backend where the current event, past history, and profile are all processed and fused together. This makes it possible to combine patterns like “miss-tap + voice command” into one clear adaptation. In the overall architecture described in Chapter 3, this fusion step is the bridge between raw input handling and adaptation generation. By sitting in the backend, SIF can remain entirely modality-agnostic while still benefiting from the structured event format defined by the Input Adapter Layer. This ensures that even when new input methods are introduced in the future, the fusion logic can remain unchanged, relying on the same JSON schema and profile-context pipeline.

\subsection{Intent Inference}
The main goal of SIF is not just to log inputs, but to figure out \textit{why} the user did them. This is called intent inference. In other words, we want the system to answer the question: “What was the user trying to do?”
If the system knows the intent, it can choose the best adaptation. The inference process is always profile-aware. Each decision is informed by a combination of static preferences (such as font size or contrast mode), learned patterns from past usage (such as repeated difficulty with sliders), and short-term contextual data (such as the most recent interactions). This means that two users producing the same input sequence could receive different adaptations if their profiles and histories differ, keeping the interaction completely personalised. 
\\For example:
\begin{itemize}
    \item If the intent is to press “Unlock” but the user misses, the adaptation could be to enlarge the button and trigger it right away.
    \item If the intent is to adjust a thermostat but the user struggles with the slider, the adaptation could be to switch to voice control and increase the slider size.
\end{itemize}

This is where LLMs can help. They are good at reasoning about context, combining clues, and filling in gaps when inputs are unclear. The downside is that they can be slow, be costly to run, and sometimes “hallucinate” an answer that doesn’t make sense even if its not defined in the prompt. That’s why SIF uses a \textbf{hybrid} approach:
\begin{itemize}
    \item \textbf{Rules} handle clear, simple cases with instant feedback.
    \item \textbf{LLM reasoning} handles more complex or ambiguous cases.
\end{itemize}

\subsection{Why Hybrid Works Best}
A purely rule-based system is predictable but rigid. It can only respond to situations that were thought of in advance. A purely LLM-based system is flexible but not always reliable, especially when it needs to work in (close to) real time for accessibility.
By combining both:
\begin{itemize}
    \item The rules guarantee that basic accessibility changes (like increasing size after a miss-tap) always work. For example:
    \begin{verbatim}
        IF event_type == "miss_tap" AND profile.motor_impaired == true
        THEN action = increase_button_size(target, 1.5)
    \end{verbatim}
    \item The LLM adds creativity and can adapt to situations the rules didn’t cover, like combining unusual input patterns, proposing a mode switch for hands-free use or even something entirely new.
\end{itemize}

\subsection{Connection to Accessibility}
The whole point of this is to improve accessibility for different kinds of users:
\begin{itemize}
    \item \textbf{Motor-impaired:} combine multiple input signals to avoid repeated failed attempts.
    \item \textbf{Visually impaired:} recognise when visual feedback is not enough and trigger higher-contrast or bigger fonts automatically.
    \item \textbf{Hands-free:} allow combinations like voice + gesture to instantly activate actions.
\end{itemize}

In short, the theoretical base for SIF comes from multimodal fusion, intent inference, and hybrid reasoning. These ideas are not new in HCI, but this framework applies them with a strong focus on accessibility and personalisation, and makes them work in real time with cross-platform UI code. For example, a motor-impaired user who misses a lock button twice and then issues a voice command might trigger a combined adaptation: the UI immediately enlarges the button for future taps, but also switches the interface to voice-first mode for the current session. This ability to layer short-term fixes on top of long-term adjustments is what makes SIF more than just a reactive system, it is a continuously learning adaptation layer.

\section{User Profile and Context Integration}
For Smart Intent Fusion to be truly “smart,” it needs more than just the current event it is processing. If SIF reacted to every tap, voice command, or gesture without knowing who the user is or how they usually interact, it would behave like a generic accessibility script rather than a personalised adaptation system. That’s why the user profile and interaction context form the backbone of the reasoning process. They give SIF a sort of memory/personality, and the ability to adapt over time, not just in the moment.
When a new event comes in like a tap, miss-tap, voice command, or gesture, SIF doesn’t look at it in isolation. It combines that event with:
\begin{enumerate}
    \item \textbf{The user profile} – a stored record in MongoDB with accessibility flags, preferred modalities, and UI settings.
    \item \textbf{Interaction history} – the last 10 events that show patterns or repeated problems.
    \item \textbf{Current UI context} – optional metadata about what’s on screen, where buttons are placed, and their sizes.
\end{enumerate}
This means SIF can make decisions that are personal and context-aware, not just reactive.

\subsection{User Profiles}
A user profile stores the information that makes one person’s interaction style different from another’s.
This can include accessibility needs (motor-impaired, visually impaired, hands-free preferred), preferred input modalities, and baseline UI settings like font size, contrast mode, and button scale. It can be seen as the \textbf{memory} of the system.
When combined with a short history of recent interactions, this profile turns SIF from a static decision engine into a continuous learning system. 

Without profiles, SIF could still make adaptations, however they would always be reactive and temporary.
For example:
\begin{itemize}
    \item If a user with tremors keeps missing a button, the button might get enlarged for that session, but as soon as they restart the app, it would shrink back.
    \item If a user prefers voice input, SIF wouldn’t know to automatically switch to voice mode when they struggle with touch, instead it would have to come up on its own that this switch is needed for this user.
\end{itemize}
Profiles ensure these adaptations stick and get better over time.

\subsection{User Profile Structure}
In the backend, each profile is a JSON document in the \texttt{profiles} collection of a MongoDB database. It’s indexed by \texttt{user\_id} so the system can look it up instantly whenever a new event arrives. A typical structure looks like this:

\begin{lstlisting}[language=json, caption=Simplified User Profile Example]
{
  "user_id": "user_123",
  "accessibility_needs": { "motor_impaired": true },
  "input_preferences": { "preferred_modality": "voice" },
  "ui_preferences": { "font_size": 16 },
  "interaction_history": [
    { "event_type": "miss_tap", "target_element": "lamp" }
  ]
}
\end{lstlisting}
The full schema with all supported fields and metadata is given in Chapter 5.
\newpage
This design keeps it simple but flexible. Architecturally, each profile contains:
\begin{itemize}
    \item \texttt{accessibility\_needs}: flags that tell SIF what kind of adaptations to prioritise.
    \item \texttt{input\_preferences}: helps the system decide which modality to switch to when needed.
    \item \texttt{ui\_preferences}: baseline visual parameters such as font size and contrast mode.
    \item \texttt{interaction\_history}: capped log of recent events to support continuous learning.
\end{itemize}
MongoDB’s indexing means the profile can be retrieved in milliseconds, even with a large user base, and capped histories ensure lookups are fast.

\subsection{How Profiles Affect Decisions}
When an event comes in, the backend follows a clear process:
\begin{enumerate}
    \item Load the profile from MongoDB using the \texttt{user\_id}. If it doesn’t exist yet, create a new default profile.
    \item Combine the event with the last few interactions from the history.
    \item Pass the profile, history, and current event into the Smart Intent Fusion reasoning step.
\end{enumerate}
This context completely changes how SIF decides on adaptations.\\\\
Some examples of influenced decisions:

\textbf{Example 1: Motor-impaired user with repeated miss-taps}\\
If the last three events in history are miss-taps on the same “Unlock/Lock” button, and \texttt{motor\_impaired} is true in their profile, SIF might do two things at once:
\begin{lstlisting}[language=json, caption={Possible motor-impaired user adaptations}]
[
  {"action": "increase_button_size", "target": "button_unlock", "value": 1.5, "reason": "Multiple miss-taps detected, enlarging button for better accessibility"},
  {"action": "highlight_border", "target": "button_unlock", "reason": "Increase button visibility for the user"}
]
\end{lstlisting}
Without the profile, it might have just enlarged the button once and moved on.

\textbf{Example 2: Hands-free preferred user}
A user with \texttt{"hands\_free\_preferred": true} points at a device card (gesture) and says “turn on the lights.”
SIF reasoning can fuse these into:
\begin{lstlisting}[language=json, caption={Hands-free user intent fusion}]
{
  "action": "trigger_button",
  "target": "button_light",
  "reason": "Gesture pointing + voice 'Turn on the lights' detected for hands-free user",
  "intent": "Activate Lights"
}
\end{lstlisting}
Because of the profile, SIF is confident enough to trigger the button immediately without asking for physical confirmation.

\subsection{Continuous Learning from History}
Profiles are not static. Every interaction is logged in the \texttt{interaction\_history} and can influence future decisions. The interaction context can be seen as the system's \textbf{short-term awareness}.
This makes SIF a learning system:
\begin{itemize}
    \item If a user keeps manually enabling high-contrast mode, the system can update \texttt{contrast\_mode} in their profile so it’s always on by default.
    \item If increasing a button size significantly reduces miss-taps, that size can become the new permanent baseline in \texttt{ui\_preferences}.
    \item If switching to voice mode solves repeated touch struggles, the profile can be updated to favour voice by default.
\end{itemize}
This feedback loop means the user doesn’t need to “train” the system manually, it adapts naturally as they use it. If SIF only looked at the current event, it would miss important patterns and make short-sighted decisions.
With profile + history + event combined:
\begin{itemize}
    \item Adaptations can be proactive instead of reactive.
    \item The UI can stay consistent between sessions.
    \item The system can learn what really helps the user over time.
\end{itemize}
From an accessibility perspective, this is the difference between a generic interface that occasionally helps and a personalised tool that feels like it understands the user.

\subsection{Role in Accessibility}
User profiles are the backbone of accessibility-focused adaptations in this framework. They act as a persistent memory of the user’s abilities, preferences, and interaction challenges, enabling the system to make targeted, proactive adjustments rather than relying solely on short-term reactive changes. By storing explicit accessibility flags alongside learned behavioural patterns, profiles allow SIF to tailor the interface to an individual user in ways that are both short-term and long-term.

From an accessibility perspective, profiles influence SIF’s reasoning in three key user categories:
\begin{itemize}
    \item \textbf{Motor-Impaired Users:} Profile flags such as \verb|motor_impaired: true| prioritise adaptations that reduce fine motor precision requirements. This can include:
    \begin{itemize}
        \item Enlarging touch targets (buttons, sliders) and increasing spacing to prevent accidental taps.
        \item Offering alternative modalities such as voice commands or keyboard navigation to bypass touch interaction altogether.
        \item Retaining enlarged target sizes across sessions once repeated miss-taps are detected.
    \end{itemize}

    \item \textbf{Visually Impaired Users:} When \verb|visual_impaired: true| is set, adaptations aim to maximise visual clarity. Examples include:
    \begin{itemize}
        \item Switching to high-contrast themes and bold colour schemes to align with WCAG 2.1 contrast requirements.
        \item Increasing font sizes and icon scales to meet text accessibility guidelines.
        \item Highlighting the active element with a strong focus border or magnified overlay to improve navigation feedback.
    \end{itemize}

    \item \textbf{Hands-Free Users:} Profiles with \verb|hands_free_preferred: true| bias SIF towards non-touch input modes, reducing physical interaction demands. Adaptations may involve:
    \begin{itemize}
        \item Automatically switching to voice or gesture navigation when interaction struggles are detected.
        \item Providing clear, speech-friendly UI labels and tooltips to improve command recognition accuracy.
        \item Simplifying layouts by reducing the number of visible controls at once, making it easier to select elements through voice or gesture.
    \end{itemize}
\end{itemize}

By embedding these accessibility considerations directly into the profile structure, SIF can reason in a way that is both context-aware and user-specific. This allows the system to:
\begin{enumerate}
    \item Anticipate needs before errors occur (e.g., pre-emptively enlarging critical controls for a motor-impaired user on a small screen).
    \item Ensure adaptations persist across sessions, avoiding the frustration of having to reconfigure accessibility settings each time.
    \item Combine profile knowledge with real-time interaction patterns, enabling nuanced decisions such as \emph{“keep high contrast on by default, but also enable voice mode when the user is multitasking”}.
\end{enumerate}

\section{Modeling Multimodal Input Fusion}
Smart Intent Fusion doesn’t just take one input, it collects \textbf{multiple inputs from different modalities} like touch, voice, gestures, and more. Then fuses them into a single, well-reasoned adaptation. This process is called \textit{multimodal input fusion}.
Without fusion, the system would treat each event separately. A miss-tap would trigger one adaptation, and a voice command would trigger another, without realising both were aimed at the same action. With fusion, those two inputs can be combined into one confident and more helpful response which saves time and reduces frustrations. Furthermore, the user profile and interaction history also directly affect how this fusion works. A hands-free preferred user will have a lower threshold for fusing gesture + voice, but a visually impaired user’s profile might cause SIF to always add a “highlight” adaptation when triggering elements via voice, even if not strictly necessary. If history shows repeated failures for a certain modality, its weight in fusion decisions can be reduced. For users with impairments, every extra action is extra effort. Because of how the fusion is designed, it cuts down on unnecessary steps so the UI adapts faster and smarter to the user’s needs.

\subsection{Event Standardisation}
Before any fusion can happen, the raw input needs to be standardised.
Every frontend in the framework Flutter, SwiftUI or a future VR client converts its local input data into the same JSON contract. This is handled by the Input Adapter Layer as described earlier.

A standardised event looks like this:
\begin{lstlisting}[language=json, caption={Standardised Event Example}]
{
  "event_type": "miss_tap",
  "source": "touch",
  "timestamp": "2025-08-04T14:41:00Z",
  "user_id": "user_123",
  "target_element": "button_unlock",
  "coordinates": {"x": 210, "y": 640},
  "confidence": 0.8,
  "metadata": {"UI_element": "button"}
}
\end{lstlisting}
This contract ensures:
\begin{itemize}
    \item \textbf{Cross-platform compatibility}: all clients speak the same “language” to the backend.
    \item \textbf{Dynamic field names}: fields like \texttt{event\_type} and \texttt{target\_element} can be extended and modified without breaking existing logic.
    \item \textbf{Extensibility}: new modalities can be added (like gaze tracking) without breaking existing clients.
\end{itemize}
By the time the event reaches SIF, it doesn’t matter whether it came from a phone, desktop, or VR headset, it always adheres to the same format for the backend.
As said earlier SIF uses a hybrid approach for multimodal input fusion. The Input Adapter handles basic pre-processing (similar to early fusion), but the actual reasoning, deciding what adaptations to apply happens in the backend using late fusion.

\textbf{Example: Touch + voice} \\
Let’s say a motor-impaired user taps just to the right of the “Lock/Unlock” button and, within a second, says “Unlock.”
Individually, the miss-tap could trigger an enlargement of the button and the voice command could trigger the unlocking action. However, with multimodal fusion, the system can understand that both inputs are related and prioritize the unlocking action while also enlarging the button for better accessibility.

With SIF fusion, the system sees both in context:

\begin{lstlisting}[{language=json}, caption=Example of Multimodal Fusion]
[
  {"action": "increase_button_size", "target": "button_unlock", "value": 1.5, "reason": "Miss-tap detected near Unlock button"},
  {"action": "trigger_button", "target": "button_unlock", "reason": "Voice command 'unlock' detected in combination with miss-tap"}
]
\end{lstlisting}
Now the button is both enlarged for future use and triggered immediately, reducing the number of actions the user needs to take, simplifying the interaction which is useful for motor-impaired users.

\subsection{Timing and Confidence}
SIF's LLM reasoning can also consider \textbf{when} and \textbf{how confidently} an input happened based on two fields from the standardised event:
\begin{itemize}
    \item \textbf{Timing}: events close together in time (e.g., within 1–2 seconds) are more likely to be related.
    \item \textbf{Confidence}: each modality can provide a confidence score (e.g., gesture detection might be 0.7 certainty). Lower confidence might require a second modality before acting.
\end{itemize}
For example, a low-confidence gesture to point at a button might do nothing alone, but if followed by a high-confidence voice command naming that button, SIF can treat them as a combined intent.

\subsection{LLM Reasoning in Fusion Decisions}
While the fusion process benefits from deterministic rules for speed and reliability, one of the key innovations of SIF is its ability to leverage LLM reasoning to interpret and combine multimodal inputs in a context-aware way. 

Once standardised events reach the backend, they are not processed in isolation. Each event is enriched with:
\begin{itemize}
    \item \textbf{User profile data:} accessibility flags, preferred modalities, and baseline UI settings.
    \item \textbf{Interaction history:} the most recent events, revealing repeated struggles or patterns.
    \item \textbf{Contextual metadata:} details about the UI state (e.g., which elements are visible), UI element type, environment and more.
\end{itemize}

This enriched dataset is then included in the LLM prompt, allowing the model to reason comprehensively about the user’s intent across modalities.  
For example, instead of treating:
\begin{itemize}
    \item a low-confidence “point” gesture at a thermostat slider, and
    \item a voice command “set temperature to 22”
\end{itemize}
as separate actions, the LLM can infer that they describe the same goal and produce a single adaptation: 
\begin{lstlisting}[language=json]
{
  "action": "adjust_slider",
  "target": "slider_thermostat",
  "value": 22,
  "reason": "Gesture and voice command combined to adjust temperature"
}
\end{lstlisting}

By bringing together different types of input modalities, user’s preferences, and their recent actions, SIF can suggest changes that are not only accurate but also anticipate what will help the user most. This means the system can adapt quickly and make the interface easier to use based on what the user's most probable intent was.

\section{Rule-Based Logic and LLM-Driven Adaptation}
Smart Intent Fusion uses two very different ways to decide what adaptation to apply:
\begin{itemize}
    \item \textbf{Rule-based logic} handles clear, deterministic cases where the system can apply a known adaptation based on the event type and user profile. It is instant, and predictable.
    \item \textbf{LLM-driven reasoning} uses the Gemini API to process complex, multimodal contexts and propose creative adaptations that go beyond simple rules.
\end{itemize}
Both have strengths and weaknesses, which is why the framework combines them instead of choosing one.

\subsection{Rule-Based Logic}
Rule-based logic works by matching incoming events to predefined conditions and applying a fixed response.\\
For example:
\begin{lstlisting}[language=python, breaklines]
    if event.event_type == "miss_tap":
        return {"action": "increase_button_size", "target": event.target_element, "value": 1.5}
\end{lstlisting}
Advantages:
\begin{itemize}
    \item \textbf{Simplicity}: Easy to implement and understand.
    \item \textbf{Speed}: Instantaneous responses to known events.
    \item \textbf{Predictability}: Consistent behavior for similar inputs and no risk of unintended consequences like hallucinations.
    \item \textbf{Baseline guarantee}: Acts as a safety net so that critical accessibility features still work if the LLM is unavailable, slow to respond, or returns unusable output.
\end{itemize}
Limitations:
\begin{itemize}
    \item Can only handle cases explicitly programmed in advance.
    \item No ability to combine signals in creative ways.
    \item Doesn’t learn new patterns unless a developer updates the rules.
\end{itemize}
This means that while rule-based logic is fast and reliable, it can also be rigid and unable to adapt to new situations without human intervention. In SIF, they are deliberately kept lightweight, mostly as a mock or backup layer for LLM-driven reasoning.


\subsection{LLM-Driven Reasoning}
The LLM can reason about the event, user profile, and history together to try and infer the user's intent more deeply. It can make connections that rules would miss, such as:
\begin{itemize}
    \item Combining a miss-tap with a voice command into a single action.
    \item Switching to a different modality when it detects repeated failure in the current one.
    \item Proposing multiple coordinated adaptations for one intent.
\end{itemize}
Advantages:
\begin{itemize}
    \item \textbf{Flexibility}: Can adapt to new situations without explicit programming by the developer.
    \item \textbf{Context-aware}: Takes profile and history into account naturally.
    \item \textbf{Learning}: Can improve over time by learning from user interactions and feedback.
\end{itemize}
Limitations:
\begin{itemize}
    \item \textbf{Less predictable}: May generate unexpected or irrelevant responses.
    \item \textbf{Slower}: Network call + reasoning time.
    \item \textbf{Needs validation}: Output must be checked more thoroughly before applying.
\end{itemize}

\subsection{Hybrid Approach in SIF}
In practice, SIF doesn’t fully choose between the two, it blends them:
\begin{enumerate}
    \item \textbf{Rules first}: If the event matches a clear, high-confidence rule, apply it immediately, this is mostly done in the frontend by the user profile.
    \item \textbf{LLM second}: Use the model for complex or ambiguous cases, or to suggest extra adaptations beyond the rules.
    \item \textbf{Timeout Fallback}: If the LLM times out or fails, return rule-based or other LLM output only.
\end{enumerate}
This ensures that basic accessibility features always work, while still allowing for creative, context-aware adaptations when needed. In other words, the rules form the “floor” of the system (the minimum guaranteed level of accessibility) while the LLM can raise the ceiling by adapting to more complex, ambiguous, or novel situations. Users are never left waiting for AI responses that might never come, and the system remains responsive even in worst-case scenarios.

\section{Multi-Agent Smart Intent Fusion (MA-SIF)}
While a single LLM can process events and suggest adaptations, it often tries to “do everything” at once.
That makes it harder to constrain, more prone to hallucinations, less predictable and in some cases slower (depending on workload).
Multi-Agent Smart Intent Fusion (MA-SIF) solves this by splitting the reasoning into specialised agents, each focused on one domain of UI adaptation, and then combining their outputs through a Validator Agent.
This design brings the benefits of modularity, parallelism, and role-specific constraints, all of which improve reliability and make the system easier to maintain.

\subsection{Why Multiple Agents?}
When one model is asked to handle UI changes, geometry adjustments, input mode switching, and validation all at once, several problems appear:
\begin{itemize}
    \item The prompt becomes long and vague.
    \item Allowed actions become harder to enforce and turn into hallucinations.
    \item Reasoning gets scattered between unrelated concerns.
\end{itemize}
By giving each agent a clear role, their prompts can be short, specific, and easy to maintain.
For example, the UI agent only ever sees actions like \texttt{increase\_font\_size} or \texttt{increase\_contrast}, while the Geometry agent only deals with spatial changes like \texttt{increase\_button\_size} or \texttt{adjust\_spacing}.
This separation means that each agent can focus on its specific task without being distracted by unrelated concerns. Parameters can be tuned independently for each agent, allowing for more precise control over their behavior (e.g., lower temperature for Geometry, slightly higher for Input). Finally, debugging becomes easier as each agent's logic is contained and easier to follow.

\subsection{Agent Roles}
At a high level, MA-SIF in this thesis's current configuration, consists of four specialised LLM agents: 
\begin{enumerate}
    \item a \textbf{UI Agent} for visual and interactive adaptations, 
    \item a \textbf{Geometry Agent} for spatial changes and layout simplification, 
    \item an \textbf{Input Agent} for modality switching and interaction simplification, 
    \item a \textbf{Validator Agent} for conflict resolution and accessibility compliance. 
\end{enumerate}
Each has a narrow focus, defined prompts, and a limited set of allowed actions, making their reasoning predictable and easier to validate. They have clearly defined scopes of responsibility, and their roles are deliberately narrow to keep prompts concise, outputs predictable, and debugging straightforward. \textbf{The UI Suggestion Agent} is concerned purely with visual accessibility changes such as increasing font size, toggling high-contrast mode, or displaying contextual tooltips. For example, when a visually impaired user interacts with a dense text block, this agent might output an adaptation like increasing the global font scale by 1.2×.

The \textbf{Geometry Suggestion Agent} deals with spatial layout changes and the physical dimensions of interactive elements. It might recommend increasing the size of a button, expanding the hit area of a slider, or adding extra spacing between cards in a list. For example, a motor-impaired user who repeatedly misses a button may trigger an output to enlarge that specific button by 1.5× while keeping the rest of the interface unchanged.

The \textbf{Input Suggestion Agent} focuses on modality switching and simplifying interaction pathways. It is capable of suggesting transitions between touch, keyboard, voice, or gesture modes, as well as proposing layout simplifications to reduce cognitive load. For instance, if a user struggles with touch but succeeds with voice commands, this agent can suggest switching to voice-first navigation, potentially combined with a reduced interface complexity.

Finally, the \textbf{Validator Agent} operates after all others have completed their reasoning. This is the most computationally demanding role, as it must examine every proposed adaptation in detail, identify and remove duplicates, verify that all actions are allowed, and ensure that no output falls outside safe parameter ranges. It also resolves potential conflicts, such as two agents targeting the same element with incompatible values. Because of the breadth of this responsibility, the validator uses a larger Gemini model, a higher timeout of 30 seconds, and a dynamic thinking budget.

\subsection{Adaptation Flow}
When an event arrives at the backend, the fusion process begins by loading the user’s profile and recent interaction history from MongoDB. This contextual data is then sent to each non-validator agent, alongside the event itself. The prompt for each agent is tailored to its domain, ensuring that the LLM only receives relevant instructions and the list of actions it is permitted to output.

Once each agent has processed the event, their suggestions are collected. Importantly, the system is tolerant of partial failures, if one or more agents fail to return a result due to a timeout or API error, the remaining agents’ outputs are still retained. These partial results are then passed to the Validator Agent, which merges them into a coherent and conflict-free final adaptation set. Another possibility is fusing LLM adaptations with rule-based suggestions to create a hybrid output when one or more agents fail.

If the validator itself fails, either due to malformed output or a processing error, the system does not discard all results. Instead, it falls back to returning the raw, unvalidated suggestions from the agents since the output may still be more useful than rule-based alternatives. Only if all agents fail to produce output does the framework revert to the rule-based \texttt{mock\_fusion} fallback. This layered approach ensures that useful adaptations are preserved whenever possible, rather than being lost due to a single point of failure.

\subsection{Dynamic Configuration}
A key advantage of MA-SIF is that it is configured almost entirely through the \texttt{sif\_config.json}, making it very adaptable without requiring backend code changes. Each agent’s behaviour can be fine-tuned individually including its allowed actions, model type, temperature, thinking budget, and timeout, simply by editing the configuration file. This flexibility extends to the ability to add new agents or duplicate existing ones with different focuses.

For instance, it is entirely possible to run two Geometry agents in parallel: one optimised for mobile devices with smaller screens and one tuned for desktop layouts. Both would analyse the same event and profile data but apply different heuristics in their prompts. The Validator Agent would then merge their suggestions, resolving overlaps and conflicts automatically. This makes it possible to create specialised agents for emerging modalities, such as gaze tracking in VR, without altering any core fusion logic.

This externalised configuration is particularly valuable in accessibility research, where rapid iteration is needed. 
For example, during a live user study, the prompt for the UI Agent can be adjusted to emphasise high-contrast themes over font scaling without redeploying the backend. 
Similarly, experimental agents for new modalities such as gaze tracking or haptic feedback can be added in minutes, allowing researchers or developers to test new adaptation strategies with minimal downtime.

%config? + model settings or in chapter 3

\subsection{Example in Action}
Consider a user with both \texttt{visual\_impaired: true} and \texttt{hands\_free\_preferred: true} in their profile. They attempt to turn on the lights by tapping near the “Turn on” button but miss slightly, then say “Turn on the lights” almost immediately afterwards. The UI Suggestion Agent, informed by the user’s visual impairment, recommends switching to high-contrast mode and displaying a tooltip to make it easier to know the app's workings next time. The Geometry Suggestion Agent identifies the repeated miss and proposes increasing the button size. The Input Suggestion Agent recognises the voice command and suggests both switching to voice mode and triggering the button directly.

These suggestions are passed to the Validator Agent, which removes any duplicate \texttt{increase\_button\_size} actions, ensures all parameters are sound, and merges the remaining actions into a single, ordered list. The final adaptation set includes the contrast adjustment, the button enlargement, the tooltip display, the voice mode switch, and the direct triggering of the button. All of these are applied in one update, making the interface immediately more accessible and easier to use in future interactions with the possibility of making them permanent for the user, by updating their profile.

This scenario also highlights MA-SIF’s integration with persistent profiles and recent history: because the profile already records both visual impairment and a hands-free preference, the agents start from a position of context-awareness rather than guessing from scratch. The resulting adaptations are therefore both reactive to the immediate miss-tap and proactive in aligning with the user’s long-term accessibility needs.

\subsection{Benefits of the Multi-Agent Approach}
The multi-agent architecture provides several practical advantages. Reliability is improved because a failure in one agent does not prevent others from giving valuable suggestions. The modularity of the design makes it straightforward to maintain, as each agent can be modified or tuned independently without risking regressions in unrelated areas. Running agents in parallel also improves responsiveness, particularly when some suggestions can be applied even before all agents have finished processing.

Specialisation further enhances the quality of the output, as each agent’s prompt scope is narrow enough to minimise irrelevant reasoning. The dynamic configuration system makes it possible to scale the number of agents up or down, or to swap in different models, without any changes to the backend logic. This adaptability is especially important in research and prototyping contexts, where requirements may change quickly.

The practical implementation of MA-SIF including the \texttt{sif\_config.json} schema, example prompts, and backend implementation is detailed in Chapter 5.

\section{Prompt Engineering for LLMs in SIF}
One of the most important parts of Smart Intent Fusion is \textbf{how we talk to the LLM}.
Unlike a traditional rule-based system, where logic is written explicitly in code, here the behaviour of the LLM depends on the instructions it receives, namely the prompt.
If the prompt, by which the LLM is queried, is unclear, missing context, or too open-ended, the output will either be wrong, inconsistent, or impossible to parse in code.
The LLM could also be more prone to hallucinations, which means it generates answers that sound plausible but are actually incorrect or nonsensical. This means the design of the prompt, the model parameters, and the output format all directly affect how useful and reliable the adaptations are. Even though the prompts used in this framework are relatively simple compared to large fine-tuned systems, they still follow a consistent structure and design philosophy that make them work for this use case. In the architecture described in Chapter 3, each event passed to the SIF Backend Layer arrives already standardised, enriched with metadata, and paired with relevant user profile and history context. The following prompt engineering process simply embeds this structured data into the LLM request, ensuring that reasoning always starts from the same reliable, modality-agnostic representation.

\subsection{LLM Prompt Design Principles}
Prompt engineering in this context is not just about getting a correct answer, it directly impacts accessibility outcomes. A poorly constrained prompt could suggest adaptations that introduce visual clutter, require unnecessary actions, or even reduce usability for the intended audience. By embedding accessibility goals, WCAG criteria, and known user needs into the prompt, the LLM is guided toward changes that genuinely improve the interface rather than merely altering it.
The main objectives that flow from this understanding when writing the SIF prompts were:
\begin{enumerate}
    \item \textbf{Be unambiguous}: Avoid instructions that could be interpreted in multiple ways.
    \item \textbf{Enforce a strict JSON schema}: The frontend and backend depend on predictable keys and types.
    \item \textbf{Constrain actions}: Only allow adaptations that are valid for the given agent type.
    \item \textbf{Tie reasoning to context}: The model should always consider the event, user profile, and recent history together.
\end{enumerate}
The idea was to make the prompts as predictable as possible. In accessibility systems, consistency often matters more than creativity or flexibility.

\subsection{Prompt Structure from \texttt{sif\_config.json}}
Each agent (UI, Geometry, Input) has its own prompt in \texttt{sif\_config.json}.\\
Here’s a shortened example from the UI agent:
\begin{lstlisting}[caption=Example UI Agent Prompt, language=json]
{
    "prompt": "You're the UI suggestion Agent.  
    Analyze user event: {event_json}  
    User profile: {profile_json}  
    Recent history (last 10 events): {history_json}  
    Suggest UI adaptations as JSON in the strict format.  
    Each suggestion must include 'intent', 'reason', and at least a 'value' and 'mode' field. Value must be a reasonable number (e.g., 1.2) with at most one decimal place, and represents a scaling value unless stated otherwise in the metadata (e.g. font size). Target can be 'all' or specific elements."
}
\end{lstlisting}
The important elements here are:
\begin{itemize}
    \item \textbf{Role definition}: explicitly stating the agent’s focus (“UI suggestion Agent”).
    \item \textbf{Context injection}: inserting the current event, profile, and history as JSON strings.
    \item \textbf{Output constraints}: telling the model exactly what keys and value types are allowed.
    \item \textbf{Value rules}: restricting numeric ranges so the model doesn’t output absurd sizes.
\end{itemize}
The complete prompt set, agent-specific instructions, and their runtime configuration are detailed in Chapter 5.

\subsection{Disjunction Ambiguity in LLM Interpretation}
During testing, it was observed that LLMs can misinterpret logical connectors such as \textbf{or} and \textbf{and}. For example, the instruction:
\mdblockquote{“The adaptation must include a value or a mode field.”}
was intended to mean \textit{at least one of these fields is required for this action type}. However, the model sometimes treated this as an exclusive choice (only one allowed) or as fully optional (neither required), even in cases where one was necessary. This can result in incomplete adaptations. This behaviour aligns with the well-known inclusive–exclusive disjunction ambiguity described in requirements engineering and computational linguistics, where natural language “or” lacks explicit semantic constraints and is prone to misinterpretation. \\
For example:
\begin{itemize}
    \item \lstinline[language=json]|{"action": "increase_button_size", "target": "lamp", "value": 1.23}|: requires \texttt{value} but not \texttt{mode}.
    \item \lstinline[language=json]|{"action": "switch_mode", "target": "all", "mode": "voice"}|: requires \texttt{mode} but not \texttt{value}.
    \item \lstinline[language=json]|{"action": "highlight_button_border", "target": "button"}|: valid with neither, as neither field is relevant for this adaptation.
\end{itemize}
Having both fields is rather uncommon, but not impossible. This should handled further by the frontend and depends on the developer's implementation choices. Even with this type of prompt engineering, it sometimes still returns a \texttt{mode} field when a \texttt{value} field would be more appropriate, and vice versa.
To reduce misinterpretation, prompts should explicitly state inclusive meaning when a field is required, such as:
\mdblockquote{“The adaptation must at least include a value and a mode field, depending on the action type.”}
This minimises the risk of missing required parameters due to inclusive-exclusive disjunction ambiguity.

\subsection{Balancing Model Parameters}
Even with a well-written prompt, model behaviour is strongly affected by:
\begin{itemize}
    \item \textbf{Temperature}: how random the outputs are. Lower values (0.2–0.3) are better for consistent JSON.
    \item \textbf{Thinking budget}: how many reasoning steps the model takes before output. Too low, and it may skip checks; too high, and it can slow down or get stuck.
    \item \textbf{Timeout}: waiting time before falling back to mock or rule-based logic.
\end{itemize}
The Validator Agent especially needs more time and budget because it has a heavier job. This includes more complex reasoning and validation tasks like checking for inconsistencies in the adaptation or checking every adaptation against allowed actions, as well as removing irrelevant adaptations or merging duplicates.
In my testing, the validator with a low thinking budget and temperature often got stuck or took a very long time to respond, which is not ideal for real-time adaptations. The goal was to stay under a timeout of max. 30 seconds.
As described earlier, the best settings for the validator are:
\begin{itemize}
    \item \textbf{Model}: \texttt{gemini-2.5-flash} (larger than the lite models used in the other agents).
    \item \textbf{Temperature}: 0.3 (slightly higher to avoid “freezing” on unusual cases).
    \item \textbf{Thinking budget}: -1 (dynamic) to give it more room for complex validation.
    \item \textbf{Timeout}: 30 seconds instead of the default 15.
\end{itemize}

\subsection{Avoiding Hallucinations and Bad Values}
One common risk with LLM-driven adaptations is hallucination, where the model invents an action, target or value that doesn’t exist within the scope of the app.
To reduce this:
\begin{enumerate}
    \item The allowed actions list is always clearly included in the prompt.
    \item Targets are validated against the current UI state before applying them as well as by the Validator Agent.
    \item A list of focus items is included to guide the model's attention and provide additional context to minimize irrelevant outputs.
    \item Prompt clearly states the required fields and their expected values as well as adhering to the allowed actions and JSON contract.
\end{enumerate}
Even so, the validator sometimes has to fix agent mistakes.\\
For example, if the geometry agent outputs: \\
\lstinline[language=json]|{"action": "increase_button_size", "target": "button_unlock"}|\\
The validator can correct it to:\\
\lstinline[language=json]|{"action": "increase_button_size", "target": "button_unlock", "value": 1.5}|

\subsection{Importance of a Strict JSON Schema}
Another critical safeguard against hallucinations and malformed outputs is the use of a strict \texttt{response\_\\json\_schema} in the LLM API call (in this case Gemini). This schema explicitly defines:
\begin{itemize}
    \item The \textbf{structure} of the output object.
    \item The \textbf{allowed fields}, their types, and descriptions.
    \item The \textbf{required fields} and conditional requirements (e.g., either \texttt{value} or \texttt{mode} must be present depending on the action type).
\end{itemize}

By embedding this schema in every LLM request, the backend ensures that:
\begin{enumerate}
    \item Any response not matching the schema has a higher probability of being rejected before it reaches the frontend.
    \item Missing critical parameters (such as \texttt{value} for size changes) are caught early.
    \item Developers integrating the framework can rely on predictable keys, reducing integration errors.
    \item The LLM is gently “steered” towards valid outputs, as many LLM APIs use the schema as a structural hint during generation.
\end{enumerate}

The schema used in this framework is shown below:

\begin{lstlisting}[language=json,caption={SIF LLM Output JSON Schema}]
{
  "type": "object",
  "properties": {
    "adaptations": {
      "type": "array",
      "items": {
        "type": "object",
        "properties": {
          "action": { "type": "string",
            "description": "The type of UI adaptation to perform" },
          "target": { "type": "string",
            "description": "UI element or component to apply the adaptation to, 'all' is allowed" },
          "value": { "type": "number",
            "description": "Numeric multiplier for size changes (e.g., 1.5 for 50% larger)" },
          "mode": { "type": "string",
            "description": "Interaction or visual mode to switch to (e.g., 'voice')" },
          "reason": { "type": "string",
            "description": "Why this adaptation was suggested" },
          "intent": { "type": "string",
            "description": "Inferred user intent based on the event" }
        },
        "required": ["action", "target", "reason", "intent"],
        "oneOf": [
          { "required": ["value"] },
          { "required": ["mode"] }
        ]
      }
    }
  },
  "required": ["adaptations"]
}
\end{lstlisting}

This schema directly supports accessibility and safety goals: by preventing incomplete or semantically invalid adaptations from being applied, it reduces the risk of unpredictable UI behaviour. Combined with the Validator Agent, it forms a two-layer defence. Furthermore, schema validation stops malformed data at the source, while semantic validation ensures that even structurally valid adaptations are contextually appropriate.

\section{Performance and Evaluation Metrics for AI Logic}
\label{sec:perf-metrics-ai-logic}

This section measures the end-to-end behaviour of the SIF and MA\mbox{-}SIF pipelines:
\emph{Flutter client $\rightarrow$ WebSocket $\rightarrow$ FastAPI $\rightarrow$ LLM agents $\rightarrow$ validator $\rightarrow$ client}.
The backend processes \emph{one event at a time} (sequential), so the reported latencies reflect a full round-trip with model inference on the critical path.  
All runs used the same user identifier (\texttt{user\_seq}). On the very first event, no profile existed, so the server created a default profile; subsequent events appended to the profile’s \texttt{interaction\_history} (capped at 10). Qualitatively, later responses carried slightly richer rationales, consistent with the growing history window.

\subsection*{Method}
Two lightweight probes were run:
\begin{enumerate}
    \item \textbf{WebSocket round-trip}: a minimal ping-pong to capture end-to-end transport + processing overhead.
    \item \textbf{Deterministic event suite}: rotation over \texttt{tap} $\rightarrow$ \texttt{miss\_tap} $\rightarrow$ \texttt{voice} $\rightarrow$ \texttt{gesture}, repeated over a short sequence.
\end{enumerate}
Each response was classified as:
\begin{itemize}
    \item \texttt{validated\_by\_validator} (final list produced by the validator),
    \item \texttt{combined\_agent\_suggestions} (validator failed; raw agent outputs returned),
    \item \texttt{mock\_rule\_fallback} (all agents failed; rule engine response).
\end{itemize}
The strict JSON output schema used in the backend was enforced to compute “schema-valid \%”. To avoid artificial timeouts during heavier MA-SIF runs, client keep-alive pings were disabled. Run sizes were intentionally small (6-10 events) due to API free-tier limits; the goal is to surface architectural trends, not saturate the service.
\newpage
\subsection*{Configurations}
\begin{table}[h]
\centering
\caption{Exact agent settings for the three measured configurations.}
\label{tab:agent-settings}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccc}
\toprule
\textbf{Config} & \textbf{Agent} & \textbf{Model} & \textbf{Temperature} & \textbf{Thinking Budget} & \textbf{Timeout (s)} \\
\midrule
\multirow{1}{*}{SIF (single agent)} 
    & SIF agent & gemini-2.5-flash & 0.2 & dynamic (-1) & api-default \\
\midrule
\multirow{4}{*}{MA-SIF (balanced)} 
    & UI agent       & gemini-2.5-flash-lite & 0.2 & 0 & 15 \\
    & Geometry agent & gemini-2.5-flash-lite & 0.2 & 0 & 15 \\
    & Input agent    & gemini-2.5-flash-lite & 0.2 & 0 & 15 \\
    & Validator      & gemini-2.5-flash      & 0.3 & dynamic (-1) & 30 \\
\midrule
\multirow{4}{*}{MA-SIF (heavy)} 
    & UI agent       & gemini-2.5-flash & 0.2 & 2048 & 30 \\
    & Geometry agent & gemini-2.5-flash & 0.2 & 2048 & 30 \\
    & Input agent    & gemini-2.5-flash & 0.2 & 2048 & 30 \\
    & Validator      & gemini-2.5-flash & 0.3 & 2048 & 30 \\
\bottomrule
\end{tabular}%
}
\end{table}

\subsection*{Results}
\begin{table}[h]
\centering
\caption{Latency and correctness across configurations (sequential backend; \texttt{user\_seq} with default profile and growing history).}
\setlength{\tabcolsep}{4pt}
\small
\resizebox{\textwidth}{!}{%
\begin{tabular}{lrrrrrrrr}
\toprule
\textbf{Config} & \textbf{WS p50} & \textbf{WS p90} & \textbf{WS max} & \textbf{Suite p50} & \textbf{Suite p90} & \textbf{Suite max} & \textbf{Schema-valid} & \textbf{n} \\
 & (ms) & (ms) & (ms) & (ms) & (ms) & (ms) & (\%) &  \\
\midrule
SIF (single agent)    &  8240.99 &  9913.63 & 10649.21 &  8261.86 & 12034.12 & 12914.24 &  40.0 & 10 \\
MA-SIF (balanced)     &  9969.99 & 11151.80 & 11164.61 & 10988.27 & 14659.71 & 15234.09 &  70.0 & 10 \\
MA-SIF (heavy)        & 26875.99 & 27613.81 & 32009.47 & 27229.24 & 30443.48 & 31347.16 & 100.0 & 10 \\
\bottomrule
\end{tabular}%
}
\caption*{\textit{Legend.} WS p50/p90/max: WebSocket round-trip latency percentiles and maximum (ms). Suite p50/p90/max: end-to-end latency percentiles and maximum for the deterministic event suite (ms). Schema-valid: share of responses that passed JSON schema validation (\%). \textit{n}: number of events.}
\end{table}

Across all runs \emph{0\% combined\_agent\_suggestions} and \emph{0\% mock\_rule\_fallback} was observed: the validator returned a final list for every event. In the heavy configuration, some individual agent calls failed (API-side), yet the validator still produced a valid final list, so external behaviour stayed stable.  
Note: in earlier exploratory tests, fallbacks mainly appeared when the free-tier quota was hit or the validator timed out, which did not occur here. Furthermore, during this evaluation it was observed that the validator agent in the MA-SIF heavy config accepted more suggestions from the agents than in the other more balanced config.

\subsection*{Interpretation}
The pattern is clear:
\begin{itemize}
    \item Moving from \textbf{SIF (single agent)} to \textbf{MA\mbox{-}SIF (balanced)} increases median latency from $\sim$8.3\,s to $\sim$11.0\,s, but schema validity jumps from \textbf{40\%} to \textbf{70\%}. Even with zero thinking budget on the specialist agents, the \emph{validator} is pulling its weight.
    \item \textbf{MA\mbox{-}SIF (heavy)} pushes median latency to $\sim$27.2\,s, but schema validity reaches \textbf{100\%}, and the \texttt{reason}/\texttt{intent} fields become noticeably richer and more proactive.
    \item The short history window helps later events rationales get slightly better as context accumulates, although the biggest quality gain clearly comes from the validator’s (higher) thinking budget.
\end{itemize}

\paragraph{Practical read:} For accessibility-centric flows, a sensible pattern is:
\begin{enumerate}
    \item \textbf{Immediate, local rules} for fast feedback (e.g., enlarge target, boost contrast).
    \item \textbf{MA-SIF in the background} for the smarter, profile-aware follow-ups (e.g., switch to voice, simplify layout).
\end{enumerate}
This preserves responsiveness while still getting the benefits of multi-agent reasoning.

\subsection*{Limitations}
The runs were kept small (6-10 events) due to free-tier limits, so statistical power is modest—though the differences are large and consistent. The backend is sequential, so times reflect worst-case per-event latency with LLM on the critical path. The event suite is deterministic; other UI states can nudge absolute times, but the overall trade-off (latency $\leftrightarrow$ schema quality) is expected to hold.

\subsection*{Compact View (Full Results)}
\begin{lstlisting}[language=json,caption={Condensed view of settings and outcomes per configuration.}]
{
  "SIF_single_agent": {
    "settings": {"model":"gemini-2.5-flash","temp":0.2,"thinking_budget":"dynamic","timeout":"api-default"},
    "results": {
      "ws_latency_ms": {"p50":8240.99,"p90":9913.63,"max":10649.21},
      "suite_latency_ms": {"p50":8261.86,"p90":12034.12,"max":12914.24},
      "schema_valid_pct": 40.0,
      "classification": {"validated_by_validator":100.0,"combined_agent_suggestions":0.0,"mock_rule_fallback":0.0},
      "profile_note": "user_seq started with default profile; history grew across events"
    }
  },
  "MA-SIF_balanced": {
    "settings": {"agents":"3x flash-lite (no thinking); validator flash (dynamic thinking, 30s)"},
    "results": {
      "ws_latency_ms": {"p50":9969.99,"p90":11151.80,"max":11164.61},
      "suite_latency_ms": {"p50":10988.27,"p90":14659.71,"max":15234.09},
      "schema_valid_pct": 70.0,
      "classification": {"validated_by_validator":100.0,"combined_agent_suggestions":0.0,"mock_rule_fallback":0.0},
      "profile_note": "same user_seq; short interaction history window provides context"
    }
  },
  "MA-SIF_heavy": {
    "settings": {"agents":"all flash, thinking_budget=2048, timeouts=30s, temps=0.2/0.3"},
    "results": {
      "ws_latency_ms": {"p50":26875.99,"p90":27613.81,"max":32009.47},
      "suite_latency_ms": {"p50":27229.24,"p90":30443.48,"max":31347.16},
      "schema_valid_pct": 100.0,
      "classification": {"validated_by_validator":100.0,"combined_agent_suggestions":0.0,"mock_rule_fallback":0.0},
      "stability_note": "some agent calls failed; validator still produced final valid list"
    }
  }
}
\end{lstlisting}

\subsection*{Implications for the Framework}
Overall, the numbers back the architecture: MA-SIF + a capable validator improves output predictability and schema adherence; latency is tunable via thinking budgets, model choices and timeouts.

\paragraph{Recommended deployment profiles:}
\begin{itemize}
    \item \textbf{Tiered response:} apply \emph{deterministic} local adaptations instantly; let MA-SIF deliver deeper, profile-aware changes a few seconds later.
    \item \textbf{Balanced default:} \emph{gemini-flash-lite} for UI/geometry/input agents (\texttt{thinking\_budget=0}, \texttt{timeout=\\15s}); \emph{gemini-flash} validator with dynamic thinking and \texttt{timeout=30s}.
    \item \textbf{Heavy mode (opt-in):} enable larger validator thinking budgets for flows that demand airtight structure and richer rationale (e.g., clinical or regulated settings). Expect $\sim$2–3$\times$ latency vs. balanced.
\end{itemize}

\paragraph{Guidelines for Model Optimization:}
\begin{itemize}
    \item \textbf{Enhanced Accuracy and Reduced Schema Errors:} Prioritize increasing the validator thinking budget for optimal cost-effectiveness.
    \item \textbf{Reduced Latency:} Utilize \emph{gemini-flash-lite} agents with zero thinking time and set a validator timeout cap.
    \item \textbf{Comprehensive Explanations:} Increment validator thinking budget first, followed by agent budget, while maintaining low temperatures (0.2-0.3).
\end{itemize}

\paragraph{Failure handling}
Even with some agent API errors, the validator still made valid final lists. Returning other agent adaptations when the validator fails will provide a great fallback while the mock rule fallback is still the best backup for quota or timeout problems on all agents, although both are barely used in normal situations.

\paragraph{Conclusion}
Use MA-SIF for better quality and consistency, but only deal with the slower latency when it’s a necessity. Adding local rules will help keep the UI smooth for accessibility needs. As profiles and histories get bigger, there will be small but steady improvements in the reasoning quality over time, although the validator’s budget is still the main way to control accuracy. Using the gemini-pro model could potentially improve adaptations even further in important scenarios.

\section{Limitations and Challenges of LLM Integration}
Large Language Models make Smart Intent Fusion far more capable than any static rule set, but they also introduce new dependencies, performance constraints, and reliability issues. In this thesis, all reasoning was powered by the Gemini API, and while this enabled rapid development, it also shaped both the strengths and weaknesses of the final system.

\subsection{LLM selection}
Gemini was chosen as the sole LLM provider for this framework for a mix of practical and technical reasons. The generous free tier and large input/output token limits allowed for frequent iteration without cost becoming a limiting factor. The API provided access to both smaller, faster models (used for the UI, Geometry, and Input agents) and larger, reasoning-focused models such as \texttt{gemini-2.5-flash} or even \texttt{pro} (used for the Validator Agent). This made it possible to optimise speed where possible and allocate more resources to roles that needed deeper analysis.

Gemini’s handling of structured JSON output was also an advantage, as Smart Intent Fusion depends on predictable schema-compliant responses. While this project did not directly benchmark other models like GPT-5 or Grok, partly to keep the system stable during development and partly because Gemini’s free tier already covered the thesis’s usage without cost. Stability was important for building and testing MA-SIF without constantly adjusting prompts and entire agents for different model behaviours. However, it also means that all testing and performance observations in this chapter are specific to Gemini’s runtime behaviour, and the results may differ if another LLM were used.

\subsection{Reliability and Latency Constraints}
A constant challenge was balancing response quality with the speed needed for near real-time accessibility. Smaller “lite” models returned in fractions of a second and were ideal for the three suggestion agents. The Validator Agent, however, required the larger gemini-2.5-flash model to perform more complex and reliable checks across multiple agent outputs. The trade-off was that validation could sometimes become the slowest part of the pipeline, especially with its increased timeout (30 seconds) and dynamic thinking budget.
Network latency or temporary API slowdowns in the agents could lead to fewer suggestions being returned for a given event. In those cases, partial results were still applied rather than waiting or retrying for a full set.

\subsection{Hallucinations and Invalid Output}
Even with strict prompts and explicit allowed-action lists, hallucinations still appeared in the output. These took the form of actions outside the approved set, targeting elements that did not exist, or producing unreasonable scaling values (for example, value: 10). The Validator Agent was able to remove or correct most of these before they reached the frontend, but this came at the cost of extra processing time and complexity. Without validation, such outputs could have caused visual glitches or broken layouts, especially in geometry-related adaptations. The Input Adapter Layer or the Frontend itself, should as a fallback also have extra validation checks.

\subsection{Token Limits and Context Size}
Gemini’s token allowance was one of the main reasons for choosing it, but token limits were still a factor. Each agent’s prompt included the current event, the user profile, and the last few events from history, all in JSON format. In cases where the history was long or metadata verbose, this could approach the model’s input size limit. The solution was to truncate history in those cases, ensuring that the event and profile data always took priority, even if it meant losing some recent context. Furthermore the token limit per minute and per day also limited the number of requests that could be made, which is why the framework was designed to handle partial failures gracefully and still return useful adaptations even if some agents timed out or exceeded limits.

\subsection{Validator Complexity}
The Validator Agent is both the most important and the most resource-intensive part of MA-SIF. It merges outputs from multiple agents, removes duplicates, corrects invalid values, resolves conflicting suggestions, and ensures the final adaptations list passes schema validation. This workload made it prone to timeouts when handling a large number of adaptations at once. Increasing the thinking budget and timeout reduced these failures but also increased total response time, creating a constant balance between reliability and speed.

\subsection{Dependency on External APIs}
Finally, using an external LLM API means the framework is dependent on network availability and the stability of the provider. If the Gemini API is unavailable or returns errors, the system falls back to the rule-based logic. While this ensures baseline functionality, it also removes the more context-aware reasoning that makes Smart Intent Fusion valuable. In a production setting, this could be mitigated with on-device models or by integrating multiple LLM providers as backups.

\section{Future Directions for AI-Driven Adaptation}
While Smart Intent Fusion in its current form is functional and effective for the scenarios tested in this thesis, it is still a first iteration of what an AI-driven, multimodal UI adaptation system could be. The underlying architecture, especially the multi-agent design and the strict JSON-based API contract, was deliberately built with future extensions in mind. Several directions could significantly expand its capabilities and make it more adaptive and autonomous.

One natural evolution is the introduction of visual context through image, vision-based or even at runtime UI analyzer models. Currently, SIF relies on structured metadata from the frontend to understand the UI state. In future versions, a lightweight computer vision model could take periodic screenshots of the interface and produce a semantic map of UI elements, their sizes, positions, and visual contrasts. This map could then be passed to the LLM alongside the existing event, profile, and history data. The advantage of this approach is that the AI would be reasoning over actual UI layouts, rather than relying on the frontend to describe them accurately which can cause misinterpretations. This opens the door to truly context-aware adaptations. For example, increasing the size of the smallest actionable element on a crowded screen, even if it hasn’t yet caused an interaction error or accurately reposition elements closer to the user's focus.

Another promising direction is integrating user feedback into the adaptation loop. At present, SIF updates the user profile implicitly, based on interaction patterns or the user itself. A future version could prompt the user after significant adaptations with a quick, accessible feedback mechanism (“Was this change helpful?”). This feedback could be stored alongside interaction history and used by the LLM to adjust its decision-making over time. 

There is also scope for dynamic, UI-level code changes driven directly by the LLM. Currently, SIF works within a fixed set of allowed actions and values prone to some hallucinations. This could be expanded so that the LLM can modify layout constraints, create new UI elements, or reorganise screens entirely with safeguards in place to prevent breaking the interface. The LLM could use its own "hallucinations" to provide creative solutions for layout issues or user interactions. This would take SIF from an adaptation system to a full UI changing layer, capable of designing new interactions on demand.

Finally, the framework could explore multi-model, multi-provider reasoning and threading. At present, all reasoning is performed by Gemini and run sequentially, which simplifies development but limits the diversity and speed of outputs. Future versions could run agents across different LLM providers and threads or even mix LLMs with specialised non-language models (e.g., reinforcement learning agents for adaptation strategies), with the Validator Agent controlling and validating the final output. This would provide extra resilience against provider outages and allow different models to play to their strengths, as well as a strong speed improvement when asynchronous processing is implemented. Every agent could potentially run in its own thread or process and joined by the validator, allowing for true parallelism and faster overall response times.

\section{Chapter Summary}
This chapter presented Smart Intent Fusion (SIF) as the core reasoning layer of the adaptive framework. SIF combines multimodal inputs, user profiles, and recent interaction history to deliver personalised, context-aware UI adaptations. A hybrid approach of rule-based logic and LLM-driven reasoning ensures both reliability and flexibility, allowing essential accessibility features to work instantly while enabling more complex, context-sensitive changes.

The multi-agent architecture (MA-SIF) was introduced, with specialised agents for UI, geometry, and input adaptations, and a Validator Agent responsible for merging and cleaning outputs. This design improves reliability, supports partial fallbacks, and can be dynamically reconfigured via \texttt{sif\_config.json}. Prompt engineering emerged as a critical factor in ensuring valid, schema-compliant LLM output, with careful wording required to avoid logical misinterpretations.

Limitations of LLM integration including latency, occasional hallucinations, and reliance on a single provider, were mitigated through strict validation and fallback mechanisms. Finally, the chapter outlined future directions for SIF, such as adding visual UI context, integrating user feedback, enabling deeper UI changes, and exploring on-device AI models.

Overall, Smart Intent Fusion was presented not just as an algorithm, but as a modular, extensible smart reasoning layer designed to work across platforms, adapt to different users, and remain robust in the face of LLM unpredictability. It is the component that turns multimodal input into meaningful, personalised adaptations for reshaping the interface to fit the user.

%Chapter 5
% diagrams!
% Move to Chapter 5 (implementation)
    % Full, unabridged prompts for each agent from sif_config.json.
    % Exact numeric parameter configs for all agents — here in 4.7, keep just the validator’s example as you have it, but put the full table or config file in Chapter 5.
    % Any concrete backend code for prompt injection.
\chapter{An Adaptive Multimodal GUI Framework using LLMs}

\section{Introduction to an Adaptive Smart Home Controller}
The Adaptive Smart Home Controller is the practical proof-of-concept used to implement and validate the multimodal AI-driven GUI framework presented in this thesis. It serves as a concrete example of how the framework’s concepts, introduced in Chapter 3, can be applied in a real, interactive application. The Smart Home Controller simulates the control of typical household devices such as lights, thermostats, and door locks. While the devices themselves are virtual, the process of capturing inputs, interpreting them through the backend with user profiles and history, and applying adaptive changes to the interface is authentic and functionally representative of a real deployment.

The choice for a smart home context was deliberate. It offers a relatable and real-life use case structured set of interaction scenarios that cover a range of UI components, buttons, sliders, text elements which are central to accessibility-focused adaptations. Furthermore, it reflects real-world situations where users may have diverse abilities and input preferences. For example, a motor-impaired user might need larger buttons to avoid frequent miss-taps, while a visually impaired user may benefit from higher-contrast modes and enlarged text. By embedding these scenarios into a single, unified application, the Smart Home Controller provides a manageable but representative testbed for the framework’s adaptive capabilities.

The system adheres to the three-layer architecture established in earlier chapters. The frontend layer, implemented in Flutter (\texttt{adaptive\_ui\_app.dart}), renders the interface, captures user interactions, and applies adaptation instructions as they are received from the backend. The Input Adapter Layer (\texttt{adaptive\\\_ui\_adapter.dart}) acts as a middleware component, converting raw inputs from multiple modalities into the framework’s JSON-based event format and ensuring user profiles are retrieved or updated before events are transmitted. The backend layer (\texttt{backend.py}), built with FastAPI, Gemini API and MongoDB, implements the Smart Intent Fusion (SIF) process. This includes both deterministic, rule-based adaptations and multi-agent LLM-driven reasoning (MA-SIF), combining event data, user profiles, and interaction histories to produce targeted adaptation actions.

It is important to note that this first iteration does not yet incorporate every capability described in the framework’s long-term vision. Certain modalities, such as gesture recognition, are currently simulated through mock events to keep the implementation focused on the adaptation pipeline rather than input hardware integration. Throughout this chapter, each component is discussed in detail, with a clear distinction made between fully implemented functionality, simulated elements, and features that remain as future work.

\section{Development Environment}
The development environment for the Adaptive Smart Home Controller was chosen to support rapid prototyping, cross-platform deployment, and straightforward integration with the multimodal adaptation framework described in earlier chapters. It needed to provide a fast feedback loop during implementation, flexibility in UI design, and robust backend capabilities to support real-time Smart Intent Fusion. The final setup reflects a balance between practical constraints such as available hardware, time and technical requirements namely WebSocket support, database persistence, and LLM integration.

Flutter was selected for the frontend because of its ability to produce visually consistent applications across desktop, mobile, and web from a single codebase. The framework’s reactive UI model and composable widget system made it straightforward to create adaptive components whose properties such as size, color, and layout can be updated dynamically in response to backend instructions. Flutter’s hot reload feature also significantly reduced iteration time, which proved essential for testing the frequent, small adjustments needed when refining adaptation behaviors.

The backend is implemented in Python using FastAPI, chosen for its simplicity, asynchronous request handling, and native WebSocket support. FastAPI’s lightweight structure allowed the project to keep the adaptation pipeline transparent and easily modifiable, while still offering the performance needed for real-time interactions. MongoDB was selected as the database because its document-based structure aligns directly with the JSON contracts used throughout the framework. It stores user profiles, interaction histories, and adaptation logs without the need for complex schema migrations, making it well-suited for iterative development.

Development and primary testing took place on macOS 15.6, with additional runs on Windows 11 to confirm cross-platform compatibility. Both environments used Visual Studio Code with the Flutter and Dart plugins for frontend work, and Python tooling for backend development. This combination made it possible to run and debug both layers side-by-side, with the frontend communicating directly to a locally hosted backend via WebSocket and HTTP.

For clarity, the main environment components were as follows:
\begin{itemize}
    \item \textbf{Operating System:} macOS 15.6 (primary), Windows 11 (secondary testing)
    \item \textbf{Frontend Framework:} Flutter SDK 3.7.0 or higher
    \item \textbf{Programming Languages:} Dart 2.19.0+ (frontend), Python 3.9+ (backend)
    \item \textbf{Backend Framework:} FastAPI with Uvicorn ASGI server
    \item \textbf{Database:} MongoDB 6.0+ for persistent profile and interaction history storage
    \item \textbf{IDE:} Visual Studio Code with relevant Flutter/Dart and Python extensions
    \item \textbf{Communication:} WebSocket for real-time adaptation updates, HTTP for profile management and batch operations
    \item \textbf{Version control:} Git, with the repository hosted on GitHub for collaboration and version tracking.
\end{itemize}

While this configuration is sufficient for the current implementation, it is also designed to be portable. The backend can be deployed to cloud environments without modification, and the frontend can target iOS, Android, or desktop platforms simply by recompiling or adjusting the build configuration. This flexibility ensures that the same codebase can serve as both a research prototype and a potential foundation for future production-ready systems.

% implementation details
\section{Frontend Implementation: Smart Home Controller}
The frontend of the Adaptive Smart Home Controller is implemented in Flutter and serves as the primary user-facing component of the framework. Its role is to render the interface, capture user interactions across multiple modalities, and apply adaptation actions received from the backend in real time. Although the backend is responsible for reasoning about what adaptations to make, the frontend is where these adaptations become visible to the user and directly influence usability.

The application is structured around a scrollable list of device “cards,” each representing a smart home device such as a lamp or thermostat. These cards contain core interactive elements, including buttons for on/off actions, sliders for settings such as temperature, and labels for contextual information. The layout is designed to be minimal and uncluttered, ensuring that accessibility adaptations, such as increased button sizes or higher contrast modes, have a clear and immediate effect on the interface.

When a user pressed on an mockup event in the test panel underneath each device, the frontend captures the event along with metadata such as the element’s type for UI context, the interaction type, and any relevant parameters. This data is sent through the Input Adapter Layer, which standardises the event into the framework’s JSON contract before forwarding it to the backend via WebSocket. The choice of WebSocket ensures that adaptations can be returned and applied in near real time, a critical requirement for maintaining a smooth user experience in accessibility scenarios.
As a loading indicator, the frontend displays rotating linear gradient border around the card being interacted with, until the adaptations are received and applied.

\textbf{Input Capture}: The frontend layer captures raw inputs from multiple modalities, including touch, voice, and gestures. Each modality is handled through specific Flutter widgets and event listeners:
\begin{itemize}
    \item \textbf{Touch}: Taps or miss-taps on buttons/sliders, detected via for example Flutter’s \verb|GestureDetector| or \verb|onPressed| callbacks.
    \item \textbf{Voice}: Commands like “Turn on lamp” or “Unlock door”, mocked for this thesis but extensible to libraries like \verb|speech_to_text| or Web Speech API.
    \item \textbf{Gestures}: Hand movements (e.g., point, swipe) via mock events, with future support for MediaPipe.
\end{itemize}

Adaptations received from the backend after being processed by the Input Adapter Layer are applied immediately using Flutter’s reactive state management. For example, a \texttt{increase\_button\_size} action triggers an \texttt{AnimatedScale} widget update, enlarging the targeted button over a short animation to make the change more noticeable without disrupting the user’s flow. Similarly, a \texttt{increase\_contrast} action adjusts the application’s color scheme by updating theme parameters, while text-related adaptations update font sizes dynamically. This direct mapping between adaptation actions and Flutter widget properties allows the frontend to respond flexibly to a wide range of changes without requiring hardcoded layouts.

The current implementation focuses on touch-based interaction as the primary live modality, with voice and gesture inputs simulated for demonstration purposes. These mock events are triggered through test buttons in the interface, allowing the adaptation pipeline to be validated end-to-end without requiring physical input hardware. This approach enables reliable testing of adaptation logic while keeping the system architecture ready for integration with real voice or gesture recognition modules in the future.

Overall, \texttt{adaptive\_ui\_app.dart} demonstrates how the frontend layer of the framework can be implemented in a way that is both platform-independent and responsive to dynamic adaptation instructions. By separating UI rendering from adaptation logic and using the Input Adapter Layer as a bridge, the application remains modular, making it easier to extend or replace individual components without affecting the overall system.

% implementation details and extensibility eye tracking
\section{Input Adapter Layer: Multimodal Input Handling}
The Input Adapter Layer is responsible for bridging the gap between raw user interactions in the frontend and the structured event format required by the backend. It ensures that all inputs, regardless of modality, are represented consistently, allowing the backend’s adaptation logic to operate on a predictable and schema-compliant data structure. This layer also manages user profile checks before events are transmitted, ensuring that adaptation decisions are always made in the context of the most up-to-date profile information.

In the current implementation, the adapter intercepts mock events generated by the frontend, such as missed button presses, missed slider adjustments, or simulated voice commands. Each event is enriched with metadata, including the user’s identifier, a timestamp, the type of interaction, and any target element references. The adapter then converts this information into the JSON event contract high-level defined in Chapter 3, which serves as the standard interface between the frontend and backend. This contract includes fields for event type, source modality, target element, coordinates if applicable, confidence level, time stamp, user\_id and additional metadata such as the spoken command in the case of voice input.

    % Event handling in the Input Adapter Layer is structured to process multimodal inputs efficiently and reliably:
    % \begin{enumerate}
    %     \item \textbf{Future Modalities}: Eye tracking or BCI inputs, integrated via custom event handlers (e.g., \verb|sendEyeTrackingEvent|).
    %     \item \textbf{Event Standardization}: Each input is transformed into a JSON contract format:
    %     \begin{lstlisting}[language=json,firstnumber=1]
    %     {
    %       "event_type": "tap",
    %       "source": "touch",
    %       "timestamp": "2025-08-04T14:41:00Z",
    %       "user_id": "user_123",
    %       "target_element": "lamp",
    %       "coordinates": {"x": 100, "y": 200},
    %       "confidence": 0.9,
    %       "metadata": {"command": "turn_on"}
    %     }
    %     \end{lstlisting}
    %     The layer validates required fields \verb|(event_type, source, timestamp, user_id)| and enriches events with optional metadata (e.g., gesture type, voice command).
    %     \item \textbf{Event Forwarding}: Standardized events are sent to the SIF Backend Layer via WebSocket \\ (\verb|/ws/adapt|) for real-time processing or HTTP (\verb|/context|) for batch operations. The adapter ensures profile existence by querying \verb|GET /profile/{user_id}| before sending events, prompting a \verb|POST /profile| if needed.
    %     \item \textbf{Error Handling}: The layer detects and flags erroneous inputs (e.g., miss-tap if coordinates miss a target) and validates all inputs, as well as includes confidence scores to aid backend reasoning. It also handles network failures by queuing events locally for retry.
    % \end{enumerate}
    
    % \subsection{Example: UIAdapterAPI Implementation}
    % The \verb|AdaptiveUIAdapter| class serves as the core implementation of the Input Adapter Layer, designed as a modular template for extensibility. A typical implementation includes:
    % \begin{itemize}
    %     \item \textbf{Initialization}: Configures the adapter with a \verb|user_id| and a callback for handling adaptations (e.g., \verb|onAdaptations| to apply backend responses directly to the frontend).
    %     \item \textbf{Profile Management}: Checks profile existence (\verb|GET /profile/{user_id}|) and creates default profiles (\verb|POST /profile|) if absent.
    %     \item \textbf{Event Sending}: Provides a \verb|sendEvent| method to standardize and send events, taking an \\ \verb|Event eventData| parameter. The detailed implementation of the \verb|Event| class and the \verb|sendEvent| method is discussed in the next chapter.
    %     \item \textbf{Extensibility}: Supports new modalities via additional methods \\ (e.g., \verb|sendEyeTrackingEvent(target, gazeCoords)|) without altering any core logic.
    %     \item \textbf{Integration}: Embeds in the Frontend Layer with minimal setup (e.g., 1 line to initialize, 1 line per event).
    % \end{itemize}
    % This API minimizes developer effort while ensuring robust event handling, making it reusable across platforms and domains.

Before forwarding an event, the adapter queries the backend via HTTP to check whether a profile exists for the given user. If no profile is found, it prompts the frontend to initiate a profile creation request, using default parameters or pre-filled accessibility preferences where available. This mechanism prevents situations where adaptation requests are processed without the necessary user context, which could lead to ineffective or even counterproductive UI changes.

Communication with the backend is handled primarily through WebSocket for low-latency adaptation feedback. HTTP requests are used for profile management, batch operations, and other non-real-time interactions. This division ensures that profile updates and administrative tasks do not interfere with the responsiveness of live adaptations.

While the adapter is fully implemented for touch-based events or voice and gesture events which can be triggered manually within the frontend’s test interface, allowing the full adaptation pipeline to be exercised without requiring live hardware or third-party input recognition services. The adapter treats these simulated events identically to real ones, ensuring that integration with actual input sources in the future will require minimal changes and solely constrained to the frontend.

The Input Adapter Layer plays a crucial role in keeping the framework modular. By isolating the event formatting and profile validation logic here, the frontend can focus purely on UI rendering and interaction capture, while the backend can rely on receiving consistent, validated input. This separation not only improves maintainability but also makes it possible to reuse the adapter design across different frontends, such as a SwiftUI application or a Unity-based VR interface, without rewriting the backend communication logic.

\section{SIF Backend Layer: Implementation of Adaptation Logic}
The backend implements the decision‑making core of the framework. It receives standardised events from the adapter, fuses them with user profiles and recent interaction history, and returns concrete adaptation actions for the frontend to apply. The service is written in Python using FastAPI, with Uvicorn for serving requests and MongoDB for persistent storage of profiles and logs. WebSocket is used for low‑latency, bidirectional communication during interaction, HTTP is used for profile management and auxiliary endpoints.

\subsection{Webserver layout and endpoints}
The application exposes a WebSocket endpoint, \texttt{/ws/adapt}, that accepts JSON events matching the contract introduced earlier. Each message is parsed into a Pydantic \texttt{Event} model, the user profile is loaded from MongoDB, and the event, profile, and short history window are passed to the Smart Intent Fusion routine. The resulting adaptation list is returned on the same socket, allowing the frontend to update the UI immediately. For non‑interactive operations the backend offers \texttt{POST /profile} for profile creation, \texttt{GET /profile/{user\_id}} for retrieval, and a small set of diagnostic endpoints such as \texttt{GET /full\_history} and \texttt{GET /modalities}. Profile writes use FastAPI \texttt{BackgroundTasks}, which keeps the interaction path responsive while updates are persisted asynchronously.

\subsection{Data persistence and history management}
MongoDB stores user profiles in a \texttt{profiles} collection and event–adaptation pairs in a \texttt{logs} collection. Profiles are indexed by \texttt{user\_id} to allow direct lookups during interaction. Each time an event is processed the backend appends a compact JSON representation to the profile’s \texttt{interaction\_history}, capped to a small sliding window. This keeps the prompt context focused on recent behaviour while avoiding unnecessary growth in the database. For redundancy, a JSONL file mirrors the log entries during development, which simplifies offline inspection or debugging purposes when the database is reset.

\subsection{Smart Intent Fusion and MA‑SIF}
The fusion step supports two paths. A multi‑agent configuration, MA‑SIF, is the default. It calls a set of specialised LLM agents, UI, Geometry, and Input, each prompted with the current event, the user profile, and a short history. These agents propose structured adaptations in their domain, for example increasing button size, adjusting spacing, switching input mode, or triggering a button when intent is clear. Their outputs are then passed to a dedicated Validator agent that consolidates, filters, and normalises the suggestions into a final list. The validator removes duplicates, corrects out‑of‑range values, and ensures that every adaptation conforms to the allowed action set and includes a target and either a value or a mode and more.

A single‑agent SIF path is also available. It produces a complete adaptation list in one call and is useful when quick iteration is preferable over agent specialisation. Both paths share the same I/O schema, which keeps the frontend indifferent to which reasoning strategy is currently set to active.

\subsection{Structured outputs and guardrails}
To reduce hallucinations and schema drift, the backend requests JSON‑typed responses from the LLM with an explicit schema and allowed actions, as described earlier in chapter 4. The prompt defines required fields, expected types, and a one‑of constraint that demands either a numeric \texttt{value} or a categorical \texttt{mode}. Agent prompts are intentionally narrow, which improves determinism. The validator prompt is broader, since it must adjust conflicting suggestions and justify final choices. Despite these controls, invalid outputs still occur occasionally. However, the schema can be enforced at two distinct levels: by the agents and by the validator. This dual-level approach makes it easier for the validator to ensure that the agents' adaptations are compliant. Furthermore, it has a defensive layer that falls back to simple rules if all LLM validation fails.

\subsection{Rule‑based fallback and resilience}
A lightweight rule engine acts as a safety net when LLM calls time out or the provider is unavailable. It covers essential accessibility behaviours, for example increasing button size after a miss‑tap, switching to voice for users flagged as motor‑impaired, or enabling high‑contrast mode for visually impaired profiles. These rules are intentionally conservative, they guarantee progress without surprising the user, and they keep the implementation usable in environments with unstable connectivity.

\subsection{Latency, partial results, and error handling}
The WebSocket loop is designed to return something useful as quickly as possible. Agent calls run in sequence within short time budgets. If one agent fails to respond, the validator operates on the remaining suggestions rather than waiting indefinitely. The backend aims to keep per‑event processing below the threshold where users notice a lag on interaction, which is important for accessibility, particularly when enlarging targets immediately after an error. In practice, most adaptations are returned quickly by the smaller suggestion agents, while validation can become the slowest step in complex scenes. When validation exceeds its budget the backend returns the best available subset, then continues to append the event to the user’s history so future interactions benefit from the context.

\subsection{Security and CORS considerations}
During development the backend enables permissive CORS to simplify local testing across platforms. Profiles are keyed by \texttt{user\_id} rather than personal data. For production deployment, stricter origins, authentication, and encryption would be required. These measures are outside the scope of this prototype, but the separation of concerns in the current design makes them straightforward to add.

\subsection{Summary}
In its current form the backend delivers a complete adaptation pipeline: events arrive over WebSocket, profiles and short history windows are loaded from MongoDB, MA‑SIF produces structured suggestions, a validator consolidates them, and the result is returned to the frontend within a single interaction loop. When LLM reasoning is unavailable, conservative rules ensure the interface remains usable. This combination of multi‑agent reasoning, strict schemas, and rule-based fallbacks gives the system both flexibility and reliability, which is essential for accessibility‑focused adaptations.

\section{User Profile and Context Implementation}
The user profile and context subsystem was implemented as a dedicated data service in the SIF backend, designed to persist accessibility needs, interaction preferences, baseline UI configurations, and a capped history of recent events. MongoDB serves as the primary storage layer, with the \texttt{profiles} collection indexed on \texttt{user\_id} as described earlier for constant-time retrieval during event processing.

When a new event is received via the WebSocket (\texttt{/ws/adapt}), the backend queries the profile store using the supplied \texttt{user\_id}. If no profile exists, the profile will be created and added to the MongoDB using \texttt{insert\_one} before continuing, otherwise the profile will be retrieved using \texttt{find\_one} or updated using \texttt{update\_one}, ensuring that all adaptation decisions are made in a contextualized environment. This retrieval step is synchronous, guaranteeing that the most recent committed profile is available to the reasoning pipeline before any LLM or rule-based evaluation occurs. Interaction history is maintained using MongoDB’s \texttt{\$push} with \texttt{\$slice} operators to append the incoming event while capping the array length at 10 entries for efficiency. This rolling history provides the SIF agents with temporal context, enabling progressive personalization; for example, recognising a pattern of repeated miss-taps and proactively switching to voice mode. Updates to the history are performed asynchronously to avoid blocking real-time adaptation.

Profile documents are structured as JSON as described in chapter 3, containing four key sections: \texttt{accessibility\_needs} (boolean capability flags), \texttt{input\_preferences} (preferred modality and fallback order), \texttt{ui\_preferences} (default font size, button scale and more), and \texttt{interaction\_history} (recent event log). This schema strikes a balance between simplicity and extensibility, allowing new fields to be added without migration overhead.

To optimise performance and safety, all profile mutations are atomic, relying on MongoDB transactions to prevent race conditions when simultaneous events and updates occur. Adaptation logs are stored separately in the \texttt{logs} collection and mirrored to a local \texttt{adaptation\_log.jsonl} file, supporting offline analysis and reproducibility of evaluation results. Furthermore, if a profile update is in-flight during an event, the backend uses the latest committed profile, mitigated by client-side checks (waiting for \texttt{POST/PUT /profile} success) and server-side transactions.

This implementation ensures that every adaptation decision, whether produced by a static rule or the multi-agent LLM pipeline, is grounded in the user’s persisted profile and immediate interaction context, enabling consistent, personalised, and stateful UI behaviour across sessions.

\section{Dynamic Adaptation Mechanisms: Implementation Details}
The dynamic adaptation mechanism is the final stage in the interaction pipeline, where decisions made by the backend are translated into immediate and visible changes in the user interface. In the current implementation, this process is tightly integrated with Flutter’s reactive widget system, allowing adaptation actions to be applied without forcing full UI rebuilds or navigation resets.

When the backend sends an adaptation list over the WebSocket connection, the frontend parses each action and routes it to the relevant UI element. Actions are defined in the strict JSON schema described earlier, containing the \texttt{action} type, a \texttt{target} identifier, a \texttt{value} or \texttt{mode}, and a human-readable \texttt{reason} and \texttt{intent} it inferred from the adaptation. This standardisation allows the same adaptation handler to process diverse actions without requiring modality-specific logic.

The application of adaptations begins with a lookup to determine whether the \texttt{target} element exists in the current view. If it does, the corresponding widget state is updated directly. For example, an \texttt{increase\_button\_size} action adjusts the scale factor property of the button widget, and an \texttt{increase\_font\_size} action updates the text style parameter. Where appropriate, changes are animated using Flutter’s \texttt{AnimatedScale} or \texttt{AnimatedContainer} to make the transition noticeable without distracting the user. This animation step is particularly important for accessibility, as it helps the user understand that the interface has been modified intentionally. The loading indicator displayed around the card being interacted is now also triggered to stop playing.

Adaptations that affect the entire interface, such as \texttt{increase\_contrast} or \texttt{switch\_mode}, are handled at the application theme level. Contrast adjustments update the color palette by replacing the primary and background colors with higher-contrast alternatives, while mode switches alter the active input modality, for example switching from touch to voice. These global changes are propagated across all widgets automatically through Flutter’s state management, ensuring consistency without manually updating each element.

Conflicting adaptations such as multiple agents proposing different size adjustments for the same element are resolved in the backend before reaching the frontend, using the validator agent to select or merge the most appropriate action. This guarantees that the frontend always receives a coherent, non-overlapping set of changes, reducing the complexity of applying them in real time.

Not all adaptations are implemented with live modality inputs. For demonstration purposes, actions triggered by voice or gesture events are generated from simulated events in the frontend’s test panel in each device card. However, these simulated actions follow the same processing path as real events, which means that integrating actual input sources in the future will require no changes to the adaptation mechanism itself.

%\section{Cross-Platform Implementation Considerations}
% swiftui and unity 

\section{Design Decisions}
The design of the Adaptive Multimodal GUI Framework using LLMs reflects a series of deliberate choices aimed at balancing accessibility, performance, scalability, extensibility, and ease of integration. These decisions were made with the primary goal of delivering real-time, personalised adaptations for motor-impaired, visually impaired, and hands-free users, while ensuring that the framework remains modular and adaptable to future platforms and domains.

\subsection{Modularity Over Monolithic Design}
\textbf{Decision:} The framework adopts a modular three-layer architecture (Frontend, Input Adapter, SIF Backend) with clear separation of concerns, connected through a standardised JSON contract.

\textbf{Reasoning:}
\begin{itemize}
    \item \textbf{Flexibility:} Each layer can be updated or replaced independently, enabling deployment across different platforms such as Flutter, SwiftUI, or Unity without reworking the entire system.
    \item \textbf{Extensibility:} The JSON contract in the Input Adapter Layer allows new modalities (e.g., eye tracking) to be integrated without modifying backend logic.
    \item \textbf{Developer accessibility:} Modularity simplifies integration, requiring minimal code for event handling and adaptation application.
\end{itemize}

\subsection{WebSocket for Real-Time vs. HTTP for Batch Processing}
\textbf{Decision:} The framework uses WebSocket (\texttt{/ws/adapt}) for real-time event processing and adaptation delivery, with HTTP (\texttt{/full\_history, /profile}) for debugging, developer tooling and profile management.

\textbf{Reasoning:}
\begin{itemize}
    \item \textbf{Low Latency:} WebSocket enables fast and bidirectional sending of data like adaptations (e.g., scaling a button after a miss-tap).
    \item \textbf{Reliability:} HTTP supports robust profile updates (\texttt{POST}/\texttt{PUT} \texttt{/profile}), ideal for non-real-time scenarios or debugging.
    \item \textbf{Accessibility:} Real-time feedback enhances usability for motor-impaired or hands-free users, where delays could disrupt interaction. 
\end{itemize}

\subsection{MongoDB for Persistent Storage}
\textbf{Decision:} MongoDB is used for storing user profiles, interaction history, and adaptation logs, with \texttt{user\_id} indexing and capped history arrays (10 events).

\textbf{Reasoning:}
\begin{itemize}
  \item \textbf{Scalability:} MongoDB’s NoSQL design and indexing ensure fast queries for large user bases, critical for real-world deployment.
  \item \textbf{Flexibility:} JSON-like documents align with the framework’s JSON contract, simplifying profile/history storage.
  \item \textbf{Continuous Learning:} Retaining a limited history supports adaptive behaviour, such as making permanent size adjustments after repeated miss-taps.
\end{itemize}

\subsection{Rule-Based Fallback with LLM Reasoning}
\textbf{Decision:} SIF combines rule-based logic (e.g., \texttt{if miss\_tap then increase\_size}) with LLM reasoning for creative adaptations, with rules as a fallback for LLM failures or time-outs, since LLM's can have high respond latencies.

\textbf{Reasoning:}
\begin{itemize}
  \item \textbf{Reliability:} Rules ensure deterministic adaptations (e.g., button enlargement for miss-taps) when LLM responses are unavailable or hallucinate.
  \item \textbf{Novelty:} LLM enables context-aware, proactive suggestions (e.g., \verb|switch_mode: voice| for hands-free users), advancing beyond static rules.
  \item \textbf{Accessibility:} Hybrid approach ensures consistent support for motor-impaired, visually impaired users (e.g., high-contrast text).
\end{itemize}

\subsection{Multi-agent LLM reasoning (MA-SIF) vs single-agent LLM reasoning (normal SIF)}
A key architectural decision in the framework is the adoption of multi-agent LLM reasoning (MA-SIF) over a single agent LLM approach (normal SIF). In the single agent SIF model, one LLM is responsible for interpreting all input events and generating adaptation actions. While this simplifies integration and reduces system complexity, it can limit the granularity and specialization of adaptation logic, especially as the diversity of user needs and input modalities grows.

MA-SIF, by contrast, distributes reasoning across multiple specialized LLM agents, each focused on a distinct domain such as UI adaptations, geometry/layout changes, input modality management, and validation. This separation of concerns enables each agent to leverage tailored prompts, domain-specific knowledge, and focused reasoning strategies, resulting in more nuanced and context-aware adaptation suggestions. The validator agent further ensures that outputs from other agents are coherent, non-conflicting, and accessibility-compliant.

The multi-agent approach offers several advantages: \begin{itemize} \item \textbf{Scalability:} New agents can be added to address emerging modalities or adaptation domains without disrupting existing logic. \item \textbf{Extensibility:} Prompts and allowed actions can be updated independently for each agent, supporting rapid iteration and domain-specific improvements. \item \textbf{Robustness:} Specialized agents reduce the risk of LLM hallucinations or conflicting adaptations, as validation is enforced before application. \item \textbf{Personalization:} Agents can incorporate user profiles and history more effectively, enabling targeted adaptations for motor-impaired, visually impaired, or hands-free users. \end{itemize}

\section{Implementation Challenges and Solutions}
Developing the Adaptive Smart Home Controller exposed several practical challenges, ranging from LLM-specific issues to general real-time system concerns. These were addressed through a mix of architectural decisions, fallback mechanisms, and compromises designed to keep the prototype functional and reliable under varying conditions.

\subsection{LLM reliability and output consistency}
One of the most persistent challenges was ensuring that the multi-agent LLM pipeline returned valid, schema-compliant output. Despite strict prompts and an explicit allowed-actions list, hallucinations still occurred, producing unsupported actions, targeting non-existent elements, or returning unreasonable values such as excessively large scaling factors. These errors risked breaking the layout or producing jarring visual changes. To mitigate this as described earlier, a validator agent was introduced to consolidate and correct suggestions before they reached the frontend. However, this validation step added extra latency and still could not guarantee complete protection against invalid values, making additional frontend-side checks a sensible next step for future versions.

\subsection{Performance under real-time constraints}
Adaptation latency was a critical factor for usability, particularly in accessibility scenarios where delayed feedback can reduce trust in the system. Smaller LLM models were assigned to the suggestion agents to keep their execution time within fractions of a second, while the validator, which required more context and reasoning, used a larger model with a higher timeout budget. Even so, occasional slowdowns occurred due to network latency or provider-side delays. The backend was therefore designed to apply partial results if all agents did not respond in time, ensuring that at least some adaptations reached the user quickly.

\subsection{Safeguards against malicious or replay attacks}
In its current form, the system does not include strong safeguards against replay attacks or intentionally crafted events designed to trigger disruptive adaptations. This is acceptable in a controlled research environment but would require strict validation and authentication for production deployment. Measures such as signing WebSocket messages, verifying sequence numbers, and rejecting stale or malformed events would be necessary to prevent exploitation as well as extra protections in the adapter or frontend. Furthermore is \texttt{user\_id} hijacking, where an attacker submits events under a different user’s profile possible, since no \texttt{user\_id} checks are done on returning adaptations. Future mitigations such as implementing \texttt{user\_id} verification and event signing will be necessary to address these vulnerabilities. This was intentional for the scope and development time constraints of this research.

\subsection{Testing with incomplete modalities}
Live gesture tracking and voice inputs were not integrated in this iteration, which meant that related adaptations had to be tested using simulated events. While this allowed the adaptation logic to be validated end-to-end, it did not account for the noise, recognition errors, or latency introduced by real input devices. Future work will need to focus on replacing these simulated events with actual input sources to fully evaluate performance in realistic conditions.

\subsection{Security and trust boundaries}
In its current implementation state, the backend accepts connections from any origin and does not require authentication for profile creation or event submission. Currently a pure localhost setup is implemented for the entire backend. This was a deliberate decision to speed up testing across platforms, but would need to be replaced with stricter CORS rules, token-based authentication, and access control in production. Without these measures, the system is vulnerable to unsolicited adaptation requests from external sources. Furthermore, some config files such as \texttt{sif\_config.json} do not have the necessary validation checks in the backend, while the validator agent is necessary, nothing stops it from being removed.

\section{Chapter Summary}
This chapter has detailed the implementation of the Adaptive Smart Home Controller as the primary proof-of-concept for the multimodal AI-driven GUI framework. The system was realised as a three-layer architecture, consisting of a Flutter-based frontend for rendering and applying adaptations, an Input Adapter Layer for standardising and transmitting events, and a FastAPI-based backend implementing multi-agent Smart Intent Fusion with MongoDB for persistent profile and history storage.

Key implementation aspects included the construction of a dynamic adaptation pipeline capable of applying size, contrast, modality, and content changes in real time, and the use of a strict JSON schema to maintain consistency between layers. While some modalities such as voice and gesture inputs were simulated for demonstration purposes, the system was designed so that integrating real devices will require minimal architectural changes.

The prototype also faced practical challenges, including LLM output validation, latency management, security considerations, and the absence of certain safeguards against malicious or replayed events. These were addressed through a combination of a validator agent, conservative rule-based fallbacks, partial-result handling, and a modular design that isolates critical components.

In its current state, the implementation demonstrates that the framework can deliver personalised, accessibility-focused adaptations in a responsive and modular manner, even under the constraints of a prototype environment. The next chapter evaluates this implementation through a feasibility study, assessing its responsiveness, adaptability, and practical potential in simulated real-world scenarios.

%Chapter 6
\chapter{Evaluation}
% Feasibility Study and System Demonstration

\section{Evaluation Overview}
The goal of evaluating the implementation is by feasibility study, to check whether the multimodal, AI-driven adaptation pipeline works end-to-end under realistic interaction traces, and whether it does so in a way that is useful for accessibility. Concretely:
\begin{enumerate}
    \item \textbf{Adaptation validity:} Does the backend return schema-valid adaptation objects across different user profiles and modalities?
    \item \textbf{Accessibility relevance:} Do the proposed adaptations match the user’s needs (motor, visual, hands-free) and the observed errors (e.g., miss-taps, slider overshoot)?
    \item \textbf{Latency:} What is the end-to-end response time (p50/p90/max) under the chosen backend configuration?
\end{enumerate}

\section{Methodology}
\paragraph{Profiles and Runs:}
Six user profiles (P0--P5) were used to cover a range of interaction needs, including neutral, motor-impaired, visual-impaired, and hands-free preferences, including combined needs:
\begin{itemize}
    \item \textbf{P0 (Baseline):} No specific accessibility needs.
    \item \textbf{P1 (Motor-impaired):} Requires larger touch targets; keyboard preferred navigation.
    \item \textbf{P2 (Visual-impaired):} Needs adaptations for visual impairment (e.g., larger font).
    \item \textbf{P3 (Hands-free):} Prefers voice control and hands-free interactions.
    \item \textbf{P4 (Combined):} Needs adaptations for both motor and hands-free interactions (voice preferred).
    \item \textbf{P5 (Combined):} Needs adaptations for motor and visual impairments (voice preferred).
\end{itemize}
For each of the six profiles, two runs were executed to allow a small interaction history to build up over time. Each run consisted of the same 7-event sequence (14 events per profile, 84 events total). 

\begin{table}[h]
\centering
\caption{User profile mapping for evaluation runs}
\begin{tabular}{ll}
\toprule
\textbf{Profile} & \textbf{Declared Needs} \\
\midrule
P0 & Baseline (no declared needs) \\
P1 & Motor-impaired, keyboard preferred \\
P2 & Visual-impaired, larger font \\
P3 & Hands-free preferred, voice modality \\
P4 & Motor-impaired + hands-free, voice modality \\
P5 & Visual-impaired + motor-impaired, voice modality \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Event Suite:}
The event suite mixes errors and successful attempts across modalities and targets (smart-home “lamp”, “thermostat”, “lock”):
\begin{itemize}
    \item \texttt{miss\_tap} on buttons (lamp/lock)
    \item \texttt{voice} commands (turn\_on/unlock/adjust)
    \item \texttt{gesture} inputs (point at lamp)
    \item \texttt{slider\_miss} with overshoot (thermostat)
\end{itemize}
This combination intentionally triggers common accessibility challenges (tap precision, slider control, and mode switching between touch/voice/gesture).

\paragraph{Backend Configuration and Metrics:} 
All traces were evaluated using the MA-SIF (balanced) configuration, with each event’s adaptation response checked for schema validity. For every event, the classification path (validator vs. fallback) and latency metrics (p50/p90/max) were recorded. Analysis was performed using the project’s extraction and evaluation scripts, which flatten the JSONL logs to CSV, compute summary statistics, and export figures and tables for reporting.
\begin{table}[htb]
\centering
\caption{MA-SIF (balanced) agent configuration}
\label{tab:ma-sif-config}
\begin{tabular}{lcccc}
\toprule
\textbf{Agent} & \textbf{Model} & \textbf{Thinking Budget} & \textbf{Temperature} & \textbf{Timeout (s)} \\
\midrule
UI         & gemini-flash-lite & 0   & 0.2 & 15 \\
Geometry   & gemini-flash-lite & 0   & 0.2 & 15 \\
Input      & gemini-flash-lite & 0   & 0.2 & 15 \\
Validator  & gemini-flash      & -1  & 0.3 & 30 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Operational definitions (micro-clarifications).}
\textbf{Schema-valid} means the response object conforms to the adaptation JSON schema (valid action names, allowed parameter ranges, resolvable UI targets, and consistency checks enforced by the Validator). 
\textbf{Latency} is measured client-side per event as $\Delta t = t_{\text{recv}} - t_{\text{send}}$, covering multi-agent coordination and Validator checks.

\section{System Demonstration and Experimental Setup}
\paragraph{Evaluation Scenarios:}
The Adaptive Smart Home Controller using LLMs was evaluated by executing the 7-event sequence with 2 runs on each profile. Typical flows included:
\begin{itemize}
    \item A user misses a tap on the lamp button $\rightarrow$ the system proposes larger buttons/borders and optionally switches to voice input.
    \item The thermostat slider overshoots $\rightarrow$ the system proposes a larger slider and spacing adjustments.
    \item A voice command is issued (turn on lamp / unlock door) $\rightarrow$ the system either confirms voice as the preferred mode or combines it with gesture hints.
\end{itemize}

\paragraph{Hardware and Environment:}
All experiments in this thesis were executed on a single development machine in a controlled environment (no network contention). Latency is end-to-end from event send to response receipt on the client.
Hardware configuration:
\begin{itemize}
    \item \textbf{Hardware:} MacBook Pro, M1 Max CPU, 32\,GB RAM, 32-core GPU
    \item \textbf{Software:} macOS 15.6 (Sequoia), Python 3.9+
    \item \textbf{Environment:} Localhost, no external API calls
\end{itemize}
\newpage
\section{Results and Observations}
\subsection{Overall Summary}
Across 84 events and 6 user profiles, the system produced adaptations with:
\begin{itemize}
    \item \textbf{Schema-valid outputs:} 84.52\% of events.
    \item \textbf{Classification path:} 100\% processed via the Validator (no fallback used).
    \item \textbf{Latency:} p50 13.19 s, p90 17.13 s, max 21.10 s.
\end{itemize}

\begin{table}[htb]
  \centering
  \caption{Overall feasibility results with MA-SIF (balanced).}
  \label{tab:overall-feasibility}
  \begin{tabular}{lr}
    \toprule
    \textbf{Metric} & \textbf{Value} \\
    \midrule
    Events (total) & 84 \\
    Users (total) & 6 \\
    Schema-valid outputs (\%) & 84.52 \\
    Validated by Validator (\%) & 100.00 \\
    Latency $p_{50}$ (s) & 13.19 \\
    Latency $p_{90}$ (s) & 17.13 \\
    Latency max (s) & 21.10 \\
    \bottomrule
  \end{tabular}
\end{table}

The most frequent actions were:
\begin{itemize}
    \item \texttt{switch\_mode} (87): often recommending voice mode.
    \item \texttt{increase\_button\_size} (74), \texttt{increase\_button\_border} (69), \texttt{increase\_font\_size} (66).
    \item \texttt{increase\_slider\_size} (28), \texttt{increase\_contrast} (19).
    \item Less frequent: \texttt{trigger\_button} (9), \texttt{adjust\_spacing} (9).
\end{itemize}
Because multiple suggestions can be attached to one event, action counts exceed the number of events. Top targets were the lamp, thermostat, and lock, with miss-taps most often on lamp and lock (by design).

\paragraph{Interpretation:} The system strongly prioritizes motor-related support (larger targets, borders, spacing) and modal switching to voice when it detects tap/slider errors, exactly the pattern that is needed for motor-impaired and hands-free users. Vision-related support (font size and contrast) also appears consistently, but is less prominent than motor support in this trace.

\subsection{Per-Profile Outcomes}
Per-profile schema validity and latency (median p50, higher-tail p90, and worst-case max):
\begin{itemize}
    \item \textbf{P0 (baseline)}: 78.57\% valid; $p50 \approx 16.02\,\mathrm{s}$; $p90 \approx 20.23\,\mathrm{s}$; max $21.10\,\mathrm{s}$.
    \item \textbf{P1 (motor)}: 85.71\% valid; $p50 \approx 13.19\,\mathrm{s}$; $p90 \approx 16.78\,\mathrm{s}$; max $17.43\,\mathrm{s}$.
    \item \textbf{P2 (visual)}: 85.71\% valid; $p50 \approx 13.65\,\mathrm{s}$; $p90 \approx 16.46\,\mathrm{s}$; max $19.04\,\mathrm{s}$.
    \item \textbf{P3 (hands-free)}: 71.43\% valid; $p50 \approx 11.82\,\mathrm{s}$; $p90 \approx 14.68\,\mathrm{s}$; max $16.49\,\mathrm{s}$.
    \item \textbf{P4 (motor + hands-free)}: 85.71\% valid; $p50 \approx 13.63\,\mathrm{s}$; $p90 \approx 15.76\,\mathrm{s}$; max $17.91\,\mathrm{s}$.
    \item \textbf{P5 (visual + motor)}: 100.00\% valid; $p50 \approx 12.70\,\mathrm{s}$; $p90 \approx 16.25\,\mathrm{s}$; max $16.60\,\mathrm{s}$.
\end{itemize}

\begin{table}[htb]
  \centering
  \caption{Per-profile schema validity and latency under MA-SIF (balanced).}
  \label{tab:per-profile}
  \begin{tabular}{l l r r r r}
    \toprule
    \textbf{Profile} & \textbf{Declared needs} & \textbf{Valid (\%)} & \textbf{$p_{50}$ (s)} & \textbf{$p_{90}$ (s)} & \textbf{max (s)} \\
    \midrule
    P0 & Baseline & 78.57 & 16.02 & 20.23 & 21.10 \\
    P1 & Motor & 85.71 & 13.19 & 16.78 & 17.43 \\
    P2 & Visual & 85.71 & 13.65 & 16.46 & 19.04 \\
    P3 & Hands-free & 71.43 & 11.82 & 14.68 & 16.49 \\
    P4 & Motor + Hands-free & 85.71 & 13.63 & 15.76 & 17.91 \\
    P5 & Visual + Motor & 100.00 & 12.70 & 16.25 & 16.60 \\
    \bottomrule
  \end{tabular}
\end{table}

\paragraph{Observations:}
\begin{itemize}
    \item Highest validity appears for combined needs (P5), likely because the event suite provides strong, consistent signals (miss-taps + slider overshoot) that align with the rules and model prior for motor/visual support.
    \item Lowest validity is P3 (hands-free) at 71.43\%. Even though P3 shows the best latency, hands-free preference alone may result in fewer structural UI changes (e.g., fewer size/contrast edits), and the Validator may reject marginal suggestions more often. This indicates room to enrich the hands-free policy (e.g., more explicit voice/gesture affordances and confirmation prompts).
    \item Latency is in the 11.8--16.0 s p50 band across profiles under the MA-SIF (balanced) config. The spread between p50 and p90 ($\approx 3$--$4\,\mathrm{s}$) suggests predictable tail behavior, with a single global max near 21 s.
\end{itemize}

\subsection{Accessibility Impact Analysis}
The distribution of suggested actions aligns with both the declared needs of the profiles and the error patterns in the event suite:

\begin{itemize}
  \item \textbf{Motor support (P1, P4, P5).} Frequent enlargement of buttons and sliders, thicker borders, and increased spacing; when repeated miss-taps occur, the policy often proposes \texttt{switch\_mode} to voice to reduce precision demands.
  \item \textbf{Visual support (P2, P5).} Font-size increases appear consistently; contrast boosts are present but less frequent, which is expected given the suite skews toward motor-related errors (miss-taps, slider overshoot).
  \item \textbf{Hands-free (P3, P4).} Many \texttt{switch\_mode} $\rightarrow$ voice suggestions; adding explicit voice-first affordances (prompting, feedback, focus outlines) is a promising next step to raise acceptance.
\end{itemize}

Overall, the system responds not only to immediate problems during interaction, such as missed taps or slider overshoots, but also to the user’s stated preferences, such as wanting to use voice commands. This means the system can react to fix recent mistakes while also making changes in advance to support the way a user prefers to interact.

In practice, this results in a strong focus on motor support, such as making buttons and sliders bigger and adding thicker borders or more space. Visual support appears where needed, such as increasing text size and contrast. For hands free users, the system often switches to voice control to reduce the need for precise touch. The share of actions aimed at each of these accessibility categories is shown in following Table~\ref{tab:overall-accessible-share}, divided into motor, visual, and hands free support.

\begin{table}[htb]
\centering
\caption{Overall accessibility-targeted actions from the global action distribution. Categories: motor, visual, hands-free.}
\label{tab:overall-accessible-share}
\begin{tabular}{l r}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Total actions (overall) & 361 \\
Accessibility-targeted (motor/visual/hands-free) & 352 \\
Share accessibility-targeted (\%) & 97.51 \\
\midrule
Aggregate aligned \% across profiles (excluding P0)\textsuperscript{*} & 63.72 \\
\bottomrule
\end{tabular}

\smallskip
\raggedright \textsuperscript{*}Computed from each profile's top-5 actions only; may underestimate alignment if lower-frequency actions are omitted.
\end{table}

\subsection{Adaptation Performance (Latency)}
With \textbf{MA-SIF (balanced)}, median latency sits around 13 s overall (p50 13.19 s / p90 17.13 s / max 21.10 s). For a first feasibility pass with multiple agents and validation, this is acceptable for asynchronous UI adaptation (e.g., adapting after an error or between interactions). For strict real-time use (e.g., continuous VR hand-tracking), optimizations are needed (Section~6.6).

\section{Discussion of Results}
\paragraph{What worked:}
The pipeline produced valid, targeted adaptations in most cases (84.5\% valid overall, 100\% through the Validator path). The action distribution shows the system prioritizes the right things for accessibility (larger targets, borders, contrast/font, and mode switching). This is consistent with the global action mix, where the majority of suggestions are accessibility-focused rather than generic UI operations.

\paragraph{What needs work:}
Hands-free profiles (P3) had lower validity, likely because they trigger fewer structural UI changes while the Validator rejects marginal or redundant mode suggestions. The plan is to surface explicit voice UI hints, add confirmation steps, and investigate voice-first layouts to improve acceptance. In addition, the share of \emph{profile-aligned} actions should be raised by weighting suggestions more strongly toward the user’s declared needs, not only toward recent errors.

\paragraph{Why latency looks like this:}
The balanced multi-agent + Validator path introduces coordination and checking overhead. This is the trade-off for higher correctness and clearer rationales. If needed, fast paths for “obvious” fixes (e.g., miss-tap $\rightarrow$ enlarge target) can be exposed without waiting for full aggregation. Complementary optimizations include prompt refining for common patterns, smaller on-device models for first-pass triage, and caching/merging of repeated suggestions. These latency results are close to Section~4.8, which provides further evidence for the results and arguments made.

\paragraph{Discussion of Table \ref{tab:overall-accessible-share}:}
The table shows that \emph{97.51\%} of all suggested actions fall into accessibility-targeted categories (motor, visual, hands-free). This indicates that the adaptation policy strongly prioritizes accessibility-oriented changes over generic UI operations. In practice, almost the entire action budget goes to increasing target size/borders/spacing, raising font size or contrast, or switching to voice mode when appropriate; only a small remainder (e.g., \texttt{trigger\_button}) is non-accessibility.

However, being “accessibility-targeted” is not the same as being “profile-aligned.” The aggregate alignment across profiles (excluding the baseline P0) is \emph{63.72\%}, meaning roughly two-thirds of the suggested actions match the declared needs of the active profile. The remaining third are either (i) generic accessibility improvements that are not harmful but not strictly aligned to the profile (e.g., a motor-focused user receiving a visual tweak), or (ii) over/under-targeted suggestions that the Validator still accepts. This aligns with the per-profile analysis, where hands-free (P3) showed lower validity and alignment, while combined-needs profiles (P4/P5) aligned well.

Two factors explain the gap between 97.51\% (coverage) and 63.72\% (personalization): (1) the event suite skews toward motor-related errors (miss-taps, slider overshoot), so motor actions are frequently triggered even for profiles without motor needs; and (2) the current rules treat \texttt{switch\_mode} as hands-free support, which is correct for most cases here but does not yet encode richer voice-first affordances (e.g., turn-taking prompts, confirmation feedback, and voice-first layouts). Overall, the table supports that the system is strongly accessibility-focused by design, while also pointing to a clear next step: make the policy more \emph{need-aware} so that a larger share of actions are aligned to the user’s declared needs, especially for hands-free profiles.

\section{Study Limitations}
\begin{itemize}
    \item \textbf{Feasibility only (no user study).} Evaluation happened with synthetic but structured traces; no human-subject study is included in this chapter.
    \item \textbf{Synthetic yet representative traces.} The event suite is representative, but not exhaustive; real users might show more variability (e.g., dwell, path, timing).
    \item \textbf{Single backend configuration.} Only MA-SIF (balanced) is tested here. Comparing against single-agent or heavier multi-agent configs would strengthen the latency/correctness trade-off. (SIF-only performance is in Section~4.8 but without multiple user profiles.)
    \item \textbf{Small $n$ per profile and per run.} Two runs (14 events/profile) limit statistical power; a larger longitudinal log would better capture learning effects across runs.
    \item \textbf{Local, single-machine environment.} Numbers reflect an offline/local setting without network contention; on-device or networked deployments may shift latency.
\end{itemize}

\section{Study Conclusion}
In summary, the framework demonstrated robust adaptation performance, consistently generating valid outputs (84.5\% overall, 100\% via the Validator) and prioritizing accessibility actions such as motor support and voice mode switching. Latency under the balanced multi-agent configuration remained within acceptable bounds for asynchronous UI updates (median $\sim$13\,s, $p_{90}$ $\sim$17\,s), with further optimization possible for more demanding scenarios. The evaluation also highlighted opportunities to enhance hands-free support, particularly by enriching voice and gesture affordances to improve schema validity for users with hands-free interaction needs.

%Chapter 7
\chapter{Discussion and Future Work}

\section{Overview}
This chapter interprets the results of this thesis in the broader context of accessibility and human-computer interaction (HCI), linking the implemented framework to existing research and real deployment. It first discusses implications for adaptive interface design, then summarises key findings and contrasts them with related work. Limitations and threats to validity are addressed explicitly. The chapter closes with a focused future-work,spanning near-term engineering improvements and longer-term research directions, including on-device, autonomous adaptation.

\section{Implications for Accessibility and HCI}
Combining deterministic rules with multi-agent, LLM-driven reasoning (SIF/MA-SIF) shows that multimodal interaction signals can be translated into personalised adaptations during live use. In the feasibility study over \textbf{84 events} and \textbf{six user profiles}, the system produced \textbf{84.5\%} schema-valid responses with a \textbf{median latency of $\sim$13\,s}, and all responses were passed through the Validator Agent for consistency. Hands-free profiles trailed slightly in validity, indicating where focused improvements are warranted.

For accessibility, this reduces manual configuration and supports users with motor, visual, or hands-free needs by learning from interaction patterns over time. For HCI, the results indicate a generalisable path towards runtime, cross-platform adaptivity that spans UI, geometry, and input modalities. The hybrid approach also offers a pragmatic design trade-off: fast, predictable fallbacks via rules for common cases; flexible LLM reasoning when context is ambiguous. The caveat is increased complexity in orchestrating two reasoning pipelines and managing latency.

\section{Key Findings and Contributions}
This research contributes to both practical system design and conceptual models for multimodal adaptive UIs:
\begin{enumerate}
    \item \textbf{Framework Architecture:} A modular, three-layer architecture separating input capture, event standardisation, and reasoning, enabling portability across Flutter, SwiftUI, and Unity.
    \item \textbf{Smart Intent Fusion (SIF):} A hybrid engine combining deterministic rules for guaranteed accessibility fallbacks with LLM-driven reasoning for complex scenarios.
    \item \textbf{Multi-Agent Extension (MA-SIF):} Specialised agents for UI, geometry, and input, validated by a dedicated Validator Agent to reduce conflicts and hallucinations.
    \item \textbf{Developer-Focused Integration:} Standardised JSON contracts and generalised methods for event sending, adaptation handling, and profile management.
    \item \textbf{Accessibility Impact (Empirical):} In the dataset, \textit{97.5\%} of suggestions directly supported motor, visual, or hands-free needs; median latency remained acceptable for asynchronous adaptation ($\sim$13\,s).
\end{enumerate}

\section{Comparison with Related Work}

\paragraph{SUPPLE \cite{Gajos2008SUPPLE}}
SUPPLE demonstrated the value of generating optimised interfaces for specific motor abilities, but relied primarily on constraint-solving over predefined UI descriptions. In contrast, the framework presented in this thesis performs \emph{real-time} adaptation during active use, using multimodal interaction signals fused by a hybrid rule-based and LLM-driven reasoning process. Whereas SUPPLE required a calibration phase, the proposed approach continuously refines adaptations based on interaction history.

\paragraph{Reflow \cite{Wu2024}}
Reflow’s pixel-based UI adaptation enables closed-source applications to be optimised without access to source code, but operates primarily at the visual layout level and does not incorporate higher-level semantic reasoning or multimodal fusion. The framework presented here incorporates profile- and history-driven adaptations across UI, geometry, and input modalities, and supports integration for both new and existing applications through standardised JSON contracts.

\paragraph{UICoder \cite{Wu2024}}
UICoder automates the generation of UI code from textual descriptions, enabling rapid interface creation. The approach taken in this work is complementary rather than competitive: whereas UICoder generates new interfaces at compile time, the proposed framework adapts \emph{existing} live interfaces at runtime, guided by multimodal context and inferred user intent.

\paragraph{GUIDe \cite{kumar2007guide}}
GUIDe’s gaze-augmented interaction techniques illustrate the potential of combining modalities for improved accuracy and reduced activation errors. This framework generalises that principle by supporting arbitrary modality combinations (e.g., voice + touch, gaze + gesture) and applying them within a cross-platform adaptation pipeline.

\paragraph{Reinforcement Learning-based UI Adaptation \cite{gaspar2023learning}}
Reinforcement learning approaches can personalise UIs over time using physiological feedback, but often require specialised sensing hardware and extended training periods. The framework presented in this thesis prioritises lightweight integration and immediate adaptation using LLM reasoning, while retaining the potential to incorporate reinforcement learning techniques in future iterations.

\medskip
Table~\ref{tab:related-comparison} summarises key differences between the framework and representative related work.

\begin{table}[h]
\centering
\small
\caption{Comparison of related adaptive UI systems}
\label{tab:related-comparison}
\begin{tabular}{p{3cm}p{2.5cm}p{3cm}p{3cm}p{3cm}}
\toprule
\textbf{System} & \textbf{Modalities Supported} & \textbf{Reasoning Method} & \textbf{Adaptation Scope} & \textbf{Extensibility} \\
\midrule
SUPPLE & Single modality (motor calibration) & Constraint optimisation & Layout generation (compile-time) & Requires redesign for new modalities \\
Reflow & Any (pixel-based only) & Visual analysis + rules & Layout changes only (runtime) & Limited; no semantic adaptation \\
UICoder & N/A (code generation) & LLM code synthesis & New UI creation (compile-time) & Extendable via prompt tuning \\
GUIDe & Gaze + keyboard/mouse & Rule-based & Targeting and scrolling (runtime) & Limited to gaze/pointing \\
RL-based UI Adaptation & Depends on sensors & Reinforcement learning & Layout, content (runtime) & Requires specialised hardware \\
\textbf{This work} & Touch, voice, gesture, keyboard (+ extensible) & Hybrid rules + multi-agent LLM & UI, geometry, input modes (runtime) & High; JSON contracts + config-based agents \\
\bottomrule
\end{tabular}
\end{table}

\section{Limitations and Challenges}
\begin{itemize}
    \item \textbf{Latency Constraints:} LLM reasoning adds delay, especially in multi-agent settings. Timeouts bound worst cases but cap reasoning depth.
    \item \textbf{Reliance on Cloud APIs:} Current Gemini usage ties performance to network availability; offline/privacy-critical contexts need on-device options.
    \item \textbf{UI Context Awareness:} Reasoning depends on predefined targets and metadata; there is no full semantic model of the live UI yet.
    \item \textbf{Synthetic Evaluation:} Metrics are based on controlled, simulated tasks rather than large-scale, real-world studies.
    \item \textbf{Developer Adoption:} Integration requires implementing the JSON contract and adapter pattern; more automation would help.
\end{itemize}

\section{Future Work} 
While the framework in its current form demonstrates the feasibility of real-time, multimodal, adaptive UI adaptation, several opportunities exist to expand its capabilities, improve its performance, and extend its reach into new domains. Future work can be divided into short-term engineering improvements (such as SDK packaging and support for additional modalities) and long-term research directions (such as autonomous, on-device reasoning agents capable of compile-time and runtime adaptation).

\subsection{User Study Plan}
A minimal but informative evaluation would help with understanding user interactions and preferences in real-world scenarios:
\begin{itemize}
    \item \textbf{Participants:} 12-18 (4-6 per profile cluster).
    \item \textbf{Design:} Within-subjects; rules-only vs. rules+MA-SIF.
    \item \textbf{Measures:} SUS, NASA-TLX, error rate, task time, adaptation acceptance, perceived control.
    \item \textbf{Tasks:} Smart home scenarios (toggle, adjust, unlock) with induced constraints (motor, visual, hands-free).
\end{itemize}

\subsection{Extending to Existing UIs}
Currently, the framework is designed for applications built with its SDK (adapter) or adhering to its JSON contracts. A significant future step is retrofitting existing applications with minimal code changes. This could involve:
\begin{itemize}
    \item Creating a UI overlay adapter capable of intercepting and adapting components without direct code modification.
    \item Developing browser extensions or OS-level accessibility hooks to capture events and inject adaptations dynamically.
\end{itemize}

\subsection{Developer SDK and Ecosystem}
A natural evolution of this framework is packaging it as a reusable SDK for popular development platforms. This would abstract away the complexity of event standardisation, profile management, and adaptation application, allowing developers to integrate adaptive capabilities with minimal code. The SDK could include:
\begin{itemize}
    \item Pre-built adapters for Flutter, SwiftUI, Unity, and web frontends.
    \item Visual debugging tools for inspecting adaptation decisions in real time.
    \item Templates for common accessibility adaptations aligned with WCAG guidelines.
\end{itemize}
By lowering the integration barrier, this would encourage adoption in both commercial and open-source projects, expanding the framework’s impact.

\subsection{UI Component Analyzer}
An automated UI Component Analyzer could parse live UI trees (or screenshots) to create a semantic model of the interface, mapping components to accessible adaptation targets. This could leverage:
\begin{itemize}
    \item \textbf{Image/vision models} for recognising UI elements from screenshots.
    \item \textbf{DOM or widget tree parsing} for native apps.
    \item Integration with the SIF backend so that adaptations are generated based on \textbf{actual UI context}, not just predefined IDs or defined metadata.
\end{itemize}

\subsection{Enhancing Multimodal Inputs}
Future versions should expand input modality coverage beyond touch, voice, and gestures:
\begin{itemize}
    \item \textbf{Full gaze tracking} to enable more intuitive and hands-free navigation in VR/AR and desktop environments.
    \item Supporting \textbf{hand tracking} for precise and more natural gesture recognition as well as UI navigation.
    \item \textbf{Brain-Computer Interface (BCI)} integration for high-accessibility scenarios. This expansion will require updates to the Input Adapter Layer to capture and standardise new modalities without breaking existing ones.
\end{itemize}

\subsection{LLM Agents for Autonomous Adaptation}
While MA-SIF improves modularity, agents currently operate on fixed prompts and allowed actions. The next step is autonomous adaptation, where agents:
\begin{itemize}
    \item Dynamically update their own prompts based on user feedback and historical success rates.
    \item Use proposed adaptation actions not initially in the allowed list (hallucinations), subject to developer approval.
    \item Collaborate with other agents through inter-agent dialogue before validation, enabling more complex adaptation strategies.
\end{itemize}

\subsection{Specialized AI Model for UI Adaptation}
Relying on general-purpose LLMs limits the precision and latency of adaptation reasoning. A dedicated, domain-specific model could be trained on:
\begin{itemize}
    \item UI adaptation logs generated by SIF.
    \item Accessibility guidelines (e.g., WCAG 2.1).
    \item Synthetic data from simulated user impairments.
\end{itemize}
This model could be run locally on-device for low-latency, privacy-friendly adaptation, making the framework viable in offline or sensitive deployments.

\subsection{Towards Autonomous, On-Device UI Adaptation Agents}
A long-term vision for this work is to move beyond general-purpose LLMs and cloud-based reasoning, towards a domain-specialised, reinforcement-learned agent capable of autonomously adapting UIs both at runtime and compile time.
Such an agent would integrate several capabilities:
\begin{enumerate}
    \item \textbf{On-Device AI Reasoning}
        \begin{itemize}
            \item Replace the dependency on external LLM APIs with a lightweight, locally hosted model optimised for UI adaptation tasks.
            \item Reduce latency and remove the need for internet connectivity, improving privacy and enabling use in sensitive domains such as healthcare and defence.
            \item Models could be quantised and optimised to run efficiently on consumer-grade CPUs, GPUs, or NPUs.
        \end{itemize}
    \item \textbf{Integrated Visual UI Understanding}
        \begin{itemize}
            \item Couple the reasoning agent with an image-based UI analyser (e.g., a fine-tuned vision transformer or CLIP-like model) to interpret the live UI.
            \item At runtime, this would allow the system to `see'' the interface, understanding spatial layouts, colour schemes, element hierarchies, and visibility issues without requiring developer-provided metadata.
            \item At compile time, the analyser could scan UI code or rendered snapshots to detect accessibility violations, recommend improvements, or auto-inject adaptation hooks.
        \end{itemize}
    \item \textbf{Reinforcement Learning from User Feedback}
        \begin{itemize}
            \item Continuously learn from user interactions by rewarding effective adaptations and penalising ineffective ones.
            \item Gradually develop personalised adaptation policies optimised for each user's needs.
        \end{itemize}
    \item \textbf{Dual-Mode Adaptation: Compile-Time + Runtime}
        \begin{itemize}
            \item Analyse static UIs at compile time to pre-configure accessibility defaults and flag issues.
            \item Dynamically adapt during runtime based on live user context and environment.
        \end{itemize}
    \item \textbf{Continuous Multi-Modal Feedback Loop}
        \begin{itemize}
            \item Combine visual UI analysis with interaction history to refine adaptation strategies.
            \item Maintain an ongoing loop where the system reasons over both the appearance of the UI and the way the user interacts with it.
        \end{itemize}
\end{enumerate}

\medskip
In summary, the results indicate that hybrid, multi-agent adaptation is feasible and immediately useful for accessibility, while the outlined extensions such as UI analysis, richer modalities, latency reduction, specialised models, and user studies form a credible path to robust, deployable adaptive interfaces.

% Chapter 8
\chapter{Conclusion}

\section{Summary of the Work}
This thesis designed and evaluated a multimodal AI-driven GUI framework that adapts user interfaces in real time. The goal was to support personalised and accessible experiences, especially in health contexts where users may have motor, visual, or hands-free constraints. The framework captures touch, keyboard, voice, and gesture inputs and turns them into concrete adaptation actions via \emph{Smart Intent Fusion} (SIF).

The architecture has three modular layers: an \textbf{Input Adapter} that standardises events and handles basic management, a \textbf{SIF Backend} that combines rules with LLM-driven reasoning, and a \textbf{Frontend} that renders the UI and applies adaptations. A key addition is \textbf{Multi-Agent SIF} (MA-SIF): separate specialists for UI, geometry, and input, with a Validator Agent to reconcile and validate outputs. This setup reduced hallucinations, improved reliability, and stayed easy to configure through an external JSON file.

\paragraph{Research Questions Answered:}
This work demonstrates that a modular framework can (i) adapt UIs in real time, (ii) deliver accessibility-oriented behaviour, and (iii) remain developer-friendly across platforms. The prototype provides instant rule-based feedback with asynchronous MA-SIF results, achieves high coverage of accessibility actions, and maintains a stable JSON contract and SDK that integrates cleanly with the Flutter stack.

\section{Summary of Contributions}
\begin{enumerate}
    \item \textbf{Modular, cross-platform architecture:} Designed to work across Flutter and SwiftUI, with a clear path to Unity/VR.
    \item \textbf{Smart Intent Fusion (SIF):} A hybrid engine that mixes deterministic rules with LLM reasoning for ambiguous cases.
    \item \textbf{MA-SIF extension:} Specialised agents (UI/geometry/input) plus a Validator Agent for conflict resolution and schema adherence.
    \item \textbf{Developer-friendly integration:} Consistent JSON contracts for events, adaptations, and profiles; minimal glue code.
    \item \textbf{Accessibility focus:} Concrete adaptations for motor, visual, and hands-free use, demonstrated via profile configurations.
\end{enumerate}

\section{Limitations}
\begin{itemize}
    \item \textbf{LLM API dependency:} Requires internet connectivity and an API key with quota limits.
    \item \textbf{Synthetic evaluation:} Results are based on controlled profiles and tasks rather than large-scale field studies.
    \item \textbf{UI context awareness:} Reasoning uses predefined metadata; there is no live semantic understanding of layouts yet.
    \item \textbf{Latency:} Multi-agent LLM reasoning adds seconds of delay, which may not suit time-critical scenarios.
    \item \textbf{Privacy and cost:} Multimodal logging can be sensitive and cloud inference can be costly; production systems need privacy controls and efficiency measures.
\end{itemize}

\section{Lessons Learned}
\begin{itemize}
    \item \textbf{Hybrid $>$ pure AI:} Rules keep the UI responsive; MA-SIF adds smarter, profile-aware changes a moment later.
    \item \textbf{Schemas are leverage:} Stable JSON contracts made the system portable and easy to debug as components evolved.
    \item \textbf{Prompts and validation matter:} Constraint-heavy prompts plus a Validator Agent noticeably improved output quality.
    \item \textbf{Simulation is useful:} Synthetic profiles/events were enough to tune reasoning and latency trade-offs before user studies.
    \item \textbf{Modularity pays off:} Loose coupling made it straightforward to swap models/modalities and iterate quickly.
\end{itemize}

\section{Evaluation Results: Conclusion}
The Adaptive Smart Home Controller shows the framework can deliver valid, accessibility-focused adaptations in real time. Across 84 events and six profiles, the system reached \textbf{84.5\%} schema-valid outputs (i.e., responses that pass the strict JSON schema), with \textbf{100\%} of responses flowing through the Validator Agent for consistency. \textbf{97.5\%} of suggested actions targeted motor, visual, or hands-free needs.

Under the balanced multi-agent configuration, median latency was about \textbf{13\,s}. This worked well with the two-tier approach: rules for instant feedback; MA-SIF for richer, profile-aware changes. Hands-free profiles trailed slightly on schema validity, pointing to future work on mode-switch prompts and input disambiguation. Overall, the combination of rules + MA-SIF + validation gave predictable outputs without sacrificing adaptability.

\section{Final Remarks}
This thesis showed that \textbf{Smart Intent Fusion}, especially in a multi-agent setup with a validator, can improve accessibility without giving up responsiveness. The architecture is designed to evolve: on-device inference to cut latency, richer UI understanding for better context, and reinforcement from user outcomes. In short: treat UIs as programmable surfaces, fuse multimodal intent, and validate before acting. That brings us closer to interfaces that are not just functional, but truly personalised, inclusive, and adaptive to the diversity of human interaction.

\printbibliography[title=References]


\end{document}
